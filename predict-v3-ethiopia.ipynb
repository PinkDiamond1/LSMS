{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import Imputer\n",
    "from fancyimpute import SoftImpute\n",
    "from sklearn.metrics import roc_curve, auc, silhouette_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from scipy import stats\n",
    "import glmnet_python\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot \n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "import math\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Output Variables\n",
    "\n",
    "This file predicts output variables using regression and classification models. Both regression are classification are applied to full dataset and clusters.\n",
    "\n",
    "Select input and output variables in the first section and then run functions at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Inputs and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We only use one output at a time. Here are the list of outputs to plug into the output variable below:\n",
    "\n",
    "* asset_owned___output\n",
    "* crop_diversification___output\n",
    "* crop_sales___output\n",
    "* crop_sales_specialization___output\n",
    "* expenditure___output\n",
    "* food_expenditure_diversification___output\n",
    "* income___output\n",
    "* income_diversification___output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classes(output_var, pred):\n",
    "    max_bins = 3\n",
    "    _, boundaries = np.histogram(output_var, bins=max_bins)\n",
    "    classes = np.digitize(pred, bins=boundaries)\n",
    "    return classes, max_bins\n",
    "\n",
    "def for_year(var, year):\n",
    "    return var + '___ethiopia_' + str(year)\n",
    "\n",
    "colors= {2011:'r', 2013:'g', 2015:'blue'}\n",
    "ax = {}\n",
    "def run_regressions(in_name, out_dir, non_policy_inputs, segment_variables, inputs, output, year):\n",
    "    global table\n",
    "    global coef_table\n",
    "    global avg_table\n",
    "    global coef_map\n",
    "    # table for regressions and classification\n",
    "    table = pd.DataFrame()\n",
    "    avg_table = pd.DataFrame()\n",
    "    coef_map = {}\n",
    "    # create table of coefficients\n",
    "    coef_table = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except:\n",
    "        print(\"Dir exists\")\n",
    "    \n",
    "    ols = linear_model.LinearRegression()\n",
    "    ridge = linear_model.Ridge(alpha=.5)\n",
    "    lasso = linear_model.Lasso(alpha = 0.01, max_iter=1e5)\n",
    "    lars_lasso = linear_model.LassoLars(alpha=.1)\n",
    "    bayes_ridge = linear_model.BayesianRidge()\n",
    "    sgd = linear_model.SGDRegressor()\n",
    "    svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    svr_lin = SVR(kernel='linear', C=1e3)\n",
    "    svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "    kernel_ridge = KernelRidge(alpha=1.0)\n",
    "    \n",
    "    # pick which regression algos to use\n",
    "    regression_algorithms = (\n",
    "#        ('OrdinaryLeastSquares', ols),\n",
    "#        ('RidgeRegression', ridge),\n",
    "        ('Lasso', lasso),\n",
    "#         ('LARS Lasso', lars_lasso),\n",
    "#         ('BayesianRidgeRegression', bayes_ridge),\n",
    "#         ('StochasticGradientDescent', sgd),\n",
    "#         ('SupportVectorRegressionRBF', svr_rbf),\n",
    "#         ('SupportVectorRegressionLinear', svr_lin),\n",
    "#         ('SupportVectorRegressionPolynomial', svr_poly),\n",
    "#         ('KernelRidgeRegression', kernel_ridge)\n",
    "    )\n",
    "    \n",
    "    df = read_csv(in_name)\n",
    "    raw_df = read_csv(in_name.replace('normed', 'raw'))\n",
    "#     df[output] = raw_df[for_year('crop_sales___output', year)]/raw_df[for_year('land_surface', year)]*10000.0\n",
    "    df = df.loc[df[output].dropna().index] # drop rows with unobserved income\n",
    "#     df = df.loc[df['household_head_is_male___ethiopia_2015']==0]\n",
    "    df['nid']= df.index.tolist()\n",
    "#     filter_var = 'lives_in_amhara___ethiopia_' + str(year)\n",
    "#     df = df[df[filter_var]==True]\n",
    "    #df = df.loc[df[output] != 0] # drop zero outputs\n",
    "    # select % of data in test set\n",
    "    test_split = 0.2\n",
    "    \n",
    "    # perform matrix completion. completed is returned as a np array\n",
    "    # we've discussed not using it, but I left it in because I wasn't able to\n",
    "    # fit the StandardScaler with a DataFrame that contained NaN's\n",
    "\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "\n",
    "    # if we want to use low rank matrix completion inst\n",
    "    # completed = SoftImpute().complete(x)\n",
    "    \n",
    "    # reconstruct dataframe with completed matrix\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    \n",
    "#     raw_df[output] = raw_df[for_year('crop_sales___output', year)]/raw_df[for_year('land_surface', year)]*10000.0\n",
    "    raw_df = raw_df.loc[df[output].dropna().index]\n",
    "#     raw_df = raw_df.loc[raw_df['household_head_is_male___ethiopia_2015']==0]\n",
    "    raw_df['nid']= raw_df.index.tolist()\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    raw_df['productivity'] = raw_df[for_year('crop_sales___output', 2015)]/raw_df[for_year('land_surface', 2015)]\n",
    "    raw_df['productivity'] = raw_df['productivity'].apply(lambda x: 0 if x == np.inf else x)\n",
    "\n",
    "#     raw_df[output] = raw_df[output].apply(lambda x: 100.0*math.exp(x))\n",
    "    #relevant_vars = ['hired_labor___policy', 'oxen_owned___policy', 'chemical_fertilizers_used___policy', 'land_surface', 'plough_owned___policy']\n",
    "#     relevant_vars = list(segment_variables.keys())\n",
    "#     for seg_var in relevant_vars:\n",
    "# #         seg_var_year = for_year(seg_var, year)\n",
    "#         seg_var_year = seg_var\n",
    "#         raw_df[seg_var_year] = raw_df[seg_var_year].apply(lambda x: math.exp(x) - 1)\n",
    "#     weight_cols = filter(lambda x: x.find('weight') != -1, mat.columns)\n",
    "#     for col in cols:\n",
    "#         if col in weight_cols:\n",
    "#             continue\n",
    "#         mat[col] = mat[col] * mat[for_year('weight', year)]\n",
    "#     mat[output] = mat[output] * raw_df[for_year('weight', year)]\n",
    "    \n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    mat[for_year('animals___policy', year)] = mat[for_year('number_of_oxen_owned___policy', year)] + mat[for_year('number_of_plough_owned___policy', year)] + mat[for_year('number_of_axe_owned___policy', year)] + mat[for_year('number_of_pick_axe_owned___policy', year)] + mat[for_year('number_of_sickle_owned___policy', year)]\n",
    "    mat[for_year('tools___policy', year)] = mat[for_year('number_of_axe_owned___policy', year)] + mat[for_year('number_of_pick_axe_owned___policy', year)] + mat[for_year('number_of_sickle_owned___policy', year)]\n",
    "    inputs += [for_year('animals___policy', year), for_year('tools___policy', year)]\n",
    "    #     mat = mat.drop(weight_cols, axis=1)\n",
    "    #scale_up_factor = 100.0\n",
    "    #mat[output] = mat[output].apply(lambda x: x*scale_up_factor)\n",
    "    y = mat[output]\n",
    "    x = mat[inputs]\n",
    "    \n",
    "    def update_best_lambda(x_scaled):\n",
    "        global regression_algorithms\n",
    "        copy_y = np.array(y, dtype=np.float64)\n",
    "        print (copy_y)\n",
    "        fit = cvglmnet(x = x_scaled.copy(), y = copy_y)\n",
    "        print ('Best alpha={0}'.format(fit['lambda_min']))\n",
    "        regression_algorithms = (('Lasso', linear_model.Lasso(alpha=fit['lambda_min'], max_iter=1e5)))\n",
    "        return fit['lambda_min']\n",
    "    \n",
    "    # Split test/train\n",
    "    indices = range(len(mat))\n",
    "    x_train, x_test, y_train, y_test, ind_train, ind_test = \\\n",
    "        train_test_split(x, y, indices, test_size=test_split, random_state=42)\n",
    "    \n",
    "    def get_train_test(input_vars):\n",
    "        x = mat[input_vars].copy()\n",
    "#         for col in x.columns.values:\n",
    "#             x[col] = x[col] * raw_df[for_year('weight', year)]\n",
    "        x_scaled = StandardScaler()\n",
    "        x_scaled.fit(x)\n",
    "        x_sc = x_scaled.transform(x)\n",
    "        # reconstruct DataFrame\n",
    "        x = pd.DataFrame(x_sc, columns=x.columns)\n",
    "        training_x = x.iloc[ind_train, :]\n",
    "        testing_x = x.iloc[ind_test, :]\n",
    "        return x_sc, training_x, testing_x\n",
    "    \n",
    "    def digitize(output_var, pred):\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        classes, max_bins = get_classes(output_var, pred)\n",
    "        b_classes = label_binarize(classes, range(max_bins))\n",
    "        return b_classes\n",
    "    \n",
    "    def calc_unsegmented(baseline, x_train, x_test): \n",
    "        global table\n",
    "        global coef_map\n",
    "        # reg keeps predictions from regressions along with keys\n",
    "        reg = dict()\n",
    "        for name, algo in regression_algorithms:\n",
    "            reg[name] = {}\n",
    "\n",
    "        # keys and values from test data\n",
    "        keys_list = []\n",
    "        y_list = []\n",
    "        for k, v in y_test.iteritems():\n",
    "            keys_list.append(k)\n",
    "            y_list.append(v)\n",
    "\n",
    "        # run regressions on full dataset\n",
    "        for name, algo in regression_algorithms:\n",
    "            ### CI calculation\n",
    "            model = sm.OLS(y_train, x_train)\n",
    "#             fit = model.fit_regularized(alpha=0.006, refit=True)\n",
    "            fit = model.fit()\n",
    "            print(\"Confidence intervals are {0}\".format(fit.conf_int()))\n",
    "            y_pred = fit.predict(x_test)\n",
    "            \n",
    "#             model = algo.fit(x_train,y_train)\n",
    "#             y_pred = model.predict(x_test)\n",
    "\n",
    "            # add predictions to dict\n",
    "            for i, p in enumerate(y_pred):\n",
    "                t = reg[name]\n",
    "                t[keys_list[i]] = p\n",
    "\n",
    "            try:\n",
    "                test_c = digitize(y, y_test)\n",
    "                pred_c = digitize(y, y_pred)\n",
    "                auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "            except ValueError:\n",
    "                auc_c = 0.5\n",
    "            mse = mean_squared_error(y_test,y_pred)\n",
    "            scaled_mse = (mse/np.std(y))\n",
    "\n",
    "            # add row to table\n",
    "            new_row = pd.DataFrame({'model': name, 'segment': '', 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': False}, index=[0])\n",
    "            table = table.append(new_row, ignore_index=True)\n",
    "\n",
    "            # add coefficients to map\n",
    "#             coef_map[name + '_' + baseline] = model.coef_\n",
    "            coef_map[name + '_' + baseline] = fit.params\n",
    "            lower_bounds = []\n",
    "            upper_bounds = []\n",
    "            ci = fit.conf_int()\n",
    "            for ci_row in ci.iterrows():\n",
    "                lower_bounds += [ci_row[1][0]]\n",
    "                upper_bounds += [ci_row[1][1]]\n",
    "#             for ci in fit.conf_int():\n",
    "#                 lower_bounds += [ci[0]]\n",
    "#                 upper_bounds += [ci[1]]\n",
    "            coef_map[name + '_' + baseline + '_lower_bound'] = lower_bounds\n",
    "            coef_map[name + '_' + baseline + '_upper_bound'] = upper_bounds\n",
    "    \n",
    "    def calc_segmented(segment_variables, baseline, x_train, x_test):\n",
    "        global table\n",
    "        global coef_map\n",
    "        global avg_table\n",
    "#         segment_vars = segment_variables\n",
    "        segment_vars = list(segment_variables.keys())\n",
    "        def iterative_clustering(sc_thres, corr_thres, alpha):\n",
    "            max_sc = 0\n",
    "            best_kmeans = None\n",
    "            \n",
    "#                 alpha = (1+a)/10.0\n",
    "            sc_incr = 1e-8\n",
    "            sc_thres = -1e-1\n",
    "            C = set([])\n",
    "            avail = set(segment_variables.keys())\n",
    "            prev_sc = 0\n",
    "            min_k = 4\n",
    "            corr_thres = 0.05\n",
    "            while sc_incr >= sc_thres:\n",
    "                u_f = 0\n",
    "                best_f = None\n",
    "                sc_best = 0\n",
    "                for f in avail:\n",
    "                    if segment_variables[f] < corr_thres:\n",
    "                        continue\n",
    "                    seg_data = mat[list(avail) + [f]]\n",
    "                    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "                    labels = kmeans.labels_\n",
    "                    sc = silhouette_score(seg_data, labels)\n",
    "                    sc_delta = sc - prev_sc\n",
    "                    if u_f < (segment_variables[f] + (alpha)*sc_delta):\n",
    "                        u_f = (segment_variables[f] + (alpha)*sc_delta)\n",
    "                        best_f = f\n",
    "                        sc_best = sc\n",
    "                if best_f == None:\n",
    "                    break\n",
    "                avail.remove(best_f)\n",
    "                C.add(best_f)\n",
    "#                 print (alpha, best_f, u_f, sc, prev_sc)\n",
    "                sc_incr = sc - prev_sc\n",
    "                prev_sc = sc\n",
    "            \n",
    "            seg_data = mat[list(C)]\n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "            labels = kmeans.labels_\n",
    "            sc = silhouette_score(seg_data, labels)\n",
    "            return sc, kmeans\n",
    "\n",
    "        \n",
    "        def add_clusters():\n",
    "#             from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "            # elbow method\n",
    "            sse = []\n",
    "        \n",
    "                \n",
    "#             #### Multiply by correlation coefficient\n",
    "            seg_data = mat[segment_vars]\n",
    "            for seg_var in segment_vars:\n",
    "#                 seg_data[seg_var] = seg_data[seg_var]*mat[for_year('weight', year)]\n",
    "                seg_data[seg_var] = seg_data[seg_var].apply(lambda x: x*segment_variables[seg_var])\n",
    "    \n",
    "            ### Plot elbow\n",
    "#             for k in range(1,9):\n",
    "#                 kmeans = KMeans(n_clusters=k).fit(seg_data)\n",
    "#                 labels = kmeans.labels_\n",
    "#                 sse.append(sum(np.min(cdist(seg_data, kmeans.cluster_centers_, 'euclidean'), axis=1)) / seg_data.shape[0])\n",
    "\n",
    "#             plt.clf()\n",
    "#             plt.plot(range(1,9), sse)\n",
    "#             plt.xlabel('k')\n",
    "#             plt.ylabel('Sum of squared error')\n",
    "#             plt.savefig(os.path.join(out_dir, 'elbow.png'))\n",
    "#             print(sse)\n",
    "#             min_k = sse.index(min(sse))\n",
    "#             print (min_k)\n",
    "\n",
    "            ### Iterative clustering\n",
    "#             max_sc = -1\n",
    "#             best_kmeans = None\n",
    "#             for sc_thres in [-0.1, -0.05, -0.01, -0.001, -1e-3, 0, 1e-3, 1e-2]:\n",
    "#                 for corr_thres in [0.05, 0.06, 0.08, 0.1, 0.12, 0.15]:\n",
    "#                     for alpha in [1e-4,1e-3,1e-2,0.1,1,10,100,1000,10000]:\n",
    "#                         sc, kmeans = iterative_clustering(sc_thres, corr_thres, alpha)\n",
    "#                         if sc > max_sc:\n",
    "#                             max_sc = sc\n",
    "#                             best_kmeans = kmeans\n",
    "#                             print ('Best found: {0}, {1}, {2}'.format(sc_thres, corr_thres, alpha))\n",
    "            \n",
    "#             kmeans = best_kmeans\n",
    "    \n",
    "            min_k = 4\n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "            labels = kmeans.labels_\n",
    "            mat['cluster'] = labels\n",
    "            means = []\n",
    "            for i in np.unique(labels):\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "                values = raw_clus[output].as_matrix()\n",
    "                weights = raw_clus[for_year('weight', year)].as_matrix()\n",
    "                average = np.average(values, weights=weights)\n",
    "                means.append(average)\n",
    "            sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "            print(sorted_ids)\n",
    "            mat['cluster'] = mat['cluster'].apply(lambda x: sorted_ids.index(x))\n",
    "#             vor = Voronoi(seg_data)\n",
    "#             plt.clf()\n",
    "#             voronoi_plot_2d(vor)\n",
    "#             plt.savefig(os.path.join(out_dir, 'voronoi.pdf'))\n",
    "            #mat['cluster_'+str(year)] = labels\n",
    "            select_variables = [output, 'latitude___ethiopia_2015', 'longitude___ethiopia_2015', 'nid'] + segment_vars\n",
    "            select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "\n",
    "#             id_df = pd.DataFrame(df.index.tolist())\n",
    "#             sel_df = pd.concat([, id_df], axis=0)\n",
    "            all_output = pd.concat([mat['cluster'], raw_df[list(select_variables)]], 1)\n",
    "            all_output.to_csv(os.path.join(out_dir,'clus_' + baseline + '_' + output + '.csv'))\n",
    "            return min_k\n",
    "            \n",
    "        \n",
    "        # Add segments\n",
    "        def add_segments():\n",
    "            median_segments = {}\n",
    "            for seg_var in segment_vars:\n",
    "                #binary\n",
    "                if len(np.unique(mat[seg_var])) <= 3:\n",
    "                    median_segments[seg_var] = 0\n",
    "                else:\n",
    "                    median_segments[seg_var] = np.median(mat[seg_var])\n",
    "            mat['cluster'] = 0\n",
    "            for seg_var in segment_vars:\n",
    "                mat['cluster'] = 2*mat['cluster'] + [int(x) for x in mat[seg_var] > median_segments[seg_var]]\n",
    "            #mat[['cluster'] + segment_vars].to_csv(os.path.join(out_dir,'segment_' + ','.join(segment_vars) + '_' + baseline + '_' + output + '.csv'))\n",
    "            return int(math.pow(2, len(segment_vars)))\n",
    "\n",
    "        def add_location_segments():\n",
    "            locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "            i = 0\n",
    "            mat['cluster'] = 0\n",
    "            for l in sorted(locations):\n",
    "                loc_feature = 'lives_in_' + l + '___ethiopia_' + str(year)\n",
    "                loc_val = mat[loc_feature].apply(lambda x: 0 if x < 0 else 1)\n",
    "                mat['cluster'] = mat['cluster'] + (i*loc_val)\n",
    "                i += 1\n",
    "            return len(locations) + 1\n",
    "            \n",
    "        def _run(max_clusters, method_name):\n",
    "            global table\n",
    "            global avg_table\n",
    "            global coef_map\n",
    "            # reg_clus keeps predictions from clustered regressions along with keys\n",
    "            reg_clus = dict()\n",
    "\n",
    "            for name, algo in regression_algorithms:\n",
    "                reg_clus[name] = {}\n",
    "\n",
    "            # need new dataframes with only training and test rows.\n",
    "            # we use this when looping through clusters\n",
    "            train_mat = mat.loc[ind_train]\n",
    "            test_mat = mat.loc[ind_test]\n",
    "            train_size = len(train_mat)\n",
    "            # same code as regular regressions, but add loop for clusters\n",
    "            series = {}\n",
    "            series[output] = []\n",
    "            for seg in segment_vars:\n",
    "                series[seg] = []\n",
    "            series = pd.DataFrame()\n",
    "            row = {}\n",
    "            raw_cols = raw_df.columns.values\n",
    "            raw_reg = re.compile('^((?!norm).)*2015$')\n",
    "            avg_variables = list(filter(raw_reg.search, raw_cols))\n",
    "#             for seg in segment_vars:\n",
    "#                 values = raw_df[seg].as_matrix()\n",
    "#                 weights = raw_df[for_year('weight', year)].as_matrix()\n",
    "#                 average = np.average(values, weights=weights)\n",
    "#                 row['weighted_mean_' + seg] = average\n",
    "#                 row['normal_mean_' + seg] = np.mean(values)\n",
    "#             new_row = pd.DataFrame(row, index=[0])\n",
    "#             avg_table = avg_table.append(new_row, ignore_index=True)\n",
    "#             avg_table = avg_table.transpose()\n",
    "            for i in range(max_clusters):\n",
    "                train_clus = x_train.loc[train_mat['cluster'] == i]\n",
    "                train_y = y_train.loc[train_mat['cluster'] == i]\n",
    "                test_clus = x_test.loc[test_mat['cluster'] == i]\n",
    "                test_y = y_test.loc[test_mat['cluster'] == i]\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "#                 avg_variables = segment_vars\n",
    "#                 avg_variables = [\n",
    "#                     'use_extension_program___policy',\n",
    "#                     'prevent_damage___policy',\n",
    "#                     'hired_labor___policy', \n",
    "#                     'oxen_owned___policy', \n",
    "#                     'chemical_fertilizers_used___policy', \n",
    "#                     'land_surface',\n",
    "#                     'plough_owned___policy',\n",
    "#                     'household_size',\n",
    "#                 ]\n",
    "#                 avg_variables = [for_year(x, year) for x in avg_variables]\n",
    "                for seg in avg_variables:\n",
    "                    values = raw_clus[seg].as_matrix()\n",
    "                    weights = raw_clus[for_year('weight', year)].as_matrix()\n",
    "                    average = np.average(values, weights=weights)\n",
    "                    row['mean_' + seg] = average\n",
    "                    row['avg_' + seg] = np.mean(values)\n",
    "                    variance = np.average((values-average)**2, weights=weights)\n",
    "                    row['stddev_' + seg] = math.sqrt(variance)\n",
    "                    row['stderr_' + seg] = math.sqrt(variance)/math.sqrt(len(values))\n",
    "                    row['25ile_' + seg] = np.percentile(values, 25)\n",
    "                    row['75ile_' + seg] = np.percentile(values, 75)\n",
    "#                     row['mean_' + seg] = np.mean(pd.concat([raw_clus[seg], test_clus[seg]], 0).as_matrix())\n",
    "#                 row['segment'] = ','.join(segment_vars)\n",
    "                row['index'] = i\n",
    "#                 row['mean_output'] = np.mean(raw_clus[output.replace('norm', 'raw')].as_matrix())\n",
    "#                 row['stddev_output'] = np.std(raw_clus[output.replace('norm', 'raw')].as_matrix())\n",
    "                row['size'] = len(raw_clus)\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "                avg_table = avg_table.append(new_row, ignore_index=True)\n",
    "                cluster_percent = (len(train_clus)*100.0)/train_size\n",
    "                if train_clus.empty or test_clus.empty: # or cluster_percent < 5:\n",
    "                    continue\n",
    "\n",
    "                keys_list = []\n",
    "                y_list = []\n",
    "                for k, v in test_y.iteritems():\n",
    "                    keys_list.append(k)\n",
    "                    y_list.append(v)\n",
    "\n",
    "                for name, algo in regression_algorithms:  \n",
    "#                     model = algo.fit(train_clus,train_y)\n",
    "#                     y_pred = model.predict(test_clus)\n",
    "                    \n",
    "                    model = sm.OLS(train_y, train_clus)\n",
    "#                     fit = model.fit_regularized(alpha=0.00617565, refit=True)\n",
    "                    fit = model.fit()\n",
    "                    print(\"Confidence intervals are {0}\".format(fit.conf_int()))\n",
    "                    y_pred = fit.predict(test_clus)\n",
    "\n",
    "                    for a, b in enumerate(y_pred):\n",
    "                        t = reg_clus[name]\n",
    "                        t[keys_list[a]] = b\n",
    "\n",
    "                    coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = fit.params\n",
    "                    lower_bounds = []\n",
    "                    upper_bounds = []\n",
    "                    ### CI for ols\n",
    "                    ci = fit.conf_int()\n",
    "                    for ci_row in ci.iterrows():\n",
    "                        lower_bounds += [ci_row[1][0]]\n",
    "                        upper_bounds += [ci_row[1][1]]\n",
    "\n",
    "                    ### CI for Lasso\n",
    "#                     for ci in fit.conf_int():\n",
    "#                         lower_bounds += [ci[0]]\n",
    "#                         upper_bounds += [ci[1]]\n",
    "\n",
    "                    coef_map[name + '_' + method_name + '_' + str(i) + '_lower_bound'] = lower_bounds\n",
    "                    coef_map[name + '_' + method_name + '_' + str(i) + '_upper_bound'] = upper_bounds\n",
    "\n",
    "#                     coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = model.coef_\n",
    "\n",
    "            # plot sorted correlation\n",
    "            sorted_series = series.sort_values(['mean_' + output])\n",
    "#             prev = None\n",
    "#             min_diff = math.inf\n",
    "#             for mean_out in sorted_series['mean_output'].as_matrix():\n",
    "#                 if prev == None:\n",
    "#                     prev = mean_out\n",
    "#                     continue\n",
    "#                 diff = mean_out - prev\n",
    "#                 min_diff = min(min_diff, diff)\n",
    "            \n",
    "            \n",
    "            raw_segment_vars = avg_variables\n",
    "#             [t.replace('norm', 'raw') for t in segment_vars] + [\n",
    "#                 'gender_equity___ethiopia_2015', \n",
    "#                 'has_saved___policy___ethiopia_2015',\n",
    "#             ]\n",
    "            for seg in raw_segment_vars:\n",
    "                plt.clf()\n",
    "                plt.plot(sorted_series['mean_' + output].as_matrix(), sorted_series['mean_'+seg].as_matrix(), marker='o')\n",
    "                plt.xlabel('Average ' + output.replace('___output___ethiopia_2015', '') + ' output')\n",
    "                plt.ylabel('Average ' + seg.replace('___policy___ethiopia_' +str(year), '').replace('_',' '))\n",
    "                plt.savefig(os.path.join(out_dir, 'plot_' + seg + '_' + output + '.pdf'))\n",
    "                \n",
    "            # add mse's to table\n",
    "            keys = sorted(y_test.keys())\n",
    "            for name, algo in regression_algorithms:\n",
    "                sort_t = []\n",
    "                sort_p = []\n",
    "\n",
    "                for key in keys:\n",
    "                    if key not in y_test or key not in reg_clus[name]:\n",
    "                        continue\n",
    "                    sort_t.append(reg_clus[name][key])\n",
    "                    sort_p.append(y_test[key])\n",
    "\n",
    "                try:\n",
    "                    test_c = digitize(y, sort_t)\n",
    "                    pred_c = digitize(y, sort_p)\n",
    "                    auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "                except ValueError:\n",
    "                    auc_c = 0.5\n",
    "                mse = mean_squared_error(sort_t,sort_p)\n",
    "                scaled_mse = (mse/np.std(y))\n",
    "                new_row = pd.DataFrame({'model': name, 'segment': ','.join(segment_vars), 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': True, 'method': method_name}, index=[0])\n",
    "                table = table.append(new_row, ignore_index=True)\n",
    "                \n",
    "    \n",
    "        ##### Run grouped regressions\n",
    "        #_run(add_location_segments(), 'segmented')\n",
    "        if (len(segment_vars) > 1):\n",
    "            _run(add_clusters(), 'clustered')\n",
    "        #_run(add_segments(), 'segmented')\n",
    "    \n",
    "    def run_with_inputs(input_vars, name):\n",
    "        # map to be used in tracking coefficients\n",
    "        global coef_map\n",
    "        global coef_table\n",
    "        global x_train\n",
    "        global x_test\n",
    "        coef_map = {}\n",
    "        x_scaled, x_train, x_test = get_train_test(input_vars)\n",
    "        update_best_lambda(x_scaled)\n",
    "        calc_unsegmented(name, x_train, x_test)\n",
    "#         univariate_segments = [[x] for x in segment_variables]\n",
    "#         from itertools import combinations\n",
    "        #for segment_vars in combinations(non_policy_inputs, 2):\n",
    "#         for segment_vars in univariate_segments:\n",
    "#             calc_segmented(segment_vars, name, x_train, x_test)\n",
    "#         for segment_vars in combinations(segment_variables, 2):\n",
    "#             calc_segmented(list(segment_vars), name, x_train, x_test)\n",
    "        calc_segmented(segment_variables, name, x_train, x_test)\n",
    "        # calc_segmented(['location'], name, x_train, x_test)\n",
    "#         q = table.loc[(table['input']==name)&\\\n",
    "#                       (table['clustered']==True)]['scaled_mse'].nsmallest(2)\n",
    "#         best_segments = list(table.iloc[q.index.values]['segment'])\n",
    "#         best_segments = best.loc[(best['input']==name)&\\\n",
    "#                                  (best['output']==output.split('___')[0])]['segment'].iloc[0].split(',')\n",
    "#         best_segments = [x + '___ethiopia_' + str(year) for x in best_segments]\n",
    "#         calc_segmented(best_segments, name, x_train, x_test)\n",
    "#         for segment_vars in best_segments:\n",
    "#             calc_segmented([segment_vars], name, x_train, x_test)\n",
    "        \n",
    "        for k,v in sorted(coef_map.items()):\n",
    "            kvp = dict()\n",
    "            kvp['model'] = k\n",
    "            kvp['inputs'] = name\n",
    "\n",
    "            for val,invar in zip(v,input_vars):\n",
    "                kvp[invar] = val\n",
    "\n",
    "            new_row = pd.DataFrame(kvp, index=[0])\n",
    "            coef_table = coef_table.append(new_row, ignore_index=True)\n",
    "    \n",
    "    #Baseline 1\n",
    "#     input_vars = inputs + non_policy_inputs\n",
    "# #     run_with_inputs(input_vars, 'All variables')\n",
    "# #     #Baseline 2 - only highly correlated\n",
    "#     imp_vars = re.compile('^.*(?=damage|irrigation|animals|water_storage|saved|hired_workers|fertilizer|credit).*$')\n",
    "# #     imp_vars = re.compile('^.*(?=improved_seeds|extension|water_storage|saved|oxen|hired_workers|fertilizer|axe|health).*$')\n",
    "#     filter_vars = list(filter(imp_vars.search, input_vars))\n",
    "#     run_with_inputs(filter_vars, 'Highly correlated policy')\n",
    "    \n",
    "#     imp_vars = re.compile('^.*(?=land_surface|household_size|animals|damage|irrigation|water_storage|saved|hired_workers|fertilizer|credit).*$')\n",
    "# #     imp_vars = re.compile('^.*(?=improved_seeds|extension|water_storage|saved|oxen|hired_workers|fertilizer|axe|health).*$')\n",
    "#     filter_vars = list(filter(imp_vars.search, input_vars))\n",
    "#     run_with_inputs(filter_vars, 'Highly correlated all')\n",
    "# #     #Baseline 3\n",
    "# #     run_with_inputs(inputs, 'Policy variables')\n",
    "        \n",
    "#     non_raw_reg = re.compile('^((?!bank|sickle|plough|literacy|monogamous|widow|greenness|droughts|longitude|latitude|female|lives_in|land_surface).)*$')\n",
    "#     filter_vars = list(filter(non_raw_reg.search, input_vars))\n",
    "#     run_with_inputs(filter_vars, 'All without correlations')\n",
    "    global all_relevant, policy_relevant\n",
    "    \n",
    "    print ('Relevant variables')\n",
    "    print(all_relevant, policy_relevant)\n",
    "    run_with_inputs(all_relevant, 'Highly correlated all')\n",
    "    run_with_inputs(policy_relevant, 'Highly correlated policy')\n",
    "    \n",
    "    # save coefficient and output tables\n",
    "    coef_table.to_csv(os.path.join(out_dir,'coef_' + output + '.csv'))\n",
    "    table.to_csv(os.path.join(out_dir,output + '.csv'))\n",
    "    avg_table.to_csv(os.path.join(out_dir,output + '_avg' + '.csv'))\n",
    "    \n",
    "\n",
    "def get_segments(df, output):\n",
    "    df_t = df.loc[df[output].dropna().index]\n",
    "    df['segment_' + output], _ = get_classes(df_t[output], df[output])\n",
    "    return df\n",
    "\n",
    "def complete(df):\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "\n",
    "    # if we want to use low rank matrix completion inst\n",
    "    # completed = SoftImpute().complete(x)\n",
    "\n",
    "    # reconstruct dataframe with completed matrix\n",
    "\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    return mat\n",
    "\n",
    "def get_clusters(mat, output, segment_vars, out_dir):\n",
    "    segment_vars = list(segment_vars.keys())\n",
    "    seg_data = mat[segment_vars]\n",
    "    min_k = 4\n",
    "    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "    labels = kmeans.labels_\n",
    "    #print(labels)\n",
    "    means = []\n",
    "    for i in np.unique(labels):\n",
    "        df_clus = mat.loc[labels == i]\n",
    "        means.append(np.mean(df_clus[output].as_matrix()))\n",
    "    sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "    mat['segment_'+ output] = labels\n",
    "    mat['segment_'+ output] = mat['segment_'+ output].apply(lambda x: sorted_ids.index(x))\n",
    "#     raw_df['nid'] = df['nid']= df.index.tolist()\n",
    "    select_variables = [output, 'latitude___ethiopia_2015', 'longitude___ethiopia_2015'] + segment_vars\n",
    "    select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "#             print (mat)\n",
    "\n",
    "#             id_df = pd.DataFrame(df.index.tolist())\n",
    "#             sel_df = pd.concat([, id_df], axis=0)\n",
    "    all_output = pd.concat([mat['segment_' + output], raw_df[select_variables]], 1)\n",
    "    all_output.to_csv(os.path.join(out_dir,'clus_' + '_' + output + '.csv'))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number_of_hired_workers___policy___ethiopia_2015': 0.273808267736554, 'land_surface___ethiopia_2015': 0.272516052599146, 'quantity_of_chemical_fertilizers_used___policy___ethiopia_2015': 0.160090553373268, 'number_of_oxen_owned___policy___ethiopia_2015': 0.157053494140115, 'household_size___ethiopia_2015': 0.14258227392332698, 'number_of_plough_owned___policy___ethiopia_2015': 0.14223988258113301, 'uses_extension_program___policy___ethiopia_2015': 0.12981246190952803}\n",
      "['crop_sales___output___ethiopia_2011']\n",
      "['crop_sales___output___ethiopia_2013']\n",
      "['crop_sales___output___ethiopia_2015']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "filename = '/home/ananth/Downloads/ethiopia_v23_normed.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# change to true if you want to use all input fields\n",
    "def get_vars(year, base_output):\n",
    "    base_suffix = '___ethiopia_{0}'.format(year)\n",
    "    year_vars = df.filter(regex='.*{0}'.format(base_suffix)).columns.values\n",
    "#     non_raw_reg = re.compile('^((?!gender|damaged|bank|price|irrigation|diversification).)*$')\n",
    "#     year_vars = list(filter(non_raw_reg.search, year_vars))\n",
    "    out_reg = re.compile('.*' + base_output + '___output.*$')\n",
    "    outputs = list(filter(out_reg.search, year_vars))\n",
    "    policy_reg = re.compile('(.*___policy.*)$')\n",
    "    policy_inputs = list(filter(policy_reg.search, year_vars))\n",
    "    non_policy_reg = re.compile('^((?!policy|output|weight).)*$')\n",
    "    non_policy_inputs = list(filter(non_policy_reg.search, year_vars))\n",
    "    \n",
    "    return outputs, policy_inputs, non_policy_inputs\n",
    "\n",
    "years = [2011, 2013, 2015]\n",
    "# years = [2015]\n",
    "base_segment_variables = [\n",
    "    'use_extension_program___policy',\n",
    "    'prevent_damage___policy',\n",
    "    'hired_labor___policy___norm', \n",
    "    'oxen_owned___policy___norm', \n",
    "    'chemical_fertilizers_used___policy___norm',\n",
    "    'gender_equity',\n",
    "    'has_saved___policy',\n",
    "#     'land_surface',\n",
    "#     'plough_owned___policy',\n",
    "#     'household_size',\n",
    "]\n",
    "\n",
    "for base_output in ['crop_sales',]:\n",
    "#                     'crop_sales_growth',\n",
    "#                     'expenditure',\n",
    "#                     'food_expenditure_diversification',\n",
    "#                     'has_medical_assistance',\n",
    "#                     'no_food_deficiency',\n",
    "# #                     'productivity',\n",
    "# #                     'productivity_growth',\n",
    "#                     'children_education']:\n",
    "    ccs = pd.read_csv('/home/ananth/Downloads/cross_correls_ethiopia_2015_v22.csv')\n",
    "    output = base_output + '___output___ethiopia_2015'\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    # policy = ccs['Unnamed: 0'].apply(lambda x: x.find(\"___policy\") != -1)\n",
    "    # no_lives = ccs['Unnamed: 0'].apply(lambda x: x.find(\"lives_in\") == -1)\n",
    "    # no_lat = ccs['Unnamed: 0'].apply(lambda x: x.find(\"latitude\") == -1)\n",
    "    # no_dist = ccs['Unnamed: 0'].apply(lambda x: x.find(\"distance\") == -1)\n",
    "    # no_lon = ccs['Unnamed: 0'].apply(lambda x: x.find(\"longitude\") == -1)\n",
    "    # no_equ = ccs['Unnamed: 0'].apply(lambda x: x.find(\"equal\") == -1)\n",
    "    # ccs = ccs[policy & no_lives & no_lat & no_lon & no_equ & no_dist]\n",
    "    \n",
    "#     ### Choose from selected variables\n",
    "    select = ccs['Unnamed: 0'].str.contains('^.*(oxen|extension|hired|chemical|plough|land_surface|household_size)') # gender, has_saved\n",
    "    ccs = ccs[select]\n",
    "    ccs = ccs.sort_values(output, ascending=False)\n",
    "    num_vars=8\n",
    "    \n",
    "    best_vars = ccs[['Unnamed: 0', output]][:num_vars].as_matrix()\n",
    "    segment_variables = {}\n",
    "    seg_vars = []\n",
    "    for i in best_vars:\n",
    "        name = i[0]\n",
    "        if 'lives' in name or 'distance' in name:\n",
    "            continue\n",
    "        segment_variables[name] = abs(i[1])\n",
    "\n",
    "    print(segment_variables)\n",
    "\n",
    "    raw_df = df.copy()\n",
    "    df = complete(df)\n",
    "    for year in years:\n",
    "        base_suffix = '___ethiopia_{0}'.format(year)\n",
    "    #     segment_variables = [x+base_suffix for x in base_segment_variables]\n",
    "        year_segment_variables = {}\n",
    "        for k, v in segment_variables.items():\n",
    "            year_segment_variables[k.replace('2015', str(year))] = v\n",
    "        outputs, policy_inputs, non_policy_inputs = get_vars(year, base_output)\n",
    "        print(outputs)\n",
    "        raw_df = normalize()\n",
    "        for output in outputs:\n",
    "            df = get_clusters(df, output, year_segment_variables, out_dir='../outputs_v23_prod7_crop_sales')\n",
    "#             for c in [0.05]:\n",
    "#                 for v in [1.5]:\n",
    "#                     out_dir = '../grid_' + base_output + '/{0}/{1}'.format(c, v)\n",
    "#                     try:\n",
    "#                         os.makedirs(out_dir)\n",
    "#                     except:\n",
    "#                         print('File exists: ' + out_dir)\n",
    "#                     global all_relevant, policy_relevant\n",
    "#                     imp_feats = select_corr_feats(c)\n",
    "#                     all_relevant = vif(imp_feats, raw_df, v)\n",
    "#                     run_regressions(in_name=filename, out_dir=out_dir, non_policy_inputs=non_policy_inputs, segment_variables=segment_variables, inputs=policy_inputs, output=output, year=year)\n",
    "                    \n",
    "#                     imp_feats = select_corr_feats(c, True)\n",
    "#                     policy_relevant = vif(imp_feats, raw_df, v)\n",
    "#                     run_regressions(in_name=filename, out_dir=out_dir, non_policy_inputs=non_policy_inputs, segment_variables=segment_variables, inputs=policy_inputs, output=output, year=year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_df = read_csv('./ethiopia_v23_raw.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "y_reg = re.compile('.*___.*' + str(y) + '$')\n",
    "y_cols = list(filter(y_reg.search, imp_cols))\n",
    "imp_df = imp_df[y_cols]\n",
    "output = for_year('crop_sales___output', 2015)\n",
    "imp_df = imp_df.loc[imp_df[output].dropna().index]\n",
    "missing = (len(imp_df.index) - imp_df.count())/len(imp_df.index)*100.0\n",
    "missing[missing > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature choice for regression based on correl and vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: average_temperature___ethiopia_2015 : 18.472989798562818\n",
      "Removed: elevation___ethiopia_2015 : 13.182135429557304\n",
      "Removed: household_size___ethiopia_2015 : 7.279737966100593\n",
      "Removed: average_precipitation___ethiopia_2015 : 5.572895977396103\n",
      "Removed: household_head_is_male___ethiopia_2015 : 3.6755065060487273\n",
      "Removed: distance_to_population_center___ethiopia_2015 : 3.0143175152781425\n",
      "Removed: number_of_plough_owned___policy___ethiopia_2015 : 2.2687246376740005\n",
      "Removed: number_of_oxen_owned___policy___ethiopia_2015 : 2.006870652998064\n",
      "Removed: uses_extension_program___policy___ethiopia_2015 : 1.7790754637073296\n",
      "Removed: land_surface___ethiopia_2015 : 1.5292427289340587\n",
      "\n",
      "Final chosen features \n",
      "\n",
      "prevent_damage___policy___ethiopia_2015 1.19743605819\n",
      "number_of_hired_workers___policy___ethiopia_2015 1.07309856147\n",
      "quantity_of_chemical_fertilizers_used___policy___ethiopia_2015 1.03744894401\n",
      "has_saved___policy___ethiopia_2015 1.13110452009\n",
      "uses_irrigation___policy___ethiopia_2015 1.06132163347\n",
      "distance_to_market___ethiopia_2015 1.32750829077\n",
      "number_of_droughts___ethiopia_2015 1.07379545835\n",
      "Removed: number_of_plough_owned___policy___ethiopia_2015 : 2.0964472039050497\n",
      "Removed: uses_extension_program___policy___ethiopia_2015 : 1.8121315670631386\n",
      "\n",
      "Final chosen features \n",
      "\n",
      "prevent_damage___policy___ethiopia_2015 1.27033550951\n",
      "number_of_hired_workers___policy___ethiopia_2015 1.09626959341\n",
      "quantity_of_chemical_fertilizers_used___policy___ethiopia_2015 1.04582659867\n",
      "has_saved___policy___ethiopia_2015 1.07660741997\n",
      "uses_irrigation___policy___ethiopia_2015 1.03508670546\n",
      "number_of_oxen_owned___policy___ethiopia_2015 1.34689380808\n"
     ]
    }
   ],
   "source": [
    "imp_df = read_csv('./ethiopia_v23_raw.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "\n",
    "\n",
    "def select_all_year_feats():\n",
    "    imp_feats = set([])\n",
    "#     for y in [2013, 2015]:\n",
    "    for y in [2011, 2013, 2015]:\n",
    "        y_reg = re.compile('.*___policy.*' + str(y) + '$')\n",
    "        y_cols = set(filter(y_reg.search, imp_cols))\n",
    "        y_cols = set([t.replace('___ethiopia_' + str(y), '') for t in y_cols])\n",
    "        if len(imp_feats) == 0:\n",
    "            imp_feats = y_cols\n",
    "        else:\n",
    "            imp_feats = imp_feats.intersection(y_cols)\n",
    "    return imp_feats\n",
    "\n",
    "def select_corr_feats(thres = 0.1, only_policy=False):\n",
    "    ccs = pd.read_csv('/home/ananth/Downloads/cross_correls_ethiopia_2015_v22.csv')\n",
    "    output = for_year('crop_sales___output', 2015)\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    ccs = ccs[ccs[output] > thres]\n",
    "    imp_feats = ccs['Unnamed: 0']\n",
    "    non_raw_reg = re.compile('^((?!lives_in|longitude|latitude).)*$')\n",
    "    imp_feats = list(filter(non_raw_reg.search, imp_feats))\n",
    "    if only_policy:\n",
    "        y_reg = re.compile('.*___policy.*$')\n",
    "        imp_feats = list(filter(y_reg.search, imp_feats))\n",
    "    return imp_feats\n",
    "\n",
    "def normalize():\n",
    "    raw_df = read_csv('./ethiopia_v23_raw.csv')\n",
    "    output = for_year('crop_sales___output', 2015)\n",
    "    raw_df = raw_df.loc[raw_df[output].dropna().index]\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    return raw_df\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif_score\n",
    "\n",
    "def spearman():\n",
    "    collinear = raw_df[feats].corr('spearman')\n",
    "\n",
    "    cols = collinear.columns\n",
    "    for c in collinear.columns:\n",
    "        for v in zip(cols, collinear[c]):\n",
    "            if v[1] > 0.4 and v[1] < 0.5 and v[1] != 1:\n",
    "                print (c, v[0], v[1])\n",
    "    # collinear.to_csv('./output_collinear.csv')\n",
    "    \n",
    "def vif(feats, raw_df, thres=5, debug=False):    \n",
    "    while True:\n",
    "        policy = raw_df[feats].as_matrix()\n",
    "        max_vif = 0\n",
    "        max_vif_feat = None\n",
    "        for i, f in enumerate(feats):\n",
    "            if max_vif < vif_score(policy, i):\n",
    "                max_vif = vif_score(policy, i)\n",
    "                max_vif_feat = f\n",
    "        if max_vif < thres:\n",
    "            break\n",
    "        feat_set = set(feats)\n",
    "        feat_set.remove(max_vif_feat)\n",
    "        if debug:\n",
    "            print ('Removed: {0} : {1}'.format(max_vif_feat, max_vif))\n",
    "        feats = list(feat_set)\n",
    "    if debug:\n",
    "        print ('\\nFinal chosen features \\n')\n",
    "        for i, f in enumerate(feats):\n",
    "            print (f, vif_score(policy, i))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "raw_df = normalize()\n",
    "vifs = []\n",
    "# thresholds = [0.05, 0.07, 0.1, 0.12, 0.14,  0.15, 0.16, 0.17, 0.2,0.25,0.3]\n",
    "# thresholds = [2,3,4,5]\n",
    "# for thres in thresholds:\n",
    "#     imp_feats = select_corr_feats(0.1)\n",
    "#     v = vif(imp_feats, raw_df, thres)\n",
    "#     vifs += [len(v)]\n",
    "c = 0.1\n",
    "v = 1.5\n",
    "imp_feats = select_corr_feats(c)\n",
    "all_relevant = vif(imp_feats, raw_df, v, True)\n",
    "\n",
    "imp_feats = select_corr_feats(c, True)\n",
    "policy_relevant = vif(imp_feats, raw_df, v, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('./output_collinear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f17e62e97f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thresholds, vifs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['number_of_hired_workers___policy___ethiopia_2015',\n",
       "        0.2738082677365537],\n",
       "       ['land_surface___ethiopia_2015', 0.27251605259914585],\n",
       "       ['average_precipitation___ethiopia_2015', 0.2294706219248923],\n",
       "       ['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015',\n",
       "        0.16009055337326802],\n",
       "       ['number_of_oxen_owned___policy___ethiopia_2015',\n",
       "        0.15705349414011496],\n",
       "       ['prevent_damage___policy___ethiopia_2015', 0.15313691644432134],\n",
       "       ['gender_equity___policy___ethiopia_2015', 0.15266280975569105],\n",
       "       ['elevation___ethiopia_2015', 0.1522841010209675],\n",
       "       ['household_size___ethiopia_2015', 0.14258227392332665],\n",
       "       ['number_of_plough_owned___policy___ethiopia_2015',\n",
       "        0.14223988258113285],\n",
       "       ['uses_extension_program___policy___ethiopia_2015',\n",
       "        0.12981246190952828],\n",
       "       ['household_head_is_male___ethiopia_2015', 0.12555273470354533],\n",
       "       ['average_temperature___ethiopia_2015', 0.12206105019205225],\n",
       "       ['uses_irrigation___policy___ethiopia_2015', 0.1030213152351026],\n",
       "       ['has_saved___policy___ethiopia_2015', 0.102381473336001]], dtype=object)"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccs = pd.read_csv('cross_correls_ethiopia_2015_v22.csv')\n",
    "policy = ccs['Unnamed: 0'].apply(lambda x: x.find(\"___policy\") != -1)\n",
    "no_lives = ccs['Unnamed: 0'].apply(lambda x: x.find(\"lives_in\") == -1)\n",
    "no_lat = ccs['Unnamed: 0'].apply(lambda x: x.find(\"latitude\") == -1)\n",
    "no_dist = ccs['Unnamed: 0'].apply(lambda x: x.find(\"distance\") == -1)\n",
    "no_lon = ccs['Unnamed: 0'].apply(lambda x: x.find(\"longitude\") == -1)\n",
    "no_equ = ccs['Unnamed: 0'].apply(lambda x: x.find(\"equal\") == -1)\n",
    "ccs = ccs[no_lives & no_lat & no_lon & no_equ & no_dist]\n",
    "cols = ccs.columns.values\n",
    "best_var_series = {}\n",
    "for output in cols:\n",
    "    #output = 'average___ethiopia_2015'#'crop_sales___output___ethiopia_2015'#\n",
    "    if output == 'Unnamed: 0':\n",
    "        continue\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    ccs = ccs.sort_values(output, ascending=False)\n",
    "    best_vars = ccs[['Unnamed: 0', output]][:15].as_matrix()\n",
    "#     best_vars = [i[0] for i in best_vars]\n",
    "    best_var_series[output] = best_vars\n",
    "best_var_series['crop_sales___output___ethiopia_2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>children_education___output___ethiopia_2011</th>\n",
       "      <th>crop_sales___output___ethiopia_2011</th>\n",
       "      <th>expenditure___output___ethiopia_2011</th>\n",
       "      <th>food_expenditure_diversification___output___ethiopia_2011</th>\n",
       "      <th>has_medical_assistance___output___ethiopia_2011</th>\n",
       "      <th>no_food_deficiency___output___ethiopia_2011</th>\n",
       "      <th>productivity___output___ethiopia_2011</th>\n",
       "      <th>amount_of_assistance_received___policy___ethiopia_2011</th>\n",
       "      <th>crop_diversification___policy___ethiopia_2011</th>\n",
       "      <th>...</th>\n",
       "      <th>longitude___ethiopia_2015</th>\n",
       "      <th>number_of_droughts___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_mainly_non-soil___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_moderate_constraint___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_no_or_slight_constraint___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_severe_constraint___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_very_severe_constraint___ethiopia_2015</th>\n",
       "      <th>rural_household___ethiopia_2015</th>\n",
       "      <th>variations_in_greenness___ethiopia_2015</th>\n",
       "      <th>weight___ethiopia_2015</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3639.000000</td>\n",
       "      <td>2549.000000</td>\n",
       "      <td>1779.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3476.000000</td>\n",
       "      <td>3612.000000</td>\n",
       "      <td>3601.000000</td>\n",
       "      <td>1731.000000</td>\n",
       "      <td>3606.000000</td>\n",
       "      <td>2702.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1819.000000</td>\n",
       "      <td>0.656267</td>\n",
       "      <td>6.239407</td>\n",
       "      <td>6.658564</td>\n",
       "      <td>0.664381</td>\n",
       "      <td>0.266112</td>\n",
       "      <td>0.714524</td>\n",
       "      <td>-2.744503</td>\n",
       "      <td>0.333306</td>\n",
       "      <td>2.556995</td>\n",
       "      <td>...</td>\n",
       "      <td>38.454400</td>\n",
       "      <td>0.254937</td>\n",
       "      <td>0.043419</td>\n",
       "      <td>0.154713</td>\n",
       "      <td>0.501786</td>\n",
       "      <td>0.156362</td>\n",
       "      <td>0.142622</td>\n",
       "      <td>0.885408</td>\n",
       "      <td>43.302830</td>\n",
       "      <td>4319.752745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1050.633142</td>\n",
       "      <td>0.366369</td>\n",
       "      <td>1.526063</td>\n",
       "      <td>1.514439</td>\n",
       "      <td>0.254461</td>\n",
       "      <td>0.322030</td>\n",
       "      <td>0.451704</td>\n",
       "      <td>1.761184</td>\n",
       "      <td>1.405252</td>\n",
       "      <td>2.883183</td>\n",
       "      <td>...</td>\n",
       "      <td>2.034458</td>\n",
       "      <td>0.441705</td>\n",
       "      <td>0.203825</td>\n",
       "      <td>0.361680</td>\n",
       "      <td>0.500066</td>\n",
       "      <td>0.363248</td>\n",
       "      <td>0.349735</td>\n",
       "      <td>0.318573</td>\n",
       "      <td>12.656131</td>\n",
       "      <td>4187.333902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.616763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>33.468357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.844484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>909.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.273818</td>\n",
       "      <td>6.171991</td>\n",
       "      <td>0.532509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.885991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37.144541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1225.015137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1819.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.292388</td>\n",
       "      <td>6.904063</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.772643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38.311341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>3269.653564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2728.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.250832</td>\n",
       "      <td>7.524839</td>\n",
       "      <td>0.859984</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.652558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.650890</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>6107.384277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3638.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.684777</td>\n",
       "      <td>13.337703</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.189095</td>\n",
       "      <td>10.204448</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>43.870658</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>32753.236328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  children_education___output___ethiopia_2011  \\\n",
       "count  3639.000000                                  2549.000000   \n",
       "mean   1819.000000                                     0.656267   \n",
       "std    1050.633142                                     0.366369   \n",
       "min       0.000000                                     0.000000   \n",
       "25%     909.500000                                     0.500000   \n",
       "50%    1819.000000                                     0.750000   \n",
       "75%    2728.500000                                     1.000000   \n",
       "max    3638.000000                                     1.000000   \n",
       "\n",
       "       crop_sales___output___ethiopia_2011  \\\n",
       "count                          1779.000000   \n",
       "mean                              6.239407   \n",
       "std                               1.526063   \n",
       "min                               0.406284   \n",
       "25%                               5.273818   \n",
       "50%                               6.292388   \n",
       "75%                               7.250832   \n",
       "max                              10.684777   \n",
       "\n",
       "       expenditure___output___ethiopia_2011  \\\n",
       "count                           3639.000000   \n",
       "mean                               6.658564   \n",
       "std                                1.514439   \n",
       "min                                0.000000   \n",
       "25%                                6.171991   \n",
       "50%                                6.904063   \n",
       "75%                                7.524839   \n",
       "max                               13.337703   \n",
       "\n",
       "       food_expenditure_diversification___output___ethiopia_2011  \\\n",
       "count                                        3476.000000           \n",
       "mean                                            0.664381           \n",
       "std                                             0.254461           \n",
       "min                                             0.000000           \n",
       "25%                                             0.532509           \n",
       "50%                                             0.750000           \n",
       "75%                                             0.859984           \n",
       "max                                             1.000000           \n",
       "\n",
       "       has_medical_assistance___output___ethiopia_2011  \\\n",
       "count                                      3612.000000   \n",
       "mean                                          0.266112   \n",
       "std                                           0.322030   \n",
       "min                                           0.000000   \n",
       "25%                                           0.000000   \n",
       "50%                                           0.166667   \n",
       "75%                                           0.428571   \n",
       "max                                           1.000000   \n",
       "\n",
       "       no_food_deficiency___output___ethiopia_2011  \\\n",
       "count                                  3601.000000   \n",
       "mean                                      0.714524   \n",
       "std                                       0.451704   \n",
       "min                                       0.000000   \n",
       "25%                                       0.000000   \n",
       "50%                                       1.000000   \n",
       "75%                                       1.000000   \n",
       "max                                       1.000000   \n",
       "\n",
       "       productivity___output___ethiopia_2011  \\\n",
       "count                            1731.000000   \n",
       "mean                               -2.744503   \n",
       "std                                 1.761184   \n",
       "min                                -8.616763   \n",
       "25%                                -3.885991   \n",
       "50%                                -2.772643   \n",
       "75%                                -1.652558   \n",
       "max                                 4.189095   \n",
       "\n",
       "       amount_of_assistance_received___policy___ethiopia_2011  \\\n",
       "count                                        3606.000000        \n",
       "mean                                            0.333306        \n",
       "std                                             1.405252        \n",
       "min                                             0.000000        \n",
       "25%                                             0.000000        \n",
       "50%                                             0.000000        \n",
       "75%                                             0.000000        \n",
       "max                                            10.204448        \n",
       "\n",
       "       crop_diversification___policy___ethiopia_2011           ...            \\\n",
       "count                                    2702.000000           ...             \n",
       "mean                                        2.556995           ...             \n",
       "std                                         2.883183           ...             \n",
       "min                                         0.000000           ...             \n",
       "25%                                         0.000000           ...             \n",
       "50%                                         2.000000           ...             \n",
       "75%                                         4.000000           ...             \n",
       "max                                        16.000000           ...             \n",
       "\n",
       "       longitude___ethiopia_2015  number_of_droughts___ethiopia_2015  \\\n",
       "count                3639.000000                         3639.000000   \n",
       "mean                   38.454400                            0.254937   \n",
       "std                     2.034458                            0.441705   \n",
       "min                    33.468357                            0.000000   \n",
       "25%                    37.144541                            0.000000   \n",
       "50%                    38.311341                            0.000000   \n",
       "75%                    39.650890                            0.693147   \n",
       "max                    43.870658                            2.944439   \n",
       "\n",
       "       rooting_conditions_:_mainly_non-soil___ethiopia_2015  \\\n",
       "count                                        3639.000000      \n",
       "mean                                            0.043419      \n",
       "std                                             0.203825      \n",
       "min                                             0.000000      \n",
       "25%                                             0.000000      \n",
       "50%                                             0.000000      \n",
       "75%                                             0.000000      \n",
       "max                                             1.000000      \n",
       "\n",
       "       rooting_conditions_:_moderate_constraint___ethiopia_2015  \\\n",
       "count                                        3639.000000          \n",
       "mean                                            0.154713          \n",
       "std                                             0.361680          \n",
       "min                                             0.000000          \n",
       "25%                                             0.000000          \n",
       "50%                                             0.000000          \n",
       "75%                                             0.000000          \n",
       "max                                             1.000000          \n",
       "\n",
       "       rooting_conditions_:_no_or_slight_constraint___ethiopia_2015  \\\n",
       "count                                        3639.000000              \n",
       "mean                                            0.501786              \n",
       "std                                             0.500066              \n",
       "min                                             0.000000              \n",
       "25%                                             0.000000              \n",
       "50%                                             1.000000              \n",
       "75%                                             1.000000              \n",
       "max                                             1.000000              \n",
       "\n",
       "       rooting_conditions_:_severe_constraint___ethiopia_2015  \\\n",
       "count                                        3639.000000        \n",
       "mean                                            0.156362        \n",
       "std                                             0.363248        \n",
       "min                                             0.000000        \n",
       "25%                                             0.000000        \n",
       "50%                                             0.000000        \n",
       "75%                                             0.000000        \n",
       "max                                             1.000000        \n",
       "\n",
       "       rooting_conditions_:_very_severe_constraint___ethiopia_2015  \\\n",
       "count                                        3639.000000             \n",
       "mean                                            0.142622             \n",
       "std                                             0.349735             \n",
       "min                                             0.000000             \n",
       "25%                                             0.000000             \n",
       "50%                                             0.000000             \n",
       "75%                                             0.000000             \n",
       "max                                             1.000000             \n",
       "\n",
       "       rural_household___ethiopia_2015  \\\n",
       "count                      3639.000000   \n",
       "mean                          0.885408   \n",
       "std                           0.318573   \n",
       "min                           0.000000   \n",
       "25%                           1.000000   \n",
       "50%                           1.000000   \n",
       "75%                           1.000000   \n",
       "max                           1.000000   \n",
       "\n",
       "       variations_in_greenness___ethiopia_2015  weight___ethiopia_2015  \n",
       "count                              3639.000000             3639.000000  \n",
       "mean                                 43.302830             4319.752745  \n",
       "std                                  12.656131             4187.333902  \n",
       "min                                  12.000000                6.844484  \n",
       "25%                                  36.000000             1225.015137  \n",
       "50%                                  44.000000             3269.653564  \n",
       "75%                                  51.000000             6107.384277  \n",
       "max                                  68.000000            32753.236328  \n",
       "\n",
       "[8 rows x 210 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/ananth/Downloads/ethiopia_v23_normed.csv')\n",
    "df.describe()\n",
    "# df = df.loc[df['crop_sales___output___norm___ethiopia_2015'].dropna().index]\n",
    "# r = np.mean(df)\n",
    "# rdict = r.to_dict()\n",
    "# rdict['land_surface___norm___ethiopia_2015']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afar 0.0398055198279 2129.3261381\n",
      "amhara 25.8150368658 1155.75359963\n",
      "benishangul_gumuz 1.59543145271 2725.0879217\n",
      "dire_dawa 0.174135908025 1150.03649518\n",
      "gambella 0.31624091125 2110.87379671\n",
      "harari 0.180543438023 8034.89341087\n",
      "oromiya 41.046503324 1767.39513676\n",
      "snnp 26.6685963573 1036.59091902\n",
      "somalie 0.409996338586 1545.2165131\n",
      "tigray 3.75370988447 1898.11291005\n"
     ]
    }
   ],
   "source": [
    "imp_df\n",
    "locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "imp_df = imp_df.loc[imp_df[output].dropna().index]\n",
    "for l in sorted(locations):\n",
    "    loc_feature = 'lives_in_' + l + '___ethiopia_' + str(2015)\n",
    "    df_loc = imp_df.loc[imp_df[loc_feature]==1]\n",
    "    output = 'crop_sales___output___ethiopia_2015'  \n",
    "    print (l, sum(df_loc['weight___ethiopia_2015'])/sum(imp_df['weight___ethiopia_2015'])*100.0, np.average(df_loc[output], weights=df_loc['weight___ethiopia_2015']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.4168190128\n",
      "67.0932358318\n",
      "82.449725777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "489.09268211400331"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(imp_df['crop_sales___output___ethiopia_2015'].dropna(), [25,50,75])\n",
    "np.std(imp_df['crop_sales___output___ethiopia_2015'].dropna())\n",
    "from scipy.stats import percentileofscore\n",
    "x = [568.191517737871, 245.304134630863, 902.739225809435]\n",
    "for i in x:\n",
    "    print(percentileofscore(df[output + '_change_value' + year].dropna(), i))\n",
    "np.percentile(df[output + '_change_value' + year].dropna(), 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantity_of_improved_seeds_used___policy 0.0 2011\n",
      "quantity_of_improved_seeds_used___policy 0.0 2013\n",
      "quantity_of_improved_seeds_used___policy 0.0 2011\n",
      "quantity_of_improved_seeds_used___policy 0.0 2013\n",
      "quantity_of_improved_seeds_used___policy 0.0 2011\n",
      "quantity_of_improved_seeds_used___policy 0.0 2013\n",
      "quantity_of_improved_seeds_used___policy 0.0 2011\n",
      "quantity_of_improved_seeds_used___policy 0.0 2013\n",
      "price_rise_of_food_item___policy 0.0 2011\n",
      "price_rise_of_food_item___policy 0.0 2013\n",
      "price_rise_of_food_item___policy 0.0 2011\n",
      "price_rise_of_food_item___policy 0.0 2013\n",
      "price_rise_of_food_item___policy 0.0 2011\n",
      "price_rise_of_food_item___policy 0.0 2013\n",
      "price_rise_of_food_item___policy 0.0 2011\n",
      "price_rise_of_food_item___policy 0.0 2013\n",
      "owns_land_certificate___policy 0.0 2011\n",
      "owns_land_certificate___policy 0.0 2013\n",
      "owns_land_certificate___policy 0.0 2011\n",
      "owns_land_certificate___policy 0.0 2013\n",
      "owns_land_certificate___policy 0.0 2011\n",
      "owns_land_certificate___policy 0.0 2013\n",
      "owns_land_certificate___policy 0.0 2011\n",
      "owns_land_certificate___policy 0.0 2013\n",
      "amount_of_assistance_received___policy 0.0 2011\n",
      "amount_of_assistance_received___policy 0.0 2013\n",
      "amount_of_assistance_received___policy 0.0 2011\n",
      "amount_of_assistance_received___policy 0.0 2013\n",
      "amount_of_assistance_received___policy 0.0 2011\n",
      "amount_of_assistance_received___policy 0.0 2013\n",
      "amount_of_assistance_received___policy 0.0 2011\n",
      "amount_of_assistance_received___policy 0.0 2013\n",
      "number_of_hired_workers___policy 0.0 2011\n",
      "number_of_hired_workers___policy 0.0 2013\n",
      "number_of_hired_workers___policy 0.0 2011\n",
      "number_of_hired_workers___policy 0.0 2013\n",
      "number_of_hired_workers___policy 0.0 2011\n",
      "number_of_hired_workers___policy 0.0 2013\n",
      "number_of_hired_workers___policy -9.0 2011\n",
      "number_of_hired_workers___policy -8.0 2013\n",
      "prevent_damage___policy 0.0 2011\n",
      "prevent_damage___policy 0.0 2013\n",
      "prevent_damage___policy 0.0 2011\n",
      "prevent_damage___policy 0.0 2013\n",
      "prevent_damage___policy 0.0 2011\n",
      "prevent_damage___policy 0.0 2013\n",
      "prevent_damage___policy 0.0 2011\n",
      "prevent_damage___policy 0.0 2013\n",
      "crop_diversification___policy 0.0 2011\n",
      "crop_diversification___policy 0.0 2013\n",
      "crop_diversification___policy 0.0 2011\n",
      "crop_diversification___policy 0.0 2013\n",
      "crop_diversification___policy 0.0 2011\n",
      "crop_diversification___policy 0.0 2013\n",
      "crop_diversification___policy 0.0 2011\n",
      "crop_diversification___policy 0.0 2013\n",
      "number_of_sickle_owned___policy 0.0 2011\n",
      "number_of_sickle_owned___policy 0.0 2013\n",
      "number_of_sickle_owned___policy 0.0 2011\n",
      "number_of_sickle_owned___policy 0.0 2013\n",
      "number_of_sickle_owned___policy 0.0 2011\n",
      "number_of_sickle_owned___policy 0.0 2013\n",
      "number_of_sickle_owned___policy 0.0 2011\n",
      "number_of_sickle_owned___policy 0.0 2013\n",
      "uses_credit___policy 0.0 2011\n",
      "uses_credit___policy 0.0 2013\n",
      "uses_credit___policy 0.0 2011\n",
      "uses_credit___policy 0.0 2013\n",
      "uses_credit___policy 0.0 2011\n",
      "uses_credit___policy 0.0 2013\n",
      "uses_credit___policy 0.0 2011\n",
      "uses_credit___policy 0.0 2013\n",
      "uses_irrigation___policy 0.0 2011\n",
      "uses_irrigation___policy 0.0 2013\n",
      "uses_irrigation___policy 0.0 2011\n",
      "uses_irrigation___policy 0.0 2013\n",
      "uses_irrigation___policy 0.0 2011\n",
      "uses_irrigation___policy 0.0 2013\n",
      "uses_irrigation___policy 0.0 2011\n",
      "uses_irrigation___policy 0.0 2013\n",
      "number_of_water_storage_pit_owned___policy 0.0 2011\n",
      "number_of_water_storage_pit_owned___policy 0.0 2013\n",
      "number_of_water_storage_pit_owned___policy 0.0 2011\n",
      "number_of_water_storage_pit_owned___policy 0.0 2013\n",
      "number_of_water_storage_pit_owned___policy 0.0 2011\n",
      "number_of_water_storage_pit_owned___policy 0.0 2013\n",
      "number_of_water_storage_pit_owned___policy 0.0 2011\n",
      "number_of_water_storage_pit_owned___policy 0.0 2013\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2011\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2013\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2011\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2013\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2011\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2013\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2011\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2013\n",
      "number_of_pick_axe_owned___policy 0.0 2011\n",
      "number_of_pick_axe_owned___policy 0.0 2013\n",
      "number_of_pick_axe_owned___policy 0.0 2011\n",
      "number_of_pick_axe_owned___policy 0.0 2013\n",
      "number_of_pick_axe_owned___policy 0.0 2011\n",
      "number_of_pick_axe_owned___policy 0.0 2013\n",
      "number_of_pick_axe_owned___policy 0.0 2011\n",
      "number_of_pick_axe_owned___policy 0.0 2013\n",
      "uses_extension_program___policy 0.0 2011\n",
      "uses_extension_program___policy 0.0 2013\n",
      "uses_extension_program___policy 0.0 2011\n",
      "uses_extension_program___policy 0.0 2013\n",
      "uses_extension_program___policy 0.0 2011\n",
      "uses_extension_program___policy 0.0 2013\n",
      "uses_extension_program___policy 0.0 2011\n",
      "uses_extension_program___policy 0.0 2013\n",
      "percentage_of_damaged_crop___policy 0.0 2011\n",
      "percentage_of_damaged_crop___policy 2.05263157895 2013\n",
      "percentage_of_damaged_crop___policy -0.8125 2011\n",
      "percentage_of_damaged_crop___policy 5.75 2013\n",
      "percentage_of_damaged_crop___policy 0.0 2011\n",
      "percentage_of_damaged_crop___policy 6.0 2013\n",
      "percentage_of_damaged_crop___policy 0.0 2011\n",
      "percentage_of_damaged_crop___policy 4.01041666667 2013\n",
      "has_borrowed___policy 0.0 2011\n",
      "has_borrowed___policy 0.0 2013\n",
      "has_borrowed___policy 0.0 2011\n",
      "has_borrowed___policy 0.0 2013\n",
      "has_borrowed___policy 0.0 2011\n",
      "has_borrowed___policy 0.0 2013\n",
      "has_borrowed___policy 0.0 2011\n",
      "has_borrowed___policy 0.0 2013\n",
      "illness_of_household_member___policy 0.0 2011\n",
      "illness_of_household_member___policy 0.0 2013\n",
      "illness_of_household_member___policy 0.0 2011\n",
      "illness_of_household_member___policy 0.0 2013\n",
      "illness_of_household_member___policy 0.0 2011\n",
      "illness_of_household_member___policy 0.0 2013\n",
      "illness_of_household_member___policy 0.0 2011\n",
      "illness_of_household_member___policy 0.0 2013\n",
      "increase_in_price_of_inputs___policy 0.0 2011\n",
      "increase_in_price_of_inputs___policy 0.0 2013\n",
      "increase_in_price_of_inputs___policy 0.0 2011\n",
      "increase_in_price_of_inputs___policy 0.0 2013\n",
      "increase_in_price_of_inputs___policy 0.0 2011\n",
      "increase_in_price_of_inputs___policy 0.0 2013\n",
      "increase_in_price_of_inputs___policy 0.0 2011\n",
      "increase_in_price_of_inputs___policy 0.0 2013\n",
      "number_of_oxen_owned___policy 0.0 2011\n",
      "number_of_oxen_owned___policy 0.0 2013\n",
      "number_of_oxen_owned___policy 0.0 2011\n",
      "number_of_oxen_owned___policy 0.0 2013\n",
      "number_of_oxen_owned___policy 0.0 2011\n",
      "number_of_oxen_owned___policy 0.0 2013\n",
      "number_of_oxen_owned___policy 0.0 2011\n",
      "number_of_oxen_owned___policy 0.0 2013\n",
      "number_of_axe_owned___policy 0.0 2011\n",
      "number_of_axe_owned___policy 0.0 2013\n",
      "number_of_axe_owned___policy 0.0 2011\n",
      "number_of_axe_owned___policy 0.0 2013\n",
      "number_of_axe_owned___policy 0.0 2011\n",
      "number_of_axe_owned___policy 0.0 2013\n",
      "number_of_axe_owned___policy 0.0 2011\n",
      "number_of_axe_owned___policy 0.0 2013\n",
      "number_of_plough_owned___policy 0.0 2011\n",
      "number_of_plough_owned___policy 0.0 2013\n",
      "number_of_plough_owned___policy 0.0 2011\n",
      "number_of_plough_owned___policy 0.0 2013\n",
      "number_of_plough_owned___policy 0.0 2011\n",
      "number_of_plough_owned___policy 0.0 2013\n",
      "number_of_plough_owned___policy 0.0 2011\n",
      "number_of_plough_owned___policy 0.0 2013\n"
     ]
    }
   ],
   "source": [
    "outputs = ['segment_children_education___output',\n",
    "           'segment_crop_sales___output',\n",
    "           'segment_crop_sales_growth___output',\n",
    "           'segment_expenditure___output',\n",
    "           'segment_food_expenditure_diversification___output']\n",
    "\n",
    "# all_coef = pd.read_excel('./Tables_fixed_diversification/'+'coef_ethiopia.xlsx')\n",
    "\n",
    "df = df_all\n",
    "def for_year(var, year):\n",
    "    return var + '___ethiopia_' + str(year)\n",
    "\n",
    "\n",
    "imp_feats = select_all_year_feats()\n",
    "# ['use_extension_program___policy',\n",
    "#     'prevent_damage___policy',\n",
    "#     'hired_labor___policy', \n",
    "#     'oxen_owned___policy', \n",
    "#     'chemical_fertilizers_used___policy']\n",
    "series = pd.DataFrame()\n",
    "for imp_feat in imp_feats:\n",
    "    for output in ['segment_crop_sales___output']:\n",
    "        years = ['2011', '2013', '2015']\n",
    "#         years =['2013', '2015']\n",
    "        raw_output = 'crop_sales___output'\n",
    "        coef= {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}\n",
    "        for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "            #try:\n",
    "            z1 = for_year(output, y1)\n",
    "            z2 = for_year(output, y2)\n",
    "            r1 = for_year(raw_output, y1)\n",
    "            r2 = for_year(raw_output, y2)\n",
    "            f1 = for_year(imp_feat,y1)\n",
    "            f2 = for_year(imp_feat,y2)\n",
    "            df[output + '_change' + y1] = (df[z1]!=df[z2])\n",
    "            df[output + '_increase' + y1] = (df[z1]<df[z2])\n",
    "            df[output + '_decrease' + y1] = (df[z1]>df[z2])\n",
    "            expected = df[z1].apply(lambda x: coef[x])\n",
    "            df[imp_feat+'_increase'+y1] = (imp_df[f2]-imp_df[f1])\n",
    "            df[output + '_inversion' + y1] = (df[z2]-df[z1])*(df[f2]-df[f1])*expected\n",
    "            df[output + '_change_value' + y1] = (imp_df[r2]-imp_df[r1])\n",
    "            \n",
    "\n",
    "    # for output in ['segment_crop_sales___output']:\n",
    "    #     imp_feat = {'2011': 'distance_to_market___policy', '2013': 'prevent_damage___policy', '2015': 'hired_labor___policy'} \n",
    "    #     #imp_feat = {'2011': 'chemical_fertilizers_used___policy', '2013': 'chemical_fertilizers_used___policy', '2015': 'damaged_crop___policy'} \n",
    "    #     coef = {'2011': -0.168, '2013':+0.184, '2015': +0.182}\n",
    "    #     segment = 'hired_labor___policy'\n",
    "    #     df[output + '_change'] = False\n",
    "    #     from itertools import combinations\n",
    "    #     years = ['2011', '2013', '2015']\n",
    "    #     for y in years:\n",
    "    #         y_segment = for_year(segment, y)\n",
    "    #         df = df.loc[df[y_segment].dropna().index]\n",
    "    #         median_s = np.median(df[y_segment])\n",
    "    #         df['cluster'+y] = [int(x) for x in df[y_segment] > median_s]\n",
    "    #     df = df[df['cluster2011']==0]\n",
    "    #     for [y1, y2] in combinations(years, 2):\n",
    "    #         #try:\n",
    "    #         z1 = for_year(output, y1)\n",
    "    #         z2 = for_year(output, y2)\n",
    "    #         f1 = for_year(imp_feat[y1],y1)\n",
    "    #         f2 = for_year(imp_feat[y2],y2)\n",
    "    #         df[output + '_change' + y1] = ((df[z1]!=df[z2]) & (df[z1]!=6) & (df[z2]!=6))\n",
    "    #         df[output + '_inversion' + y1] = (df[z2]-df[z1])*(df[f2]-df[f1])*coef[y1]\n",
    "    # #         except:\n",
    "    # #             continue\n",
    "    #         year = y1\n",
    "        \n",
    "        thresholds = {'2011': {0: 582.074037761, 1: 932.333779703, 2: 585.830244031, 3: 1026.46232607},\n",
    "                      '2013': {0: 466.006285787, 1: 539.93544304, 2: 350.635222524, 3: 830.09513343}}\n",
    "#         thresholds = [489.1]*4\n",
    "#         df = df[df[output].dropna().index]\n",
    "        for per in [0]:#[0, 5,10,15,20,25,30,35,40,45,50,55,60,70,75,80,85,90,95]:\n",
    "            for seg in range(4):\n",
    "                exp_y_i = []\n",
    "                exp_y = []\n",
    "                exp = []\n",
    "                exp_i = []\n",
    "                avg_y_i = []\n",
    "                avg_y = []\n",
    "                std_y_i = []\n",
    "                std_y = []\n",
    "                \n",
    "                for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "                    dec_y = []\n",
    "                    num_dec_y = []\n",
    "                    dec_base_y = []\n",
    "                    year = y1\n",
    "                    weight = for_year('weight', year)\n",
    "    #                 thres = thresholds[year][seg]\n",
    "                    seg_y = for_year(output, year)\n",
    "                    df_seg = df[df[seg_y]==seg]\n",
    "                    df_seg = df_seg.loc[df_seg[output + '_change_value' + year].dropna().index]\n",
    "#                     thres = np.percentile(df_seg[output + '_change_value' + year].dropna(), per)\n",
    "    # #                 print (min(df_seg[output + '_change_value' + year].abs()))\n",
    "    #                 df_x = df_seg[(df_seg[output + '_change'+year]==True)]\n",
    "    #                 x = len(df_x)\n",
    "    # #                 y = len(df[(df[seg_y]==seg)])\n",
    "    #                 z = len(df.loc[df[output + '_change_value' + year].dropna().index])\n",
    "    #                 y = len(df_seg)\n",
    "#                     high_df = df_seg[df_seg[output + '_change_value' + year].apply(lambda x : x >= thres)]\n",
    "    #                 high_change = len(high_df)\n",
    "    #                 incr = len(df[(df[seg_y]==seg) & (df[output + '_increase'+year]==True) & (df[output + '_change_value' + year] > thres)])\n",
    "    #                 decr = len(df[(df[seg_y]==seg) & (df[output + '_decrease'+year]==True) & (df[output + '_change_value' + year] < -1.0*thres)])\n",
    "    #     #           print(df_x['segment_crop_sales___output_inversion'+year])\n",
    "    #                 c_mean = np.mean(df_seg[output + '_change_value' + year].as_matrix())\n",
    "    #                 c_stddev = np.std(df_seg[output + '_change_value' + year].as_matrix())\n",
    "    #                 a = len(high_df[(high_df[output + '_inversion'+year]>=0)])\n",
    "    #                 b = len(high_df[(high_df[output + '_inversion'+year]<0)])\n",
    "    \n",
    "#                     high_df = df_seg[(df_seg[imp_feat + '_increase'+year]>0)]\n",
    "                    high_df = df_seg\n",
    "                    std_dev = np.std(high_df[imp_feat + '_increase'+year])\n",
    "                    feat_mean = np.mean(high_df[imp_feat + '_increase'+year])\n",
    "                    def get_thresholds(v):\n",
    "                        if v < -std_dev:\n",
    "                            return 0\n",
    "                        elif v < 0:\n",
    "                            return 1\n",
    "                        elif v > std_dev:\n",
    "                            return 3\n",
    "                        elif v >= 0:\n",
    "                            return 2\n",
    "                    \n",
    "                    median = np.median(high_df[imp_feat + '_increase'+year].dropna())\n",
    "                    print (imp_feat, median, year)\n",
    "                    def get_simple_thresholds(v):\n",
    "                        if v < median:\n",
    "                            return 0\n",
    "                        elif v == median:\n",
    "                            return 1\n",
    "                        elif v > median:\n",
    "                            return 2\n",
    "                        else:\n",
    "                            return -1\n",
    "                \n",
    "                    high_df[imp_feat + '_decile'+year] = high_df[imp_feat + '_increase'+year].apply(lambda x: get_simple_thresholds(x))\n",
    "#                     high_df[imp_feat + '_decile'+year] = pd.cut(high_df[imp_feat + '_increase'+year], 4, labels=False) #, duplicates='drop')\n",
    "                    clus_avg = np.mean(df_seg[output + '_change_value' + year])\n",
    "                    dec_base_y = clus_avg\n",
    "                    for t in range(3):\n",
    "                        num_dec_y.append(len(high_df[high_df[imp_feat + '_decile'+year]==t]))\n",
    "                        dec_y.append(np.mean(high_df[high_df[imp_feat + '_decile'+year]==t][output + '_change_value' + year])/clus_avg)\n",
    "                    exp_y_i.append(len(high_df[(high_df[imp_feat + '_increase'+year]>0)]))\n",
    "                    exp_y.append(len(high_df))\n",
    "                    exp = len(df_seg)\n",
    "                    exp_i.append(len(df_seg[df_seg[imp_feat + '_increase'+year]>0]))\n",
    "                    avg_y_i.append(np.mean(high_df[(high_df[imp_feat + '_increase'+year]>0)][output + '_change_value' + year]))\n",
    "                    avg_y.append(np.mean(high_df[output + '_change_value' + year].dropna()))\n",
    "                    std_y_i.append(np.std(high_df[(high_df[imp_feat + '_increase'+year]>0)][output + '_change_value' + year]))\n",
    "                    std_y.append(np.std(high_df[output + '_change_value' + year].dropna()))\n",
    "                    avg = np.median(df[output + '_change_value' + year].dropna())\n",
    "                    row = {}\n",
    "#                     row['threshold'] = per\n",
    "                    row['input'] = imp_feat\n",
    "                    row['from'] = year\n",
    "                    row['to'] = y2\n",
    "                    row['cluster #'] = seg\n",
    "    #                 row['movement overall'] = (sum(exp_y)/sum(exp))*100.0\n",
    "    #                 row['movement conditioned'] = (sum(exp_y_i)/sum(exp_i))*100.0\n",
    "    #                 row['movement lift'] = (sum(exp_y_i)/sum(exp_i))/(sum(exp_y)/sum(exp))\n",
    "                    row['num_hh_in_cluster'] = exp\n",
    "    #                 row['num_hh_with_increase_in_input'] = sum(exp_i)\n",
    "                    row['num_hh_in_bins'] = str(num_dec_y)\n",
    "                    row['binned_conditional_lift_ratios'] = str(dec_y) \n",
    "                    row['average_change_in_output'] = dec_base_y\n",
    "                \n",
    "#                 row['conditional_ratio_of_averages'] = np.mean([j/k for j, k in zip(avg_y_i, avg_y)])\n",
    "#                 row['population_average_change'] = str(avg_y)\n",
    "#                 row['conditional_average_change'] = str(avg_y_i)\n",
    "#                 row['population_std_change'] = str(std_y)\n",
    "#                 row['conditional_std_change'] = str(std_y_i)\n",
    "#                 row['raw lift'] = avg_y_i/avg_y\n",
    "                    new_row = pd.DataFrame(row, index=[0])\n",
    "                    series = series.append(new_row, ignore_index=True)\n",
    "\n",
    "series.to_csv('./threshold-lift-simple.csv')\n",
    "    #                 print (year, seg, np.percentile(df_seg[output + '_change_value' + year].dropna(), 75))\n",
    "    #                 _i = np.mean(df_seg[df_seg[imp_feat + '_increase'+year]==True][output + '_change_value' + year])\n",
    "    #                 print (imp_feat, year, y2, seg, (a/(a+b))*100.0, x/y*100.0, high_change/y*100.0, incr/y*100.0, decr/y*100.0, y/z*100.0, y, x, a, c_mean, c_stddev)\n",
    "#                     print (per, imp_feat, year, y2, seg, exp_i, (exp_y/exp)*100.0, (exp_y_i/exp_i)*100.0, (exp_y_i/exp_i)/(exp_y/exp), avg_y_i/avg_y) \n",
    "                \n",
    "#                 seg_y = for_year(output, year)\n",
    "#                 df_x = imp_df[(df[seg_y]==seg) & (df[output + '_change'+year]==True)]\n",
    "#                 incr = sum(imp_df[(df[seg_y]==seg) & (df[output + '_increase'+year]==True)][weight])\n",
    "#                 decr = sum(imp_df[(df[seg_y]==seg) & (df[output + '_decrease'+year]==True)][weight])\n",
    "#                 x = sum(df_x[weight])\n",
    "#                 y = sum(imp_df[(df[seg_y]==seg)][weight])\n",
    "#                 z = sum(imp_df[weight])\n",
    "#                 df_seg = df[df[seg_y]==seg]\n",
    "#                 df_seg = df_seg.loc[df_seg[output + '_change_value' + year].dropna().index]\n",
    "#                 imp_seg = imp_df.loc[df[df[seg_y]==seg][output + '_change_value' + year].dropna().index]\n",
    "#     #             print(df_x['segment_crop_sales___output_inversion'+year])\n",
    "#                 thres = 1.0\n",
    "#                 high_change = len(df_seg[output + '_change_value' + year].abs().apply(lambda x : x > thres))\n",
    "#                 values = df_seg[output + '_change_value' + year].as_matrix()\n",
    "#                 c_mean = np.average(values, weights=imp_seg[weight])\n",
    "#                 average = c_mean\n",
    "#                 variance = np.average((values-average)**2, weights=imp_seg[weight])\n",
    "#                 c_stddev = math.sqrt(variance)/math.sqrt(len(values))\n",
    "#                 a = sum(imp_df[(df[seg_y]==seg) & (df[output + '_change'+year]==True) & (df[output + '_inversion'+year]>=0)][weight])\n",
    "#                 b = sum(imp_df[(df[seg_y]==seg) & (df[output + '_change'+year]==True) & (df[output + '_inversion'+year]<0)][weight])\n",
    "#                 print (imp_feat, year, y2, seg, (a/(a+b))*100.0, x/y*100.0, high_change/y*100.0, incr/y*100.0, decr/y*100.0, y/z*100.0, c_mean, c_stddev)\n",
    "\n",
    "# df['any_change']= False\n",
    "# for output in outputs:\n",
    "#     df['any_change'] |= (df[output+'_change']==True)\n",
    "\n",
    "\n",
    "# a = len(df[(df['segment_crop_sales___output_change2011']==True) & (df['segment_crop_sales___output_inversion2011']>=0)])\n",
    "# b = len(df[(df['segment_crop_sales___output_change2011']==True) & (df['segment_crop_sales___output_inversion2011']<0)])\n",
    "# c = len(df[(df['segment_crop_sales___output_change2013']==True) & (df['segment_crop_sales___output_inversion2013']>=0)])\n",
    "# d = len(df[(df['segment_crop_sales___outavg_yput_change2013']==True) & (df['segment_crop_sales___output_inversion2013']<0)])\n",
    "# [a/(a+b), c/(c+d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.7433489827856025\n",
      "8.450704225352112\n",
      "29.42097026604069\n",
      "24.49139280125196\n",
      "3.7167449139280127\n",
      "33.17683881064163\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(len(df[df['segment_crop_sales___output___ethiopia_2011'] == i])/len(df)*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.596042868920033"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-len(df[df['cluster2011']==0])/len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9280125195618153"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2372/(2372+184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
       "            ...\n",
       "            3628, 3629, 3630, 3631, 3632, 3633, 3635, 3636, 3637, 3638],\n",
       "           dtype='int64', length=3608)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_1_2011 = a/(a+b)\n",
    "t_1_2013 = c/(c+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-9faf8e73a38b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
