{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import Imputer\n",
    "from fancyimpute import SoftImpute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate cross correlation file\n",
    "imp_df = read_csv('/home/ananth/Downloads/tanzania_2014_v8_cleaned.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "# corr = imp_df.corr('spearman')\n",
    "corr = imp_df.corr('spearman')['crop_sales___output']\n",
    "corr.to_csv('tanzania_corr.csv')\n",
    "#Inspect this file manually and see if all features are written correctly and add an extra row on top with the column name:crop_sales___output___tanzania_2014 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y4_hhid                                   0.000000\n",
      "children_education___output              15.535957\n",
      "crop_sales___output                       0.000000\n",
      "expenditure___output                      0.000000\n",
      "has_medical_assistance___output           0.000000\n",
      "no_food_deficiency___output               0.135685\n",
      "crop_diversification___policy             4.002714\n",
      "has_bank_account___policy                 0.000000\n",
      "has_borrowed___policy                     0.000000\n",
      "has_hired_workers___policy               29.036635\n",
      "months_hired_workers___policy             0.000000\n",
      "number_of_animals_owned___policy         33.175034\n",
      "number_of_hired_workers___policy         44.979647\n",
      "number_of_ploughs_owned___policy          0.000000\n",
      "owns_land_certificate___policy           19.063772\n",
      "quantity_of_fertilizers_used___policy     0.067843\n",
      "quantity_of_pesticides_used___policy      0.067843\n",
      "uses_credit___policy                      0.000000\n",
      "uses_irrigation___policy                  0.949796\n",
      "attended_school                           0.000000\n",
      "distance_to_road                          0.067843\n",
      "distance_to_market                        0.067843\n",
      "household_head_age                        0.000000\n",
      "household_head_is_divorced                0.000000\n",
      "household_head_is_male                    0.000000\n",
      "household_head_is_monogamous              0.000000\n",
      "household_head_is_polygamous              0.000000\n",
      "household_head_is_separated               0.000000\n",
      "household_head_is_widowed                 0.000000\n",
      "household_head_never_married              0.000000\n",
      "                                           ...    \n",
      "lives_in_geita                            0.000000\n",
      "lives_in_iringa                           0.000000\n",
      "lives_in_kagera                           0.000000\n",
      "lives_in_kaskazinipemba                   0.000000\n",
      "lives_in_kaskaziniunguja                  0.000000\n",
      "lives_in_katavi                           0.000000\n",
      "lives_in_kigoma                           0.000000\n",
      "lives_in_kilimanjaro                      0.000000\n",
      "lives_in_kusinipemba                      0.000000\n",
      "lives_in_kusiniunguja                     0.000000\n",
      "lives_in_manyara                          0.000000\n",
      "lives_in_mara                             0.000000\n",
      "lives_in_mbeya                            0.000000\n",
      "lives_in_mjinimagharibi                   0.000000\n",
      "lives_in_morogoro                         0.000000\n",
      "lives_in_mtwara                           0.000000\n",
      "lives_in_mwanza                           0.000000\n",
      "lives_in_njombe                           0.000000\n",
      "lives_in_pwani                            0.000000\n",
      "lives_in_rukwa                            0.000000\n",
      "lives_in_ruvuma                           0.000000\n",
      "lives_in_shinyanga                        0.000000\n",
      "lives_in_simiyu                           0.000000\n",
      "lives_in_singida                          0.000000\n",
      "lives_in_tabora                           0.000000\n",
      "lives_in_tanga                            0.000000\n",
      "rural_household                           0.000000\n",
      "weight                                    0.000000\n",
      "latitude                                  0.000000\n",
      "longitude                                 0.000000\n",
      "Length: 66, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "imp_df = read_csv('/home/ananth/Downloads/tanzania_2014_v8_cleaned.csv')\n",
    "cdf = imp_df.loc[imp_df['crop_sales___output'].dropna().index] #___tanzania_2014\n",
    "# Print missingness values for relevant inputs.\n",
    "print(100-(cdf.apply(lambda x: x.count(), axis=0)/len(cdf))*100.0)\n",
    "\n",
    "#Plot variations of input features w.r.t output to pick variables to cluster on.\n",
    "varies = imp_df.groupby(pd.qcut(imp_df['crop_sales___output'],5,duplicates='drop')).mean()\n",
    "plt.clf()\n",
    "varies.plot(x='crop_sales___output', subplots=True,legend=True, figsize=(50,200),kind='bar',fontsize=20)\n",
    "plt.savefig('variations.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset matplotlib defaults if plots are skewed after generating variations.pdf.\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from scipy import stats\n",
    "import glmnet_python\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot \n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "\n",
    "country = 'tanzania'\n",
    "\n",
    "def get_classes(output_var, pred):\n",
    "    max_bins = 3\n",
    "    _, boundaries = np.histogram(output_var, bins=max_bins)\n",
    "    classes = np.digitize(pred, bins=boundaries)\n",
    "    return classes, max_bins\n",
    "\n",
    "def for_year(var, year):\n",
    "    return var + '___' + country + '_' + str(year)\n",
    "\n",
    "def run_regressions(fixed_k, in_name, out_dir, non_policy_inputs, segment_variables, inputs, output, year):\n",
    "    global table\n",
    "    global coef_table\n",
    "    global avg_table\n",
    "    global coef_map\n",
    "    # table for regressions and classification\n",
    "    table = pd.DataFrame()\n",
    "    avg_table = pd.DataFrame()\n",
    "    coef_map = {}\n",
    "    # create table of coefficients\n",
    "    coef_table = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except:\n",
    "        print(\"Dir exists\")\n",
    "    \n",
    "    ols = linear_model.LinearRegression()\n",
    "    ridge = linear_model.Ridge(alpha=.5)\n",
    "    lasso = linear_model.Lasso(alpha = 0.001, max_iter=1e5)\n",
    "    lars_lasso = linear_model.LassoLars(alpha=.1)\n",
    "    bayes_ridge = linear_model.BayesianRidge()\n",
    "    sgd = linear_model.SGDRegressor()\n",
    "    svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    svr_lin = SVR(kernel='linear', C=1e3)\n",
    "    svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "    kernel_ridge = KernelRidge(alpha=1.0)\n",
    "\n",
    "    # pick which regression algos to use\n",
    "    regression_algorithms = (\n",
    "#        ('OrdinaryLeastSquares', ols),\n",
    "#        ('RidgeRegression', ridge),\n",
    "        ('Lasso', lasso),\n",
    "#         ('LARS Lasso', lars_lasso),\n",
    "#         ('BayesianRidgeRegression', bayes_ridge),\n",
    "#         ('StochasticGradientDescent', sgd),\n",
    "#         ('SupportVectorRegressionRBF', svr_rbf),\n",
    "#         ('SupportVectorRegressionLinear', svr_lin),\n",
    "#         ('SupportVectorRegressionPolynomial', svr_poly),\n",
    "#         ('KernelRidgeRegression', kernel_ridge)\n",
    "    )\n",
    "    \n",
    "    df = read_csv(in_name)\n",
    "    df = df.loc[df[output].dropna().index] # drop rows with unobserved income\n",
    "    df = df.loc[df['weight'].dropna().index]\n",
    "    df = df.loc[df[output] != 0] # drop zero outputs\n",
    "    # Transform input\n",
    "    logged_inputs = ['crop_sales___output', 'expenditure___output', 'crop_diversification___policy', 'number_of_animals_owned___policy', 'number_of_hired_workers___policy', 'quantity_of_fertilizers_used___policy', 'quantity_of_pesticides_used___policy', 'distance_to_road', 'distance_to_market', 'household_size', 'land_surface']\n",
    "    for inp in logged_inputs:\n",
    "        df[inp] = df[inp].apply(lambda x: np.log(1+x))\n",
    "\n",
    "    df['nid']= df.index.tolist()\n",
    "    \n",
    "    # Uncomment to add filters based on either gender or location or zero outputs.\n",
    "    #     df = df.loc[df['household_head_is_male___ethiopia_2015']==0]\n",
    "    #     filter_var = 'lives_in_amhara___ethiopia_' + str(year)\n",
    "    #     df = df[df[filter_var]==True]\n",
    "    #     df = df.loc[df[output] != 0] # drop zero outputs\n",
    "\n",
    "    # select % of data in test set\n",
    "    test_split = 0.2\n",
    "    \n",
    "    # perform matrix completion. completed is returned as a np array\n",
    "    # we've discussed not using it, but I left it in because I wasn't able to\n",
    "    # fit the StandardScaler with a DataFrame that contained NaN's\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "    \n",
    "    # reconstruct dataframe with completed matrix\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    \n",
    "    # Redo the same, but without the transformations, only matrix completion for raw matrix.\n",
    "    raw_df = read_csv(in_name)\n",
    "    raw_df = raw_df.loc[raw_df[output].dropna().index]\n",
    "    raw_df = raw_df.loc[raw_df['weight'].dropna().index]\n",
    "    raw_df = raw_df.loc[raw_df[output] != 0] # drop zero outputs\n",
    "    raw_df['nid']= raw_df.index.tolist()\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    raw_df['productivity'] = raw_df['crop_sales___output']/raw_df['land_surface']\n",
    "    raw_df['productivity'] = raw_df['productivity'].apply(lambda x: 0 if x == np.inf else x)\n",
    "\n",
    "    # z-score the matrix mat used for clustering/regression.\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    \n",
    "    y = mat[output]\n",
    "    x = mat[inputs]\n",
    "    \n",
    "    def update_best_lambda(x_scaled):\n",
    "        global regression_algorithms\n",
    "        copy_y = np.array(y, dtype=np.float64)\n",
    "        print (copy_y)\n",
    "        fit = cvglmnet(x = x_scaled.copy(), y = copy_y)\n",
    "        print(fit['lambda_min'])\n",
    "        regression_algorithms = (('Lasso', linear_model.Lasso(alpha=fit['lambda_min'], max_iter=1e5)))\n",
    "    \n",
    "    # Split test/train\n",
    "    indices = range(len(mat))\n",
    "    x_train, x_test, y_train, y_test, ind_train, ind_test = \\\n",
    "        train_test_split(x, y, indices, test_size=test_split, random_state=42)\n",
    "    \n",
    "    def get_train_test(input_vars):\n",
    "        x = mat[input_vars].copy()\n",
    "        x_scaled = StandardScaler()\n",
    "        x_scaled.fit(x)\n",
    "        x_sc = x_scaled.transform(x)\n",
    "        # reconstruct DataFrame\n",
    "        x = pd.DataFrame(x_sc, columns=x.columns)\n",
    "        training_x = x.iloc[ind_train, :]\n",
    "        testing_x = x.iloc[ind_test, :]\n",
    "        return x_sc, training_x, testing_x\n",
    "    \n",
    "    def digitize(output_var, pred):\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        classes, max_bins = get_classes(output_var, pred)\n",
    "        b_classes = label_binarize(classes, range(max_bins))\n",
    "        return b_classes\n",
    "    \n",
    "    def calc_unsegmented(baseline, x_train, x_test): \n",
    "        global table\n",
    "        global coef_map\n",
    "        # reg keeps predictions from regressions along with keys\n",
    "        reg = dict()\n",
    "        for name, algo in regression_algorithms:\n",
    "            reg[name] = {}\n",
    "\n",
    "        # keys and values from test data\n",
    "        keys_list = []\n",
    "        y_list = []\n",
    "        for k, v in y_test.iteritems():\n",
    "            keys_list.append(k)\n",
    "            y_list.append(v)\n",
    "\n",
    "        # run regressions on full dataset\n",
    "        for name, algo in regression_algorithms:\n",
    "#             model = algo.fit(x_train,y_train)\n",
    "#             y_pred = model.predict(x_test)\n",
    "            model = sm.OLS(y_train, x_train)\n",
    "            fit = model.fit_regularized(alpha=1e-5, refit=True)\n",
    "            y_pred = fit.predict(x_test)\n",
    "\n",
    "            # add predictions to dict\n",
    "            for i, p in enumerate(y_pred):\n",
    "                t = reg[name]\n",
    "                t[keys_list[i]] = p\n",
    "\n",
    "            try:\n",
    "                test_c = digitize(y, y_test)\n",
    "                pred_c = digitize(y, y_pred)\n",
    "                auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "            except ValueError:\n",
    "                auc_c = 0.5\n",
    "            mse = mean_squared_error(y_test,y_pred)\n",
    "            scaled_mse = (mse/np.std(y))\n",
    "\n",
    "            # add row to table\n",
    "            new_row = pd.DataFrame({'model': name, 'segment': '', 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': False}, index=[0])\n",
    "            table = table.append(new_row, ignore_index=True)\n",
    "\n",
    "            # add coefficients to map\n",
    "#             coef_map[name + '_' + baseline] = model.coef_\n",
    "            coef_map[name + '_' + baseline] = fit.params\n",
    "            lower_bounds = []\n",
    "            upper_bounds = []\n",
    "            for ci in fit.conf_int():\n",
    "                lower_bounds += [ci[0]]\n",
    "                upper_bounds += [ci[1]]\n",
    "            coef_map[name + '_' + baseline + '_lower_bound'] = lower_bounds\n",
    "            coef_map[name + '_' + baseline + '_upper_bound'] = upper_bounds\n",
    "    \n",
    "    def calc_segmented(segment_variables, baseline, x_train, x_test):\n",
    "        global table\n",
    "        global coef_map\n",
    "        global avg_table\n",
    "        segment_vars = list(segment_variables.keys())\n",
    "        \n",
    "        def add_clusters():\n",
    "            # elbow method\n",
    "            sse = []\n",
    "            seg_data = mat[segment_vars]\n",
    "            for seg_var in segment_vars:\n",
    "                # Uncomment if clusters' features need to be weighted.\n",
    "                # seg_data[seg_var] = seg_data[seg_var]*mat[for_year('weight', year)]\n",
    "                # Multiply covariance weights so that features of the clusters are oriented towards output.\n",
    "                seg_data[seg_var] = seg_data[seg_var].apply(lambda x: x*segment_variables[seg_var])\n",
    "            for k in range(1,9):\n",
    "                kmeans = KMeans(n_clusters=k).fit(seg_data)\n",
    "                labels = kmeans.labels_\n",
    "                sse.append(sum(np.min(cdist(seg_data, kmeans.cluster_centers_, 'euclidean'), axis=1)) / seg_data.shape[0])\n",
    "\n",
    "            # K-means elbow calculation\n",
    "            plt.clf()\n",
    "            plt.plot(range(1,9), sse)\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel('Sum of squared error')\n",
    "            plt.savefig(os.path.join(out_dir, 'elbow.png'))\n",
    "            print(sse)\n",
    "            min_k = sse.index(min(sse))\n",
    "            print (min_k)\n",
    "            \n",
    "            # K-means fixed K calculation.\n",
    "            min_k = fixed_k\n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "\n",
    "            labels = kmeans.labels_\n",
    "            mat['cluster'] = labels\n",
    "            # Sort cluster labels in order of mean of output within cluster.\n",
    "            means = []\n",
    "            for i in np.unique(labels):\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "                values = list(raw_clus[output].as_matrix())\n",
    "                # Weighted average can be done if the weight column exists.\n",
    "                weights = list(raw_clus['weight'].as_matrix())\n",
    "#                 print (values)\n",
    "#                 print(weights)\n",
    "                average = np.average(values, weights=weights)\n",
    "                means.append(average)\n",
    "            sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "            print(sorted_ids)\n",
    "            mat['cluster'] = mat['cluster'].apply(lambda x: sorted_ids.index(x))\n",
    "\n",
    "            # Output ids of households, their cluster number, variables on which cluster is done along with lat/long if exists\n",
    "            # select_variables += ['latitude___ethiopia_2015', 'longitude___ethiopia_2015']\n",
    "            # Note (Sam): This is where the file I give you with Ids, cluster numbers is written.\n",
    "            # The baseline = relevant variables\n",
    "            select_variables = [output, 'y4_hhid'] + segment_vars\n",
    "            select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "            all_output = pd.concat([mat['cluster'], raw_df[select_variables]], 1)\n",
    "            all_output.to_csv(os.path.join(out_dir,'clus_' + baseline + '_' + output + '.csv'))\n",
    "            return min_k\n",
    "            \n",
    "        \n",
    "        # Add segments based on median.\n",
    "        def add_segments():\n",
    "            median_segments = {}\n",
    "            for seg_var in segment_vars:\n",
    "                #binary\n",
    "                if len(np.unique(mat[seg_var])) <= 3:\n",
    "                    median_segments[seg_var] = 0\n",
    "                else:\n",
    "                    median_segments[seg_var] = np.median(mat[seg_var])\n",
    "            mat['cluster'] = 0\n",
    "            for seg_var in segment_vars:\n",
    "                mat['cluster'] = 2*mat['cluster'] + [int(x) for x in mat[seg_var] > median_segments[seg_var]]\n",
    "            #mat[['cluster'] + segment_vars].to_csv(os.path.join(out_dir,'segment_' + ','.join(segment_vars) + '_' + baseline + '_' + output + '.csv'))\n",
    "            return int(math.pow(2, len(segment_vars)))\n",
    "\n",
    "        # Segments based solely on location.\n",
    "        def add_location_segments():\n",
    "            locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "            i = 0\n",
    "            mat['cluster'] = 0\n",
    "            for l in sorted(locations):\n",
    "                loc_feature = 'lives_in_' + l + '___ethiopia_' + str(year)\n",
    "                loc_val = mat[loc_feature].apply(lambda x: 0 if x < 0 else 1)\n",
    "                mat['cluster'] = mat['cluster'] + (i*loc_val)\n",
    "                i += 1\n",
    "            return len(locations) + 1\n",
    "            \n",
    "        def _run(max_clusters, method_name):\n",
    "            global table\n",
    "            global avg_table\n",
    "            global coef_map\n",
    "            # reg_clus keeps predictions from clustered regressions along with keys\n",
    "            reg_clus = dict()\n",
    "\n",
    "            for name, algo in regression_algorithms:\n",
    "                reg_clus[name] = {}\n",
    "\n",
    "            # need new dataframes with only training and test rows.\n",
    "            # we use this when looping through clusters\n",
    "            train_mat = mat.loc[ind_train]\n",
    "            test_mat = mat.loc[ind_test]\n",
    "            train_size = len(train_mat)\n",
    "            \n",
    "            series = {}\n",
    "            series[output] = []\n",
    "            for seg in segment_vars:\n",
    "                series[seg] = []\n",
    "            series = pd.DataFrame()\n",
    "            row = {}\n",
    "            raw_cols = raw_df.columns.values\n",
    "            raw_reg = re.compile('^((?!norm).)*$') #+ str(year) +\n",
    "            # avg_variables is set of all variables whose mean, 25%ile, 75%ile, stddev, stderr stats are written to *_avg file.\n",
    "            avg_variables = list(filter(raw_reg.search, raw_cols))\n",
    "            for i in range(max_clusters):\n",
    "                train_clus = x_train.loc[train_mat['cluster'] == i]\n",
    "                train_y = y_train.loc[train_mat['cluster'] == i]\n",
    "                test_clus = x_test.loc[test_mat['cluster'] == i]\n",
    "                test_y = y_test.loc[test_mat['cluster'] == i]\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "\n",
    "                for seg in avg_variables:\n",
    "                    values = raw_clus[seg].as_matrix()\n",
    "                    # Weighted average if weight column exists.\n",
    "                    weights = raw_clus['weight'].as_matrix()\n",
    "                    average = np.average(values, weights=weights)\n",
    "                    row['mean_' + seg] = average\n",
    "#                     print (average)\n",
    "                    variance = np.average((values-average)**2, weights=weights)\n",
    "                    row['stddev_' + seg] = math.sqrt(variance)\n",
    "                    row['stderr_' + seg] = math.sqrt(variance)/math.sqrt(len(values))\n",
    "                    row['25ile_' + seg] = np.percentile(values, 25)\n",
    "                    row['75ile_' + seg] = np.percentile(values, 75)\n",
    "                \n",
    "                row['index'] = i\n",
    "                row['size'] = len(raw_clus)\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "                avg_table = avg_table.append(new_row, ignore_index=True)\n",
    "                cluster_percent = (len(train_clus)*100.0)/train_size\n",
    "                if train_clus.empty or test_clus.empty:\n",
    "                    continue\n",
    "\n",
    "                keys_list = []\n",
    "                y_list = []\n",
    "                for k, v in test_y.iteritems():\n",
    "                    keys_list.append(k)\n",
    "                    y_list.append(v)\n",
    "\n",
    "                # Regress per cluster\n",
    "                for name, algo in regression_algorithms:  \n",
    "#                     model = algo.fit(train_clus,train_y)\n",
    "#                     y_pred = model.predict(test_clus)\n",
    "                    model = sm.OLS(train_y, train_clus)\n",
    "                    fit = model.fit_regularized(alpha=1e-5, refit=True)\n",
    "                    y_pred = fit.predict(test_clus)\n",
    "\n",
    "                    for a, b in enumerate(y_pred):\n",
    "                        t = reg_clus[name]\n",
    "                        t[keys_list[a]] = b\n",
    "\n",
    "#                     coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = model.coef_\n",
    "                    coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = fit.params\n",
    "                    lower_bounds = []\n",
    "                    upper_bounds = []\n",
    "                    for ci in fit.conf_int():\n",
    "                        lower_bounds += [ci[0]]\n",
    "                        upper_bounds += [ci[1]]\n",
    "                    coef_map[name + '_' + method_name + '_' + str(i) + '_lower_bound'] = lower_bounds\n",
    "                    coef_map[name + '_' + method_name + '_' + str(i) + '_upper_bound'] = upper_bounds\n",
    "\n",
    "            # plot sorted correlation\n",
    "            sorted_series = series.sort_values(['mean_' + output])\n",
    "#             print (sorted_series)\n",
    "            for seg in avg_variables:\n",
    "                plt.clf()\n",
    "                plt.plot(sorted_series['mean_' + output].as_matrix(), sorted_series['mean_'+seg].as_matrix(), marker='o')\n",
    "                plt.xlabel('Average ' + output.replace('___output___' + country + '_' + str(year), '') + ' output')\n",
    "                plt.ylabel('Average ' + seg.replace('___policy___'  + country + '_' +str(year), '').replace('_',' '))\n",
    "                plt.savefig(os.path.join(out_dir, 'plot_' + seg + '_' + output + '.pdf'))\n",
    "                \n",
    "            # add mse's to table\n",
    "            keys = sorted(y_test.keys())\n",
    "            for name, algo in regression_algorithms:\n",
    "                sort_t = []\n",
    "                sort_p = []\n",
    "\n",
    "                for key in keys:\n",
    "                    if key not in y_test or key not in reg_clus[name]:\n",
    "                        continue\n",
    "                    sort_t.append(reg_clus[name][key])\n",
    "                    sort_p.append(y_test[key])\n",
    "\n",
    "                try:\n",
    "                    test_c = digitize(y, sort_t)\n",
    "                    pred_c = digitize(y, sort_p)\n",
    "                    auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "                except ValueError:\n",
    "                    auc_c = 0.5\n",
    "                mse = mean_squared_error(sort_t,sort_p)\n",
    "                scaled_mse = (mse/np.std(y))\n",
    "                new_row = pd.DataFrame({'model': name, 'segment': ','.join(segment_vars), 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': True, 'method': method_name}, index=[0])\n",
    "                table = table.append(new_row, ignore_index=True)\n",
    "                \n",
    "    \n",
    "        ##### Run grouped regressions\n",
    "        #_run(add_location_segments(), 'segmented')\n",
    "        if (len(segment_vars) > 1):\n",
    "            _run(add_clusters(), 'clustered')\n",
    "        #_run(add_segments(), 'segmented')\n",
    "    \n",
    "    def run_with_inputs(input_vars, name):\n",
    "        # map to be used in tracking coefficients\n",
    "        global coef_map\n",
    "        global coef_table\n",
    "        global x_train\n",
    "        global x_test\n",
    "        coef_map = {}\n",
    "        x_scaled, x_train, x_test = get_train_test(input_vars)\n",
    "        # Update Lasso Lambda using GLMNET.\n",
    "        calc_unsegmented(name, x_train, x_test)\n",
    "        update_best_lambda(x_scaled)\n",
    "        calc_segmented(segment_variables, name, x_train, x_test)\n",
    "        \n",
    "        for k,v in sorted(coef_map.items()):\n",
    "            kvp = dict()\n",
    "            kvp['model'] = k\n",
    "            kvp['inputs'] = name\n",
    "\n",
    "            for val,invar in zip(v,input_vars):\n",
    "                kvp[invar] = val\n",
    "\n",
    "            new_row = pd.DataFrame(kvp, index=[0])\n",
    "            coef_table = coef_table.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # Baseline 1\n",
    "    input_vars = inputs + non_policy_inputs\n",
    "#     run_with_inputs(input_vars, 'All variables')\n",
    "    # Baseline 2 - only policy variables that have high correlations.\n",
    "    # Note(Sam): Modifying this regex will change variables to regress on.\n",
    "    imp_vars = re.compile('^.*(?=plough|has_hired|irrigation|animals|borrowed).*$')\n",
    "    input_vars = list(filter(imp_vars.search, input_vars))\n",
    "    run_with_inputs(input_vars, 'Highly correlated policy and non-policy variables')\n",
    "    # Baseline 3 with only policy variables\n",
    "#     run_with_inputs(inputs, 'Policy variables')    \n",
    "\n",
    "    # save coefficient and output tablesdrop\n",
    "    coef_table.to_csv(os.path.join(out_dir,'coef_' + output + '.csv'))\n",
    "    table.to_csv(os.path.join(out_dir,output + '.csv'))\n",
    "    avg_table.to_csv(os.path.join(out_dir,output + '_avg' + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to just populate clusters across years.\n",
    "\n",
    "def get_segments(df, output):\n",
    "    df_t = df.loc[df[output].dropna().index]\n",
    "    df['segment_' + output], _ = get_classes(df_t[output], df[output])\n",
    "    return df\n",
    "\n",
    "def complete(df):\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    return mat\n",
    "\n",
    "# Although this repeats calculation done above with clustering/regression, it allows us to add columns per year\n",
    "# in the same dataframe.\n",
    "def get_clusters(mat, output, segment_vars):\n",
    "    segment_vars = list(segment_vars.keys())\n",
    "    seg_data = mat[segment_vars]\n",
    "    min_k = 4\n",
    "    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "    labels = kmeans.labels_\n",
    "    means = []\n",
    "    for i in np.unique(labels):\n",
    "        df_clus = mat.loc[labels == i]\n",
    "        means.append(np.mean(df_clus[output].as_matrix()))\n",
    "    sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "    mat['segment_'+ output] = labels\n",
    "    mat['segment_'+ output] = mat['segment_'+ output].apply(lambda x: sorted_ids[x])\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'land_surface': 0.35053444487641, 'number_of_animals_owned___policy': 0.28171114112379303, 'has_hired_workers___policy': 0.19052291722913, 'household_size': 0.17627112215002, 'household_head_is_widowed': 0.10803100253121302}\n",
      "Dir exists\n",
      "[ 0.07582827  0.48055669 -0.90791973 ..., -2.50892367 -1.4343748\n",
      " -1.65375061]\n",
      "[ 0.00321071]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:213: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.48209705328214764, 0.38923715116180546, 0.35584060303805398, 0.32792634240295437, 0.30445543907817291, 0.29279452166475445, 0.27706305580272506, 0.26784019108643575]\n",
      "7\n",
      "[3, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "filename = '/home/ananth/Downloads/tanzania_2014_v8_cleaned.csv'\n",
    "df = pd.read_csv(filename)\n",
    "country = 'tanzania'\n",
    "\n",
    "# change to true if you want to use all input fields\n",
    "def get_vars(year):\n",
    "    base_suffix = ''#'___'+country+'_{0}'.format(year)\n",
    "    year_vars = df.filter(regex='.*{0}'.format(base_suffix)).columns.values\n",
    "    # If we need to filter out by variables or year.\n",
    "    # non_raw_reg = re.compile('^((?!gender|damaged|bank|price|irrigation|diversification).)*$')\n",
    "    # year_vars = list(filter(non_raw_reg.search, year_vars))\n",
    "    out_reg = re.compile('.*' + base_output + '___output.*$')\n",
    "    outputs = list(filter(out_reg.search, year_vars))\n",
    "    policy_reg = re.compile('(.*___policy.*)$')\n",
    "    policy_inputs = list(filter(policy_reg.search, year_vars))\n",
    "    non_policy_reg = re.compile('^((?!policy|output|weight).)*$')\n",
    "    non_policy_inputs = list(filter(non_policy_reg.search, year_vars))\n",
    "    \n",
    "    return outputs, policy_inputs, non_policy_inputs\n",
    "\n",
    "# Modify this based on the years in the dataset.\n",
    "# years = [2011, 2013, 2015]\n",
    "years = [2014]\n",
    "\n",
    "# When true, df will contain columns for clusters across years, which then can be used to calculate\n",
    "# evidence of change across years and agreement numbers.\n",
    "populate_across_year_clusters = True\n",
    "\n",
    "for base_output in ['crop_sales',]:\n",
    "    # Choose variables to segment on based on correlation file.\n",
    "    ccs = pd.read_csv(country + '_corr.csv')\n",
    "    output = base_output + '___output' + '___'+country+'_2014'\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    # In some cases, we choose based on stability across variations in clusters after manual inspection.\n",
    "    # Note(Sam): Modifying this regex will change variables to cluster on.\n",
    "    select = ccs['Unnamed: 0'].str.contains('^.*(?=animals|household_size|has_hired|land_surface|widowed).*$') # |quantity_of_pesticides,crop_diversification, separated\n",
    "    ccs = ccs[select]\n",
    "    ccs = ccs.sort_values(output, ascending=False)\n",
    "    num_vars=8\n",
    "    best_vars = ccs[['Unnamed: 0', output]][:num_vars].as_matrix()\n",
    "    segment_variables = {}\n",
    "    seg_vars = []\n",
    "    for i in best_vars:\n",
    "        name = i[0]\n",
    "        segment_variables[name] = i[1]\n",
    "\n",
    "    print(segment_variables)\n",
    "\n",
    "    raw_df = df.copy()\n",
    "    df = complete(df)\n",
    "    for year in years:\n",
    "        outputs, policy_inputs, non_policy_inputs = get_vars(year)\n",
    "        for output in outputs:\n",
    "            if populate_across_year_clusters:\n",
    "                df = get_clusters(df, output, segment_variables)\n",
    "            run_regressions(fixed_k=4, in_name=filename, out_dir='./tanzania_v6_7_' + base_output, non_policy_inputs=non_policy_inputs, segment_variables=segment_variables, inputs=policy_inputs, output=output, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this once to avoid overwriting df in the code below.\n",
    "df_all = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_df = read_csv('/home/ananth/Downloads/tanzania_2014_v8_cleaned.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "imp_feats = set([])\n",
    "for y in [2011, 2013, 2015]:\n",
    "    y_reg = re.compile('.*___.*' + str(y) + '$')\n",
    "    y_cols = set(filter(y_reg.search, imp_cols))\n",
    "    y_cols = set([t.replace('___tanzania_' + str(y), '') for t in y_cols])\n",
    "    if len(imp_feats) == 0:\n",
    "        imp_feats = y_cols\n",
    "    else:\n",
    "        imp_feats = imp_feats.intersection(y_cols)\n",
    "imp_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imp_feats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6319b9156ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimp_feat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimp_feats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'segment_crop_sales___output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0myears\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'2011'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2013'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2015'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imp_feats' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute evidence of movement across years, the lift due to movement in relevant inputs.\n",
    "df = df_all\n",
    "series = pd.DataFrame()\n",
    "for imp_feat in imp_feats:\n",
    "    for output in ['segment_crop_sales___output']:\n",
    "        years = ['2011', '2013', '2015']\n",
    "        raw_output = 'crop_sales___output'\n",
    "        coef= {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}\n",
    "        for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "            #try:\n",
    "            z1 = for_year(output, y1)\n",
    "            z2 = for_year(output, y2)\n",
    "            r1 = for_year(raw_output, y1)\n",
    "            r2 = for_year(raw_output, y2)\n",
    "            f1 = for_year(imp_feat,y1)\n",
    "            f2 = for_year(imp_feat,y2)\n",
    "            df[output + '_change' + y1] = (df[z1]!=df[z2])\n",
    "            df[output + '_increase' + y1] = (df[z1]<df[z2])\n",
    "            df[output + '_decrease' + y1] = (df[z1]>df[z2])\n",
    "            expected = df[z1].apply(lambda x: coef[x])\n",
    "            df[imp_feat+'_increase'+y1] = (imp_df[f1]<imp_df[f2])\n",
    "            df[output + '_inversion' + y1] = (df[z2]-df[z1])*(df[f2]-df[f1])*expected\n",
    "            df[output + '_change_value' + y1] = (imp_df[r2]-imp_df[r1])\n",
    "\n",
    "        for per in [50,55,60,70,75,80,85,90,95]:\n",
    "            for seg in range(4):\n",
    "                exp_y_i = []\n",
    "                exp_y = []\n",
    "                exp = []\n",
    "                exp_i = []\n",
    "                for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "                    year = y1\n",
    "                    weight = for_year('weight', year)\n",
    "                    seg_y = for_year(output, year)\n",
    "                    df_seg = df[df[seg_y]==seg]\n",
    "                    df_seg = df_seg.loc[df_seg[output + '_change_value' + year].dropna().index]\n",
    "                    thres = np.percentile(df_seg[output + '_change_value' + year].dropna(), per)\n",
    "                    high_df = df_seg[df_seg[output + '_change_value' + year].apply(lambda x : x >= thres)]\n",
    "                    exp_y_i.append(len(high_df[(high_df[imp_feat + '_increase'+year]==True)]))\n",
    "                    exp_y.append(len(high_df))\n",
    "                    exp.append(len(df_seg))\n",
    "                    exp_i.append(len(df_seg[df_seg[imp_feat + '_increase'+year]==True]))\n",
    "                    avg_y_i = np.mean(high_df[(high_df[imp_feat + '_increase'+year]==True)][output + '_change_value' + year])\n",
    "                    avg_y = np.mean(high_df[output + '_change_value' + year].dropna())\n",
    "                    avg = np.median(df[output + '_change_value' + year].dropna())\n",
    "                row = {}\n",
    "                row['threshold'] = per\n",
    "                row['input'] = imp_feat\n",
    "                row['cluster'] = seg\n",
    "                row['movement overall'] = (sum(exp_y)/sum(exp))*100.0\n",
    "                row['movement conditioned'] = (sum(exp_y_i)/sum(exp_i))*100.0\n",
    "                row['movement lift'] = (sum(exp_y_i)/sum(exp_i))/(sum(exp_y)/sum(exp))\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "\n",
    "series.to_csv('./threshold-lift-tanzania.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect best cross correlated variables\n",
    "# Might vary for Tanzania/Uganda.\n",
    "ccs = pd.read_csv('cross_correls_ethiopia_2015_v22.csv')\n",
    "policy = ccs['Unnamed: 0'].apply(lambda x: x.find(\"___policy\") != -1)\n",
    "no_lives = ccs['Unnamed: 0'].apply(lambda x: x.find(\"lives_in\") == -1)\n",
    "no_lat = ccs['Unnamed: 0'].apply(lambda x: x.find(\"latitude\") == -1)\n",
    "no_dist = ccs['Unnamed: 0'].apply(lambda x: x.find(\"distance\") == -1)\n",
    "no_lon = ccs['Unnamed: 0'].apply(lambda x: x.find(\"longitude\") == -1)\n",
    "no_equ = ccs['Unnamed: 0'].apply(lambda x: x.find(\"equal\") == -1)\n",
    "ccs = ccs[no_lives & no_lat & no_lon & no_equ & no_dist]\n",
    "cols = ccs.columns.values\n",
    "best_var_series = {}\n",
    "for output in cols:\n",
    "    if output == 'Unnamed: 0':\n",
    "        continue\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    ccs = ccs.sort_values(output, ascending=False)\n",
    "    best_vars = ccs[['Unnamed: 0', output]][:15].as_matrix()\n",
    "    best_var_series[output] = best_vars\n",
    "best_var_series['crop_sales___output___ethiopia_2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_axe_owned___policy___ethiopia_2015\n",
      "[['number_of_pick_axe_owned___policy___ethiopia_2015' 0.3153059718665343]\n",
      " ['crop_diversification___policy___ethiopia_2015' 0.14174075630319244]\n",
      " ['number_of_oxen_owned___policy___ethiopia_2015' 0.10308466772245027]\n",
      " ['increase_in_price_of_inputs___policy___ethiopia_2015'\n",
      "  0.09004134147752284]]\n",
      "number_of_pick_axe_owned___policy___ethiopia_2015\n",
      "[['number_of_axe_owned___policy___ethiopia_2015' 0.3153059718665343]\n",
      " ['crop_diversification___policy___ethiopia_2015' 0.1839787891236533]\n",
      " ['number_of_plough_owned___policy___ethiopia_2015' 0.12247590046462667]\n",
      " ['number_of_oxen_owned___policy___ethiopia_2015' 0.10963989145180954]]\n",
      "number_of_hired_workers___policy___ethiopia_2015\n",
      "[['uses_extension_program___policy___ethiopia_2015' 0.11621771224868127]\n",
      " ['quantity_of_improved_seeds_used___policy___ethiopia_2015'\n",
      "  0.114399538356677]\n",
      " ['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015'\n",
      "  0.11266978387196736]\n",
      " ['number_of_water_storage_pit_owned___policy___ethiopia_2015'\n",
      "  0.07793389329977922]]\n",
      "number_of_water_storage_pit_owned___policy___ethiopia_2015\n",
      "[['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015'\n",
      "  0.08885118604709569]\n",
      " ['uses_extension_program___policy___ethiopia_2015' 0.08064853564246081]\n",
      " ['number_of_hired_workers___policy___ethiopia_2015' 0.07793389329977922]\n",
      " ['prevent_damage___policy___ethiopia_2015' 0.053664190085626916]]\n",
      "uses_extension_program___policy___ethiopia_2015\n",
      "[['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015'\n",
      "  0.7049263290651897]\n",
      " ['number_of_plough_owned___policy___ethiopia_2015' 0.3540572752092405]\n",
      " ['number_of_oxen_owned___policy___ethiopia_2015' 0.3422291333866238]\n",
      " ['quantity_of_improved_seeds_used___policy___ethiopia_2015'\n",
      "  0.3295411492751853]]\n",
      "number_of_oxen_owned___policy___ethiopia_2015\n",
      "[['number_of_plough_owned___policy___ethiopia_2015' 0.5298331091317873]\n",
      " ['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015'\n",
      "  0.4260470268422771]\n",
      " ['number_of_sickle_owned___policy___ethiopia_2015' 0.3674807047540996]\n",
      " ['uses_extension_program___policy___ethiopia_2015' 0.3422291333866238]]\n",
      "quantity_of_chemical_fertilizers_used___policy___ethiopia_2015\n",
      "[['uses_extension_program___policy___ethiopia_2015' 0.7049263290651897]\n",
      " ['number_of_oxen_owned___policy___ethiopia_2015' 0.4260470268422771]\n",
      " ['quantity_of_improved_seeds_used___policy___ethiopia_2015'\n",
      "  0.41338043227674703]\n",
      " ['number_of_plough_owned___policy___ethiopia_2015' 0.39857969868746135]]\n"
     ]
    }
   ],
   "source": [
    "# Print highly correlated policy recommendations.\n",
    "collinear = read_csv('./policy_collinear.csv')\n",
    "cols = collinear.columns.values\n",
    "for c in cols:\n",
    "    if c == 'Unnamed: 0' or not(c.find('hired')!=-1 or c.find('oxen')!=-1 or c.find('axe')!=-1 or c.find('water')!=-1 or c.find('extension')!=-1 or c.find('fertilizer')!=-1):\n",
    "        continue\n",
    "    print(c)\n",
    "    print(collinear.nlargest(5, c)[['Unnamed: 0', c]].as_matrix()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['owns_land_certificate___policy___ethiopia_2015', 'price_rise_of_food_item___policy___ethiopia_2015', 'percentage_of_damaged_crop___policy___ethiopia_2015', 'number_of_axe_owned___policy___ethiopia_2015', 'number_of_pick_axe_owned___policy___ethiopia_2015', 'number_of_hired_workers___policy___ethiopia_2015', 'number_of_water_storage_pit_owned___policy___ethiopia_2015', 'uses_extension_program___policy___ethiopia_2015', 'number_of_oxen_owned___policy___ethiopia_2015', 'illness_of_household_member___policy___ethiopia_2015', 'has_health_issues___policy___ethiopia_2015', 'amount_of_assistance_received___policy___ethiopia_2015', 'number_of_plough_owned___policy___ethiopia_2015', 'increase_in_price_of_inputs___policy___ethiopia_2015', 'crop_diversification___policy___ethiopia_2015', 'prevent_damage___policy___ethiopia_2015', 'number_of_sickle_owned___policy___ethiopia_2015', 'has_borrowed___policy___ethiopia_2015', 'uses_credit___policy___ethiopia_2015', 'quantity_of_chemical_fertilizers_used___policy___ethiopia_2015', 'quantity_of_improved_seeds_used___policy___ethiopia_2015', 'uses_irrigation___policy___ethiopia_2015']\n"
     ]
    }
   ],
   "source": [
    "# Write input cross-correlation rank scores to csv file.\n",
    "feats = list(imp_feats)\n",
    "feats = [for_year(f, 2015) for f in feats]\n",
    "print(feats)\n",
    "imp_vars = re.compile('^.*(?=improved_seeds|water_storage|saved|extension|plough|oxen|hired_workers|fertilizer|health_issues|sickle|axe|credit).*$')\n",
    "input_vars = list(filter(imp_vars.search, feats))\n",
    "collinear = imp_df[feats].corr('spearman')\n",
    "collinear.to_csv('./policy_collinear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afar 0.0398055198279 2129.3261381\n",
      "amhara 25.8150368658 1155.75359963\n",
      "benishangul_gumuz 1.59543145271 2725.0879217\n",
      "dire_dawa 0.174135908025 1150.03649518\n",
      "gambella 0.31624091125 2110.87379671\n",
      "harari 0.180543438023 8034.89341087\n",
      "oromiya 41.046503324 1767.39513676\n",
      "snnp 26.6685963573 1036.59091902\n",
      "somalie 0.409996338586 1545.2165131\n",
      "tigray 3.75370988447 1898.11291005\n"
     ]
    }
   ],
   "source": [
    "#Inspect per area % population and average output.\n",
    "locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "imp_df = imp_df.loc[imp_df[output].dropna().index]\n",
    "for l in sorted(locations):\n",
    "    loc_feature = 'lives_in_' + l + '___ethiopia_' + str(2015)\n",
    "    df_loc = imp_df.loc[imp_df[loc_feature]==1]\n",
    "    output = 'crop_sales___output___ethiopia_2015'  \n",
    "    print (l, sum(df_loc['weight___ethiopia_2015'])/sum(imp_df['weight___ethiopia_2015'])*100.0, np.average(df_loc[output], weights=df_loc['weight___ethiopia_2015']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.4168190128\n",
      "67.0932358318\n",
      "82.449725777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "489.09268211400331"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To determine thresholds for change between clusters over years, some stats on what quantile is the difference\n",
    "# between mean of outputs of clusters.\n",
    "np.percentile(imp_df['crop_sales___output___ethiopia_2015'].dropna(), [25,50,75])\n",
    "np.std(imp_df['crop_sales___output___ethiopia_2015'].dropna())\n",
    "from scipy.stats import percentileofscore\n",
    "x = [568.191517737871, 245.304134630863, 902.739225809435]\n",
    "for i in x:\n",
    "    print(percentileofscore(df[output + '_change_value' + year].dropna(), i))\n",
    "# 75%ile overall\n",
    "np.percentile(df[output + '_change_value' + year].dropna(), 75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
