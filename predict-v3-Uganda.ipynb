{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import Imputer\n",
    "from fancyimpute import SoftImpute\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate cross correlation file\n",
    "filename = '/home/ananth/Downloads/uganda_2013_cleaned.csv'\n",
    "imp_df = read_csv(filename)\n",
    "imp_cols = imp_df.columns.values\n",
    "corr = imp_df.corr('spearman')\n",
    "corr = imp_df.corr('spearman')['crop_sales___output']\n",
    "corr.to_csv('uganda_corr.csv')\n",
    "#Inspect this file manually and see if all features are written correctly and add an extra row on top with the column name:crop_sales___output___tanzania_2014 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y4_hhid                                   0.000000\n",
      "children_education___output              16.429354\n",
      "crop_sales___output                       0.000000\n",
      "expenditure___output                     59.802848\n",
      "medical_assistance___output               0.219058\n",
      "no_food_deficiency___output               0.164294\n",
      "crop_diversification___policy             0.000000\n",
      "damaged_crop___policy                     0.054765\n",
      "has_borrowed___policy                     0.438116\n",
      "has_hired_workers___policy                3.559693\n",
      "number_of_animals_owned___policy          3.614458\n",
      "number_of_bulls_owned___policy            0.000000\n",
      "number_of_cows_owned___policy             0.000000\n",
      "number_of_days_hired_workers___policy     3.559693\n",
      "number_of_hired_workers___policy          3.559693\n",
      "number_of_ploughs_owned___policy          0.109529\n",
      "number_of_tools_owned___policy            0.109529\n",
      "owns_land_certificate___policy           10.240964\n",
      "quantity_of_fertilizers_used___policy     0.000000\n",
      "quantity_of_improved_seeds___policy       0.000000\n",
      "quantity_of_pesticides_used___policy      0.000000\n",
      "uses_irrigation___policy                 23.767798\n",
      "land_surface                              1.204819\n",
      "household_size                            0.164294\n",
      "attended_school                           0.164294\n",
      "household_head_age                        0.054765\n",
      "household_head_is_divorced                0.054765\n",
      "household_head_is_male                    0.054765\n",
      "household_head_is_monogamous              0.054765\n",
      "household_head_is_polygamous              0.054765\n",
      "household_head_is_widowed                 0.054765\n",
      "household_head_never_married              0.054765\n",
      "literacy                                  0.164294\n",
      "weight                                    0.000000\n",
      "rural_household                           0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "imp_df = read_csv(filename)\n",
    "cdf = imp_df.loc[imp_df['crop_sales___output'].dropna().index] #___tanzania_2014\n",
    "# Print missingness values for relevant inputs.\n",
    "print(100-(cdf.apply(lambda x: x.count(), axis=0)/len(cdf))*100.0)\n",
    "\n",
    "#Plot variations of input features w.r.t output to pick variables to cluster on.\n",
    "varies = imp_df.groupby(pd.qcut(imp_df['crop_sales___output'],5,duplicates='drop')).mean()\n",
    "plt.clf()\n",
    "varies.plot(x='crop_sales___output', subplots=True,legend=True, figsize=(50,200),kind='bar',fontsize=20)\n",
    "plt.savefig('variations.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset matplotlib defaults if plots are skewed after generating variations.pdf.\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from scipy import stats\n",
    "import glmnet_python\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot \n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "import math\n",
    "\n",
    "country = 'uganda'\n",
    "\n",
    "def get_classes(output_var, pred):\n",
    "    max_bins = 3\n",
    "    _, boundaries = np.histogram(output_var, bins=max_bins)\n",
    "    classes = np.digitize(pred, bins=boundaries)\n",
    "    return classes, max_bins\n",
    "\n",
    "def for_year(var, year):\n",
    "    return var + '___' + country + '_' + str(year)\n",
    "\n",
    "def run_regressions(fixed_k, in_name, out_dir, non_policy_inputs, segment_variables, inputs, output, year):\n",
    "    global table\n",
    "    global coef_table\n",
    "    global avg_table\n",
    "    global coef_map\n",
    "    # table for regressions and classification\n",
    "    table = pd.DataFrame()\n",
    "    avg_table = pd.DataFrame()\n",
    "    coef_map = {}\n",
    "    # create table of coefficients\n",
    "    coef_table = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except:\n",
    "        print(\"Dir exists\")\n",
    "    \n",
    "    ols = linear_model.LinearRegression()\n",
    "    ridge = linear_model.Ridge(alpha=.5)\n",
    "    lasso = linear_model.Lasso(alpha = 0.001, max_iter=1e5)\n",
    "    lars_lasso = linear_model.LassoLars(alpha=.1)\n",
    "    bayes_ridge = linear_model.BayesianRidge()\n",
    "    sgd = linear_model.SGDRegressor()\n",
    "    svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    svr_lin = SVR(kernel='linear', C=1e3)\n",
    "    svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "    kernel_ridge = KernelRidge(alpha=1.0)\n",
    "\n",
    "    # pick which regression algos to use\n",
    "    regression_algorithms = (\n",
    "#        ('OrdinaryLeastSquares', ols),\n",
    "#        ('RidgeRegression', ridge),\n",
    "        ('Lasso', lasso),\n",
    "#         ('LARS Lasso', lars_lasso),\n",
    "#         ('BayesianRidgeRegression', bayes_ridge),\n",
    "#         ('StochasticGradientDescent', sgd),\n",
    "#         ('SupportVectorRegressionRBF', svr_rbf),\n",
    "#         ('SupportVectorRegressionLinear', svr_lin),\n",
    "#         ('SupportVectorRegressionPolynomial', svr_poly),\n",
    "#         ('KernelRidgeRegression', kernel_ridge)\n",
    "    )\n",
    "    \n",
    "    df = read_csv(in_name)\n",
    "    df = df.loc[df[output].dropna().index] # drop rows with unobserved income\n",
    "    df = df.loc[df['weight'].dropna().index]\n",
    "    df = df.loc[df[output] != 0]\n",
    "    # Transform input\n",
    "    logged_inputs = ['crop_sales___output', 'expenditure___output', 'crop_diversification___policy', 'number_of_animals_owned___policy', 'number_of_hired_workers___policy', 'quantity_of_fertilizers_used___policy', 'quantity_of_pesticides_used___policy', 'household_size', 'land_surface']\n",
    "    for inp in logged_inputs:\n",
    "        df[inp] = df[inp].apply(lambda x: np.log(1+x))\n",
    "\n",
    "    df['nid']= df.index.tolist()\n",
    "    \n",
    "    # Uncomment to add filters based on either gender or location or zero outputs.\n",
    "    #     df = df.loc[df['household_head_is_male___ethiopia_2015']==0]\n",
    "    #     filter_var = 'lives_in_amhara___ethiopia_' + str(year)\n",
    "    #     df = df[df[filter_var]==True]\n",
    "    #     df = df.loc[df[output] != 0] # drop zero outputs\n",
    "\n",
    "    # select % of data in test set\n",
    "    test_split = 0.2\n",
    "    \n",
    "    # perform matrix completion. completed is returned as a np array\n",
    "    # we've discussed not using it, but I left it in because I wasn't able to\n",
    "    # fit the StandardScaler with a DataFrame that contained NaN's\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "    \n",
    "    # reconstruct dataframe with completed matrix\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    \n",
    "    # Redo the same, but without the transformations, only matrix completion for raw matrix.\n",
    "    raw_df = read_csv(in_name)\n",
    "    raw_df = raw_df.loc[raw_df[output].dropna().index]\n",
    "    raw_df = raw_df.loc[raw_df['weight'].dropna().index]\n",
    "    raw_df = raw_df.loc[raw_df[output] != 0]\n",
    "    raw_df['nid']= raw_df.index.tolist()\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    raw_df['productivity'] = raw_df['crop_sales___output']/raw_df['land_surface']\n",
    "    raw_df['productivity'] = raw_df['productivity'].apply(lambda x: 0 if x == np.inf else x)\n",
    "    \n",
    "    # z-score the matrix mat used for clustering/regression.\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    \n",
    "    y = mat[output]\n",
    "    x = mat[inputs]\n",
    "    \n",
    "    def update_best_lambda(x_scaled):\n",
    "        global regression_algorithms\n",
    "        copy_y = np.array(y, dtype=np.float64)\n",
    "        print (copy_y)\n",
    "        fit = cvglmnet(x = x_scaled.copy(), y = copy_y)\n",
    "        print(fit['lambda_min'])\n",
    "        regression_algorithms = (('Lasso', linear_model.Lasso(alpha=fit['lambda_min'], max_iter=1e5)))\n",
    "    \n",
    "    # Split test/train\n",
    "    indices = range(len(mat))\n",
    "    x_train, x_test, y_train, y_test, ind_train, ind_test = \\\n",
    "        train_test_split(x, y, indices, test_size=test_split, random_state=42)\n",
    "    \n",
    "    def get_train_test(input_vars):\n",
    "        x = mat[input_vars].copy()\n",
    "        x_scaled = StandardScaler()\n",
    "        x_scaled.fit(x)\n",
    "        x_sc = x_scaled.transform(x)\n",
    "        # reconstruct DataFrame\n",
    "        x = pd.DataFrame(x_sc, columns=x.columns)\n",
    "        training_x = x.iloc[ind_train, :]\n",
    "        testing_x = x.iloc[ind_test, :]\n",
    "        return x_sc, training_x, testing_x\n",
    "    \n",
    "    def digitize(output_var, pred):\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        classes, max_bins = get_classes(output_var, pred)\n",
    "        b_classes = label_binarize(classes, range(max_bins))\n",
    "        return b_classes\n",
    "    \n",
    "    def calc_unsegmented(baseline, x_train, x_test): \n",
    "        global table\n",
    "        global coef_map\n",
    "        # reg keeps predictions from regressions along with keys\n",
    "        reg = dict()\n",
    "        for name, algo in regression_algorithms:\n",
    "            reg[name] = {}\n",
    "\n",
    "        # keys and values from test data\n",
    "        keys_list = []\n",
    "        y_list = []\n",
    "        for k, v in y_test.iteritems():\n",
    "            keys_list.append(k)\n",
    "            y_list.append(v)\n",
    "\n",
    "        # run regressions on full dataset\n",
    "        for name, algo in regression_algorithms:\n",
    "#             model = algo.fit(x_train,y_train)\n",
    "#             y_pred = model.predict(x_test)\n",
    "            model = sm.OLS(y_train, x_train)\n",
    "            fit = model.fit_regularized(alpha=1e-5, refit=True)\n",
    "            y_pred = fit.predict(x_test)\n",
    "\n",
    "            # add predictions to dict\n",
    "            for i, p in enumerate(y_pred):\n",
    "                t = reg[name]\n",
    "                t[keys_list[i]] = p\n",
    "\n",
    "            try:\n",
    "                test_c = digitize(y, y_test)\n",
    "                pred_c = digitize(y, y_pred)\n",
    "                auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "            except ValueError:\n",
    "                auc_c = 0.5\n",
    "            mse = mean_squared_error(y_test,y_pred)\n",
    "            scaled_mse = (mse/np.std(y))\n",
    "\n",
    "            # add row to table\n",
    "            new_row = pd.DataFrame({'model': name, 'segment': '', 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': False}, index=[0])\n",
    "            table = table.append(new_row, ignore_index=True)\n",
    "\n",
    "            # add coefficients to map\n",
    "#             coef_map[name + '_' + baseline] = model.coef_\n",
    "            coef_map[name + '_' + baseline] = fit.params\n",
    "            lower_bounds = []\n",
    "            upper_bounds = []\n",
    "            for ci in fit.conf_int():\n",
    "                lower_bounds += [ci[0]]\n",
    "                upper_bounds += [ci[1]]\n",
    "            coef_map[name + '_' + baseline + '_lower_bound'] = lower_bounds\n",
    "            coef_map[name + '_' + baseline + '_upper_bound'] = upper_bounds\n",
    "    \n",
    "    def calc_segmented(segment_variables, baseline, x_train, x_test):\n",
    "        global table\n",
    "        global coef_map\n",
    "        global avg_table\n",
    "        segment_vars = list(segment_variables.keys())\n",
    "        \n",
    "        def add_clusters():\n",
    "            # elbow method\n",
    "            sse = []\n",
    "            seg_data = mat[segment_vars]\n",
    "            for seg_var in segment_vars:\n",
    "                # Uncomment if clusters' features need to be weighted.\n",
    "                # seg_data[seg_var] = seg_data[seg_var]*mat[for_year('weight', year)]\n",
    "                # Multiply covariance weights so that features of the clusters are oriented towards output.\n",
    "                seg_data[seg_var] = seg_data[seg_var].apply(lambda x: x*segment_variables[seg_var])\n",
    "            for k in range(1,9):\n",
    "                kmeans = KMeans(n_clusters=k).fit(seg_data)\n",
    "                labels = kmeans.labels_\n",
    "                sse.append(sum(np.min(cdist(seg_data, kmeans.cluster_centers_, 'euclidean'), axis=1)) / seg_data.shape[0])\n",
    "\n",
    "            # K-means elbow calculation\n",
    "            plt.clf()\n",
    "            plt.plot(range(1,9), sse)\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel('Sum of squared error')\n",
    "            plt.savefig(os.path.join(out_dir, 'elbow.png'))\n",
    "            print(sse)\n",
    "            min_k = sse.index(min(sse))\n",
    "            print (min_k)\n",
    "            \n",
    "            # K-means fixed K calculation.\n",
    "            min_k = fixed_k\n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "\n",
    "            labels = kmeans.labels_\n",
    "            mat['cluster'] = labels\n",
    "            # Sort cluster labels in order of mean of output within cluster.\n",
    "            means = []\n",
    "            for i in np.unique(labels):\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "                values = list(raw_clus[output].as_matrix())\n",
    "                # Weighted average can be done if the weight column exists.\n",
    "                weights = list(raw_clus['weight'].as_matrix())\n",
    "#                 print (values)\n",
    "#                 print(weights)\n",
    "                average = np.average(values, weights=weights)\n",
    "                means.append(average)\n",
    "            sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "            print(sorted_ids)\n",
    "            mat['cluster'] = mat['cluster'].apply(lambda x: sorted_ids.index(x))\n",
    "\n",
    "            # Output ids of households, their cluster number, variables on which cluster is done along with lat/long if exists\n",
    "            # select_variables += ['latitude___ethiopia_2015', 'longitude___ethiopia_2015']\n",
    "            # Note (Sam): This is where the file I give you with Ids, cluster numbers is written.\n",
    "            # The baseline = relevant variables\n",
    "            select_variables = [output, 'y4_hhid'] + segment_vars\n",
    "            select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "            all_output = pd.concat([mat['cluster'], raw_df[select_variables]], 1)\n",
    "            all_output.to_csv(os.path.join(out_dir,'clus_' + baseline + '_' + output + '.csv'))\n",
    "            return min_k\n",
    "            \n",
    "        \n",
    "        # Add segments based on median.\n",
    "        def add_segments():\n",
    "            median_segments = {}\n",
    "            for seg_var in segment_vars:\n",
    "                #binary\n",
    "                if len(np.unique(mat[seg_var])) <= 3:\n",
    "                    median_segments[seg_var] = 0\n",
    "                else:\n",
    "                    median_segments[seg_var] = np.median(mat[seg_var])\n",
    "            mat['cluster'] = 0\n",
    "            for seg_var in segment_vars:\n",
    "                mat['cluster'] = 2*mat['cluster'] + [int(x) for x in mat[seg_var] > median_segments[seg_var]]\n",
    "            #mat[['cluster'] + segment_vars].to_csv(os.path.join(out_dir,'segment_' + ','.join(segment_vars) + '_' + baseline + '_' + output + '.csv'))\n",
    "            return int(math.pow(2, len(segment_vars)))\n",
    "\n",
    "        # Segments based solely on location.\n",
    "        def add_location_segments():\n",
    "            locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "            i = 0\n",
    "            mat['cluster'] = 0\n",
    "            for l in sorted(locations):\n",
    "                loc_feature = 'lives_in_' + l + '___ethiopia_' + str(year)\n",
    "                loc_val = mat[loc_feature].apply(lambda x: 0 if x < 0 else 1)\n",
    "                mat['cluster'] = mat['cluster'] + (i*loc_val)\n",
    "                i += 1\n",
    "            return len(locations) + 1\n",
    "            \n",
    "        def _run(max_clusters, method_name):\n",
    "            global table\n",
    "            global avg_table\n",
    "            global coef_map\n",
    "            # reg_clus keeps predictions from clustered regressions along with keys\n",
    "            reg_clus = dict()\n",
    "\n",
    "            for name, algo in regression_algorithms:\n",
    "                reg_clus[name] = {}\n",
    "\n",
    "            # need new dataframes with only training and test rows.\n",
    "            # we use this when looping through clusters\n",
    "            train_mat = mat.loc[ind_train]\n",
    "            test_mat = mat.loc[ind_test]\n",
    "            train_size = len(train_mat)\n",
    "            \n",
    "            series = {}\n",
    "            series[output] = []\n",
    "            for seg in segment_vars:\n",
    "                series[seg] = []\n",
    "            series = pd.DataFrame()\n",
    "            row = {}\n",
    "            raw_cols = raw_df.columns.values\n",
    "            raw_reg = re.compile('^((?!norm).)*$') #+ str(year) +\n",
    "            # avg_variables is set of all variables whose mean, 25%ile, 75%ile, stddev, stderr stats are written to *_avg file.\n",
    "            avg_variables = list(filter(raw_reg.search, raw_cols))\n",
    "            for i in range(max_clusters):\n",
    "                train_clus = x_train.loc[train_mat['cluster'] == i]\n",
    "                train_y = y_train.loc[train_mat['cluster'] == i]\n",
    "                test_clus = x_test.loc[test_mat['cluster'] == i]\n",
    "                test_y = y_test.loc[test_mat['cluster'] == i]\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "\n",
    "                for seg in avg_variables:\n",
    "                    values = raw_clus[seg].as_matrix()\n",
    "                    # Weighted average if weight column exists.\n",
    "                    weights = raw_clus['weight'].as_matrix()\n",
    "                    average = np.average(values, weights=weights)\n",
    "                    row['mean_' + seg] = average\n",
    "#                     print (average)\n",
    "                    variance = np.average((values-average)**2, weights=weights)\n",
    "                    row['stddev_' + seg] = math.sqrt(variance)\n",
    "                    row['stderr_' + seg] = math.sqrt(variance)/math.sqrt(len(values))\n",
    "                    row['25ile_' + seg] = np.percentile(values, 25)\n",
    "                    row['75ile_' + seg] = np.percentile(values, 75)\n",
    "                \n",
    "                row['index'] = i\n",
    "                row['size'] = len(raw_clus)\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "                avg_table = avg_table.append(new_row, ignore_index=True)\n",
    "                cluster_percent = (len(train_clus)*100.0)/train_size\n",
    "                if train_clus.empty or test_clus.empty:\n",
    "                    continue\n",
    "\n",
    "                keys_list = []\n",
    "                y_list = []\n",
    "                for k, v in test_y.iteritems():\n",
    "                    keys_list.append(k)\n",
    "                    y_list.append(v)\n",
    "\n",
    "                # Regress per cluster\n",
    "                for name, algo in regression_algorithms:  \n",
    "#                     model = algo.fit(train_clus,train_y)\n",
    "#                     y_pred = model.predict(test_clus)\n",
    "                    model = sm.OLS(train_y, train_clus)\n",
    "                    fit = model.fit_regularized(alpha=1e-5, refit=True)\n",
    "                    y_pred = fit.predict(test_clus)\n",
    "\n",
    "                    for a, b in enumerate(y_pred):\n",
    "                        t = reg_clus[name]\n",
    "                        t[keys_list[a]] = b\n",
    "\n",
    "#                     coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = model.coef_\n",
    "                    coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = fit.params\n",
    "                    lower_bounds = []\n",
    "                    upper_bounds = []\n",
    "                    for ci in fit.conf_int():\n",
    "                        lower_bounds += [ci[0]]\n",
    "                        upper_bounds += [ci[1]]\n",
    "                    coef_map[name + '_' + method_name + '_' + str(i) + '_lower_bound'] = lower_bounds\n",
    "                    coef_map[name + '_' + method_name + '_' + str(i) + '_upper_bound'] = upper_bounds\n",
    "            \n",
    "            # plot sorted correlation\n",
    "            sorted_series = series.sort_values(['mean_' + output])\n",
    "#             print (sorted_series)\n",
    "            for seg in avg_variables:\n",
    "                plt.clf()\n",
    "                plt.plot(sorted_series['mean_' + output].as_matrix(), sorted_series['mean_'+seg].as_matrix(), marker='o')\n",
    "                plt.xlabel('Average ' + output.replace('___output___' + country + '_' + str(year), '') + ' output')\n",
    "                plt.ylabel('Average ' + seg.replace('___policy___'  + country + '_' +str(year), '').replace('_',' '))\n",
    "                plt.savefig(os.path.join(out_dir, 'plot_' + seg + '_' + output + '.pdf'))\n",
    "                \n",
    "            # add mse's to table\n",
    "            keys = sorted(y_test.keys())\n",
    "            for name, algo in regression_algorithms:\n",
    "                sort_t = []\n",
    "                sort_p = []\n",
    "\n",
    "                for key in keys:\n",
    "                    if key not in y_test or key not in reg_clus[name]:\n",
    "                        continue\n",
    "                    sort_t.append(reg_clus[name][key])\n",
    "                    sort_p.append(y_test[key])\n",
    "\n",
    "                try:\n",
    "                    test_c = digitize(y, sort_t)\n",
    "                    pred_c = digitize(y, sort_p)\n",
    "                    auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "                except ValueError:\n",
    "                    auc_c = 0.5\n",
    "                mse = mean_squared_error(sort_t,sort_p)\n",
    "                scaled_mse = (mse/np.std(y))\n",
    "                new_row = pd.DataFrame({'model': name, 'segment': ','.join(segment_vars), 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': True, 'method': method_name}, index=[0])\n",
    "                table = table.append(new_row, ignore_index=True)\n",
    "                \n",
    "    \n",
    "        ##### Run grouped regressions\n",
    "        #_run(add_location_segments(), 'segmented')\n",
    "        if (len(segment_vars) > 1):\n",
    "            _run(add_clusters(), 'clustered')\n",
    "        #_run(add_segments(), 'segmented')\n",
    "    \n",
    "    def run_with_inputs(input_vars, name):\n",
    "        # map to be used in tracking coefficients\n",
    "        global coef_map\n",
    "        global coef_table\n",
    "        global x_train\n",
    "        global x_test\n",
    "        coef_map = {}\n",
    "        x_scaled, x_train, x_test = get_train_test(input_vars)\n",
    "        # Update Lasso Lambda using GLMNET.\n",
    "        update_best_lambda(x_scaled)\n",
    "        calc_unsegmented(name, x_train, x_test)\n",
    "        calc_segmented(segment_variables, name, x_train, x_test)\n",
    "        \n",
    "        for k,v in sorted(coef_map.items()):\n",
    "            kvp = dict()\n",
    "            kvp['model'] = k\n",
    "            kvp['inputs'] = name\n",
    "\n",
    "            for val,invar in zip(v,input_vars):\n",
    "                kvp[invar] = val\n",
    "\n",
    "            new_row = pd.DataFrame(kvp, index=[0])\n",
    "            coef_table = coef_table.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # Baseline 1\n",
    "    input_vars = inputs + non_policy_inputs\n",
    "#     run_with_inputs(input_vars, 'All variables')\n",
    "    # Baseline 2 - only policy variables that have high correlations.\n",
    "    # Note(Sam): Modifying this regex will change variables to regress on.\n",
    "    imp_vars = re.compile('^.*(?=number_of_days_hired|tools|animals|seeds|pest|fert).*$')\n",
    "    input_vars = list(filter(imp_vars.search, input_vars))\n",
    "    run_with_inputs(input_vars, 'Highly correlated policy and non-policy variables')\n",
    "    # Baseline 3 with only policy variables , #'amount_borrowed___policy'\n",
    "    imp_vars = ['damaged_crop___policy', 'has_borrowed___policy', 'number_of_animals_owned___policy', 'number_of_days_hired_workers___policy', 'number_of_tools_owned___policy', 'owns_land_certificate___policy', 'quantity_of_fertilizers_used___policy', 'quantity_of_improved_seeds___policy', 'quantity_of_pesticides_used___policy', 'uses_irrigation___policy']\n",
    "    run_with_inputs(imp_vars, 'Policy variables')    \n",
    "\n",
    "    # save coefficient and output tablesdrop\n",
    "    coef_table.to_csv(os.path.join(out_dir,'coef_' + output + '.csv'))\n",
    "    table.to_csv(os.path.join(out_dir,output + '.csv'))\n",
    "    avg_table.to_csv(os.path.join(out_dir,output + '_avg' + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions to just populate clusters across years.\n",
    "\n",
    "def get_segments(df, output):\n",
    "    df_t = df.loc[df[output].dropna().index]\n",
    "    df['segment_' + output], _ = get_classes(df_t[output], df[output])\n",
    "    return df\n",
    "\n",
    "def complete(df):\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    return mat\n",
    "\n",
    "# Although this repeats calculation done above with clustering/regression, it allows us to add columns per year\n",
    "# in the same dataframe.\n",
    "def get_clusters(mat, output, segment_vars):\n",
    "    segment_vars = list(segment_vars.keys())\n",
    "    seg_data = mat[segment_vars]\n",
    "    min_k = 4\n",
    "    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "    labels = kmeans.labels_\n",
    "    means = []\n",
    "    for i in np.unique(labels):\n",
    "        df_clus = mat.loc[labels == i]\n",
    "        means.append(np.mean(df_clus[output].as_matrix()))\n",
    "    sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "    mat['segment_'+ output] = labels\n",
    "    mat['segment_'+ output] = mat['segment_'+ output].apply(lambda x: sorted_ids[x])\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'land_surface': 0.43542883956779704, 'number_of_tools_owned___policy': 0.34446039457680705, 'crop_diversification___policy': 0.334653341523057, 'number_of_days_hired_workers___policy': 0.14621648709179, 'household_head_is_monogamous': 0.043576416427216}\n",
      "Dir exists\n",
      "[ 0.19456129  0.74992855  1.41741665 ...,  0.61468031  0.13070197\n",
      "  1.42912404]\n",
      "[ 0.00112765]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:212: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59016390378043759, 0.47186541199992998, 0.42616254156270189, 0.39889530410539364, 0.3775724552521883, 0.35226847057705801, 0.33629645577847284, 0.32844511154631539]\n",
      "7\n",
      "[0, 3, 1, 2]\n",
      "[ 0.19456129  0.74992855  1.41741665 ...,  0.61468031  0.13070197\n",
      "  1.42912404]\n",
      "[ 0.00093619]\n",
      "[0.59016390378043759, 0.47186541199992998, 0.42616254156270189, 0.39901142034518772, 0.37754946762158809, 0.35214623802331707, 0.33672963741409534, 0.32919624775886031]\n",
      "7\n",
      "[0, 2, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#filename = '../uganda_clean_v4.csv'\n",
    "df = pd.read_csv(filename)\n",
    "country = 'uganda'\n",
    "\n",
    "# change to true if you want to use all input fields\n",
    "def get_vars(year):\n",
    "    base_suffix = ''#'___'+country+'_{0}'.format(year)\n",
    "    year_vars = df.filter(regex='.*{0}'.format(base_suffix)).columns.values\n",
    "    # If we need to filter out by variables or year.\n",
    "    # non_raw_reg = re.compile('^((?!gender|damaged|bank|price|irrigation|diversification).)*$')\n",
    "    # year_vars = list(filter(non_raw_reg.search, year_vars))\n",
    "    out_reg = re.compile('.*' + base_output + '___output.*$')\n",
    "    outputs = list(filter(out_reg.search, year_vars))\n",
    "    policy_reg = re.compile('(.*___policy.*)$')\n",
    "    policy_inputs = list(filter(policy_reg.search, year_vars))\n",
    "    non_policy_reg = re.compile('^((?!policy|output|weight).)*$')\n",
    "    non_policy_inputs = list(filter(non_policy_reg.search, year_vars))\n",
    "    \n",
    "    return outputs, policy_inputs, non_policy_inputs\n",
    "\n",
    "# Modify this based on the years in the dataset.\n",
    "# years = [2011, 2013, 2015]\n",
    "years = [2013]\n",
    "\n",
    "# When true, df will contain columns for clusters across years, which then can be used to calculate\n",
    "# evidence of change across years and agreement numbers.\n",
    "populate_across_year_clusters = True\n",
    "\n",
    "for base_output in ['crop_sales',]:\n",
    "    # Choose variables to segment on based on correlation file.\n",
    "    ccs = pd.read_csv(country + '_corr.csv')\n",
    "    output = base_output + '___output' #___'+country+'_2014'\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    # In some cases, we choose based on stability across variations in clusters after manual inspection.\n",
    "    # Note(Sam): Modifying this regex will change variables to cluster on.\n",
    "    #fertilizer|certificate|number_of_hired|land_surface\n",
    "    select = ccs['Unnamed: 0'].str.contains('^.*(land_surface|number_of_days_hired|crop_diver|tools|mono)') # |quantity_of_pesticides,crop_diversification, separated\n",
    "    ccs = ccs[select]\n",
    "    ccs = ccs.sort_values(output, ascending=False)\n",
    "    num_vars=8\n",
    "    best_vars = ccs[['Unnamed: 0', output]][:num_vars].as_matrix()\n",
    "    segment_variables = {}\n",
    "    seg_vars = []\n",
    "    for i in best_vars:\n",
    "        name = i[0]\n",
    "        segment_variables[name] = i[1]\n",
    "\n",
    "    print(segment_variables)\n",
    "\n",
    "    raw_df = df.copy()\n",
    "    df = complete(df)\n",
    "    for year in years:\n",
    "        outputs, policy_inputs, non_policy_inputs = get_vars(year)\n",
    "        for output in outputs:\n",
    "            if populate_across_year_clusters:\n",
    "                df = get_clusters(df, output, segment_variables)\n",
    "            run_regressions(fixed_k=4, in_name=filename, out_dir='../uganda_v2_' + base_output, non_policy_inputs=non_policy_inputs, segment_variables=segment_variables, inputs=policy_inputs, output=output, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this once to avoid overwriting df in the code below.\n",
    "df_all = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owns_land_certificate___policy 2011 2013 0 100 6.666666666666667 11.0 1.6500000000000001 63.2173423708 98.0094046816\n",
      "owns_land_certificate___policy 2011 2013 1 129 18.441558441558442 20.930232558139537 1.1349492302653128 187.719676667 98.0094046816\n",
      "owns_land_certificate___policy 2011 2013 2 94 16.693944353518823 18.085106382978726 1.0833333333333335 25.8017629654 98.0094046816\n",
      "owns_land_certificate___policy 2011 2013 3 38 24.369747899159663 21.052631578947366 0.8638838475499092 -8.95413798691 98.0094046816\n",
      "owns_land_certificate___policy 2013 2015 0 160 12.430632630410656 10.0 0.8044642857142857 -27.3660277734 -22.2788003332\n",
      "owns_land_certificate___policy 2013 2015 1 149 16.028708133971293 22.14765100671141 1.3817489732545327 3.01906953167 -22.2788003332\n",
      "owns_land_certificate___policy 2013 2015 2 185 5.785123966942149 10.81081081081081 1.8687258687258688 -16.2521572793 -22.2788003332\n",
      "owns_land_certificate___policy 2013 2015 3 57 22.188449848024316 21.052631578947366 0.9488103821196827 -57.6725140427 -22.2788003332\n",
      "price_rise_of_food_item___policy 2011 2013 0 194 6.666666666666667 6.701030927835052 1.0051546391752577 63.2173423708 98.0094046816\n",
      "price_rise_of_food_item___policy 2011 2013 1 142 18.441558441558442 16.901408450704224 0.9164848244395952 187.719676667 98.0094046816\n",
      "price_rise_of_food_item___policy 2011 2013 2 24 16.693944353518823 16.666666666666664 0.9983660130718953 25.8017629654 98.0094046816\n",
      "price_rise_of_food_item___policy 2011 2013 3 6 24.369747899159663 16.666666666666664 0.6839080459770115 -8.95413798691 98.0094046816\n",
      "price_rise_of_food_item___policy 2013 2015 0 176 12.430632630410656 7.954545454545454 0.6399147727272727 -27.3660277734 -22.2788003332\n",
      "price_rise_of_food_item___policy 2013 2015 1 92 16.028708133971293 16.304347826086957 1.017196625567813 3.01906953167 -22.2788003332\n",
      "price_rise_of_food_item___policy 2013 2015 2 281 5.785123966942149 5.6939501779359425 0.9842399593289273 -16.2521572793 -22.2788003332\n",
      "price_rise_of_food_item___policy 2013 2015 3 44 22.188449848024316 20.454545454545457 0.9218555417185554 -57.6725140427 -22.2788003332\n",
      "percentage_of_damaged_crop___policy 2011 2013 0 316 6.666666666666667 13.924050632911392 2.088607594936709 63.2173423708 98.0094046816\n",
      "percentage_of_damaged_crop___policy 2011 2013 1 359 18.441558441558442 24.79108635097493 1.3443053866373729 187.719676667 98.0094046816\n",
      "percentage_of_damaged_crop___policy 2011 2013 2 264 16.693944353518823 15.530303030303031 0.9302956030897207 25.8017629654 98.0094046816\n",
      "percentage_of_damaged_crop___policy 2011 2013 3 86 24.369747899159663 25.581395348837212 1.04971932638332 -8.95413798691 98.0094046816\n",
      "percentage_of_damaged_crop___policy 2013 2015 0 582 12.430632630410656 12.199312714776632 0.9813911389297987 -27.3660277734 -22.2788003332\n",
      "percentage_of_damaged_crop___policy 2013 2015 1 516 16.028708133971293 17.05426356589147 1.0639824135138263 3.01906953167 -22.2788003332\n",
      "percentage_of_damaged_crop___policy 2013 2015 2 484 5.785123966942149 10.330578512396695 1.7857142857142858 -16.2521572793 -22.2788003332\n",
      "percentage_of_damaged_crop___policy 2013 2015 3 196 22.188449848024316 18.367346938775512 0.8277886497064579 -57.6725140427 -22.2788003332\n",
      "number_of_axe_owned___policy 2011 2013 0 290 6.666666666666667 9.655172413793103 1.4482758620689655 63.2173423708 98.0094046816\n",
      "number_of_axe_owned___policy 2011 2013 1 205 18.441558441558442 19.024390243902438 1.031604259704569 187.719676667 98.0094046816\n",
      "number_of_axe_owned___policy 2011 2013 2 89 16.693944353518823 17.97752808988764 1.0768891826393479 25.8017629654 98.0094046816\n",
      "number_of_axe_owned___policy 2011 2013 3 49 24.369747899159663 18.367346938775512 0.7536945812807883 -8.95413798691 98.0094046816\n",
      "number_of_axe_owned___policy 2013 2015 0 210 12.430632630410656 11.428571428571429 0.9193877551020407 -27.3660277734 -22.2788003332\n",
      "number_of_axe_owned___policy 2013 2015 1 192 16.028708133971293 19.270833333333336 1.2022699004975124 3.01906953167 -22.2788003332\n",
      "number_of_axe_owned___policy 2013 2015 2 256 5.785123966942149 9.375 1.6205357142857142 -16.2521572793 -22.2788003332\n",
      "number_of_axe_owned___policy 2013 2015 3 83 22.188449848024316 13.253012048192772 0.5972932827199208 -57.6725140427 -22.2788003332\n",
      "number_of_pick_axe_owned___policy 2011 2013 0 307 6.666666666666667 12.37785016286645 1.8566775244299674 63.2173423708 98.0094046816\n",
      "number_of_pick_axe_owned___policy 2011 2013 1 260 18.441558441558442 21.153846153846153 1.1470747562296857 187.719676667 98.0094046816\n",
      "number_of_pick_axe_owned___policy 2011 2013 2 130 16.693944353518823 17.692307692307693 1.0598039215686275 25.8017629654 98.0094046816\n",
      "number_of_pick_axe_owned___policy 2011 2013 3 67 24.369747899159663 14.925373134328357 0.6124549665465774 -8.95413798691 98.0094046816\n",
      "number_of_pick_axe_owned___policy 2013 2015 0 214 12.430632630410656 18.69158878504673 1.503671562082777 -27.3660277734 -22.2788003332\n",
      "number_of_pick_axe_owned___policy 2013 2015 1 153 16.028708133971293 15.032679738562091 0.9378597210028289 3.01906953167 -22.2788003332\n",
      "number_of_pick_axe_owned___policy 2013 2015 2 245 5.785123966942149 8.571428571428571 1.4816326530612245 -16.2521572793 -22.2788003332\n",
      "number_of_pick_axe_owned___policy 2013 2015 3 83 22.188449848024316 25.301204819277107 1.1402871761016669 -57.6725140427 -22.2788003332\n",
      "number_of_hired_workers___policy 2011 2013 0 62 6.666666666666667 24.193548387096776 3.6290322580645165 63.2173423708 98.0094046816\n",
      "number_of_hired_workers___policy 2011 2013 1 67 18.441558441558442 17.91044776119403 0.9712003363464368 187.719676667 98.0094046816\n",
      "number_of_hired_workers___policy 2011 2013 2 47 16.693944353518823 34.04255319148936 2.0392156862745097 25.8017629654 98.0094046816\n",
      "number_of_hired_workers___policy 2011 2013 3 45 24.369747899159663 35.55555555555556 1.4590038314176246 -8.95413798691 98.0094046816\n",
      "number_of_hired_workers___policy 2013 2015 0 113 12.430632630410656 18.58407079646018 1.4950221238938053 -27.3660277734 -22.2788003332\n",
      "number_of_hired_workers___policy 2013 2015 1 135 16.028708133971293 21.48148148148148 1.340187949143173 3.01906953167 -22.2788003332\n",
      "number_of_hired_workers___policy 2013 2015 2 102 5.785123966942149 17.647058823529413 3.050420168067227 -16.2521572793 -22.2788003332\n",
      "number_of_hired_workers___policy 2013 2015 3 62 22.188449848024316 35.483870967741936 1.5992045956694654 -57.6725140427 -22.2788003332\n",
      "number_of_water_storage_pit_owned___policy 2011 2013 0 30 6.666666666666667 10.0 1.5 63.2173423708 98.0094046816\n",
      "number_of_water_storage_pit_owned___policy 2011 2013 1 27 18.441558441558442 25.925925925925924 1.4058424621804901 187.719676667 98.0094046816\n",
      "number_of_water_storage_pit_owned___policy 2011 2013 2 31 16.693944353518823 25.806451612903224 1.5458570524984185 25.8017629654 98.0094046816\n",
      "number_of_water_storage_pit_owned___policy 2011 2013 3 9 24.369747899159663 33.33333333333333 1.367816091954023 -8.95413798691 98.0094046816\n",
      "number_of_water_storage_pit_owned___policy 2013 2015 0 39 12.430632630410656 17.94871794871795 1.4439102564102564 -27.3660277734 -22.2788003332\n",
      "number_of_water_storage_pit_owned___policy 2013 2015 1 56 16.028708133971293 16.071428571428573 1.0026652452025586 3.01906953167 -22.2788003332\n",
      "number_of_water_storage_pit_owned___policy 2013 2015 2 69 5.785123966942149 11.594202898550725 2.0041407867494825 -16.2521572793 -22.2788003332\n",
      "number_of_water_storage_pit_owned___policy 2013 2015 3 32 22.188449848024316 34.375 1.5492294520547945 -57.6725140427 -22.2788003332\n",
      "uses_extension_program___policy 2011 2013 0 139 6.666666666666667 19.424460431654676 2.9136690647482015 63.2173423708 98.0094046816\n",
      "uses_extension_program___policy 2011 2013 1 263 18.441558441558442 26.61596958174905 1.4432603223906175 187.719676667 98.0094046816\n",
      "uses_extension_program___policy 2011 2013 2 5 16.693944353518823 20.0 1.1980392156862745 25.8017629654 98.0094046816\n",
      "uses_extension_program___policy 2011 2013 3 67 24.369747899159663 32.83582089552239 1.3474009264024704 -8.95413798691 98.0094046816\n",
      "uses_extension_program___policy 2013 2015 0 269 12.430632630410656 17.100371747211895 1.375663834306957 -27.3660277734 -22.2788003332\n",
      "uses_extension_program___policy 2013 2015 1 11 16.028708133971293 18.181818181818183 1.1343283582089552 3.01906953167 -22.2788003332\n",
      "uses_extension_program___policy 2013 2015 2 138 5.785123966942149 12.318840579710146 2.129399585921325 -16.2521572793 -22.2788003332\n",
      "uses_extension_program___policy 2013 2015 3 54 22.188449848024316 18.51851851851852 0.8346017250126838 -57.6725140427 -22.2788003332\n",
      "number_of_oxen_owned___policy 2011 2013 0 83 6.666666666666667 10.843373493975903 1.6265060240963856 63.2173423708 98.0094046816\n",
      "number_of_oxen_owned___policy 2011 2013 1 190 18.441558441558442 24.736842105263158 1.3413639733135656 187.719676667 98.0094046816\n",
      "number_of_oxen_owned___policy 2011 2013 2 103 16.693944353518823 17.475728155339805 1.0468303826384924 25.8017629654 98.0094046816\n",
      "number_of_oxen_owned___policy 2011 2013 3 40 24.369747899159663 22.5 0.9232758620689656 -8.95413798691 98.0094046816\n",
      "number_of_oxen_owned___policy 2013 2015 0 205 12.430632630410656 11.707317073170733 0.9418118466898955 -27.3660277734 -22.2788003332\n",
      "number_of_oxen_owned___policy 2013 2015 1 179 16.028708133971293 20.670391061452513 1.2895855915950971 3.01906953167 -22.2788003332\n",
      "number_of_oxen_owned___policy 2013 2015 2 110 5.785123966942149 9.090909090909092 1.5714285714285714 -16.2521572793 -22.2788003332\n",
      "number_of_oxen_owned___policy 2013 2015 3 71 22.188449848024316 28.169014084507044 1.269535018329153 -57.6725140427 -22.2788003332\n",
      "illness_of_household_member___policy 2011 2013 0 149 6.666666666666667 4.697986577181208 0.7046979865771812 63.2173423708 98.0094046816\n",
      "illness_of_household_member___policy 2011 2013 1 90 18.441558441558442 12.222222222222221 0.6627543035993739 187.719676667 98.0094046816\n",
      "illness_of_household_member___policy 2011 2013 2 45 16.693944353518823 22.22222222222222 1.3311546840958604 25.8017629654 98.0094046816\n",
      "illness_of_household_member___policy 2011 2013 3 16 24.369747899159663 25.0 1.0258620689655173 -8.95413798691 98.0094046816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "illness_of_household_member___policy 2013 2015 0 175 12.430632630410656 13.714285714285715 1.1032653061224489 -27.3660277734 -22.2788003332\n",
      "illness_of_household_member___policy 2013 2015 1 134 16.028708133971293 16.417910447761194 1.0242815771886835 3.01906953167 -22.2788003332\n",
      "illness_of_household_member___policy 2013 2015 2 338 5.785123966942149 5.029585798816568 0.8693998309382924 -16.2521572793 -22.2788003332\n",
      "illness_of_household_member___policy 2013 2015 3 63 22.188449848024316 22.22222222222222 1.0015220700152205 -57.6725140427 -22.2788003332\n",
      "has_health_issues___policy 2011 2013 0 509 6.666666666666667 5.893909626719057 0.8840864440078586 63.2173423708 98.0094046816\n",
      "has_health_issues___policy 2011 2013 1 426 18.441558441558442 17.136150234741784 0.9292137803345897 187.719676667 98.0094046816\n",
      "has_health_issues___policy 2011 2013 2 218 16.693944353518823 18.34862385321101 1.0991185465011692 25.8017629654 98.0094046816\n",
      "has_health_issues___policy 2011 2013 3 85 24.369747899159663 24.705882352941178 1.013793103448276 -8.95413798691 98.0094046816\n",
      "has_health_issues___policy 2013 2015 0 201 12.430632630410656 12.935323383084576 1.0406005685856432 -27.3660277734 -22.2788003332\n",
      "has_health_issues___policy 2013 2015 1 183 16.028708133971293 16.939890710382514 1.0568469129761031 3.01906953167 -22.2788003332\n",
      "has_health_issues___policy 2013 2015 2 339 5.785123966942149 7.669616519174041 1.3257479983143698 -16.2521572793 -22.2788003332\n",
      "has_health_issues___policy 2013 2015 3 61 22.188449848024316 27.86885245901639 1.256007186166629 -57.6725140427 -22.2788003332\n",
      "amount_of_assistance_received___policy 2011 2013 0 258 6.666666666666667 1.937984496124031 0.29069767441860467 63.2173423708 98.0094046816\n",
      "amount_of_assistance_received___policy 2011 2013 1 165 18.441558441558442 12.727272727272727 0.6901408450704224 187.719676667 98.0094046816\n",
      "amount_of_assistance_received___policy 2011 2013 2 45 16.693944353518823 17.77777777777778 1.0649237472766884 25.8017629654 98.0094046816\n",
      "amount_of_assistance_received___policy 2011 2013 3 5 24.369747899159663 40.0 1.6413793103448278 -8.95413798691 98.0094046816\n",
      "amount_of_assistance_received___policy 2013 2015 0 182 12.430632630410656 10.989010989010989 0.8840266875981161 -27.3660277734 -22.2788003332\n",
      "amount_of_assistance_received___policy 2013 2015 1 109 16.028708133971293 13.761467889908257 0.8585512802957689 3.01906953167 -22.2788003332\n",
      "amount_of_assistance_received___policy 2013 2015 2 333 5.785123966942149 4.2042042042042045 0.7267267267267268 -16.2521572793 -22.2788003332\n",
      "amount_of_assistance_received___policy 2013 2015 3 27 22.188449848024316 14.814814814814813 0.6676813800101471 -57.6725140427 -22.2788003332\n",
      "number_of_plough_owned___policy 2011 2013 0 250 6.666666666666667 10.0 1.5 63.2173423708 98.0094046816\n",
      "number_of_plough_owned___policy 2011 2013 1 161 18.441558441558442 23.60248447204969 1.2798530312308634 187.719676667 98.0094046816\n",
      "number_of_plough_owned___policy 2011 2013 2 113 16.693944353518823 23.008849557522122 1.3782752038868644 25.8017629654 98.0094046816\n",
      "number_of_plough_owned___policy 2011 2013 3 38 24.369747899159663 23.684210526315788 0.9718693284936479 -8.95413798691 98.0094046816\n",
      "number_of_plough_owned___policy 2013 2015 0 126 12.430632630410656 15.079365079365079 1.2130810657596371 -27.3660277734 -22.2788003332\n",
      "number_of_plough_owned___policy 2013 2015 1 155 16.028708133971293 18.064516129032256 1.1270101107366393 3.01906953167 -22.2788003332\n",
      "number_of_plough_owned___policy 2013 2015 2 245 5.785123966942149 8.571428571428571 1.4816326530612245 -16.2521572793 -22.2788003332\n",
      "number_of_plough_owned___policy 2013 2015 3 69 22.188449848024316 26.08695652173913 1.1756998213222156 -57.6725140427 -22.2788003332\n",
      "increase_in_price_of_inputs___policy 2011 2013 0 63 6.666666666666667 11.11111111111111 1.6666666666666665 63.2173423708 98.0094046816\n",
      "increase_in_price_of_inputs___policy 2011 2013 1 83 18.441558441558442 22.89156626506024 1.241303241133548 187.719676667 98.0094046816\n",
      "increase_in_price_of_inputs___policy 2011 2013 2 43 16.693944353518823 13.953488372093023 0.8358413132694938 25.8017629654 98.0094046816\n",
      "increase_in_price_of_inputs___policy 2011 2013 3 9 24.369747899159663 0.0 0.0 -8.95413798691 98.0094046816\n",
      "increase_in_price_of_inputs___policy 2013 2015 0 103 12.430632630410656 12.62135922330097 1.0153432732316228 -27.3660277734 -22.2788003332\n",
      "increase_in_price_of_inputs___policy 2013 2015 1 151 16.028708133971293 14.56953642384106 0.908965108233666 3.01906953167 -22.2788003332\n",
      "increase_in_price_of_inputs___policy 2013 2015 2 96 5.785123966942149 14.583333333333334 2.5208333333333335 -16.2521572793 -22.2788003332\n",
      "increase_in_price_of_inputs___policy 2013 2015 3 83 22.188449848024316 25.301204819277107 1.1402871761016669 -57.6725140427 -22.2788003332\n",
      "crop_diversification___policy 2011 2013 0 279 6.666666666666667 16.48745519713262 2.4731182795698925 63.2173423708 98.0094046816\n",
      "crop_diversification___policy 2011 2013 1 291 18.441558441558442 19.243986254295535 1.0435119306906733 187.719676667 98.0094046816\n",
      "crop_diversification___policy 2011 2013 2 180 16.693944353518823 15.0 0.8985294117647058 25.8017629654 98.0094046816\n",
      "crop_diversification___policy 2011 2013 3 75 24.369747899159663 18.666666666666668 0.7659770114942529 -8.95413798691 98.0094046816\n",
      "crop_diversification___policy 2013 2015 0 230 12.430632630410656 18.26086956521739 1.4690217391304348 -27.3660277734 -22.2788003332\n",
      "crop_diversification___policy 2013 2015 1 262 16.028708133971293 17.17557251908397 1.0715506437279252 3.01906953167 -22.2788003332\n",
      "crop_diversification___policy 2013 2015 2 274 5.785123966942149 9.48905109489051 1.6402502606882168 -16.2521572793 -22.2788003332\n",
      "crop_diversification___policy 2013 2015 3 86 22.188449848024316 24.418604651162788 1.1005097164702133 -57.6725140427 -22.2788003332\n",
      "prevent_damage___policy 2011 2013 0 94 6.666666666666667 21.27659574468085 3.1914893617021276 63.2173423708 98.0094046816\n",
      "prevent_damage___policy 2011 2013 1 194 18.441558441558442 26.288659793814436 1.4255118338899375 187.719676667 98.0094046816\n",
      "prevent_damage___policy 2011 2013 2 174 16.693944353518823 13.218390804597702 0.7918075276087446 25.8017629654 98.0094046816\n",
      "prevent_damage___policy 2011 2013 3 48 24.369747899159663 33.33333333333333 1.367816091954023 -8.95413798691 98.0094046816\n",
      "prevent_damage___policy 2013 2015 0 222 12.430632630410656 13.513513513513514 1.0871138996138996 -27.3660277734 -22.2788003332\n",
      "prevent_damage___policy 2013 2015 1 246 16.028708133971293 17.88617886178862 1.115883994660842 3.01906953167 -22.2788003332\n",
      "prevent_damage___policy 2013 2015 2 121 5.785123966942149 14.87603305785124 2.5714285714285716 -16.2521572793 -22.2788003332\n",
      "prevent_damage___policy 2013 2015 3 108 22.188449848024316 21.296296296296298 0.9597919837645865 -57.6725140427 -22.2788003332\n",
      "number_of_sickle_owned___policy 2011 2013 0 404 6.666666666666667 6.188118811881188 0.9282178217821783 63.2173423708 98.0094046816\n",
      "number_of_sickle_owned___policy 2011 2013 1 343 18.441558441558442 18.367346938775512 0.9959758551307847 187.719676667 98.0094046816\n",
      "number_of_sickle_owned___policy 2011 2013 2 180 16.693944353518823 16.666666666666664 0.9983660130718953 25.8017629654 98.0094046816\n",
      "number_of_sickle_owned___policy 2011 2013 3 71 24.369747899159663 25.352112676056336 1.0403108305002429 -8.95413798691 98.0094046816\n",
      "number_of_sickle_owned___policy 2013 2015 0 227 12.430632630410656 15.418502202643172 1.240363436123348 -27.3660277734 -22.2788003332\n",
      "number_of_sickle_owned___policy 2013 2015 1 228 16.028708133971293 16.666666666666664 1.0398009950248754 3.01906953167 -22.2788003332\n",
      "number_of_sickle_owned___policy 2013 2015 2 344 5.785123966942149 6.686046511627906 1.1557308970099667 -16.2521572793 -22.2788003332\n",
      "number_of_sickle_owned___policy 2013 2015 3 91 22.188449848024316 24.175824175824175 1.089567966280295 -57.6725140427 -22.2788003332\n",
      "has_borrowed___policy 2011 2013 0 212 6.666666666666667 9.433962264150944 1.4150943396226416 63.2173423708 98.0094046816\n",
      "has_borrowed___policy 2011 2013 1 182 18.441558441558442 21.978021978021978 1.191765980498375 187.719676667 98.0094046816\n",
      "has_borrowed___policy 2011 2013 2 124 16.693944353518823 17.741935483870968 1.0627767235926628 25.8017629654 98.0094046816\n",
      "has_borrowed___policy 2011 2013 3 39 24.369747899159663 35.8974358974359 1.4730327144120248 -8.95413798691 98.0094046816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_borrowed___policy 2013 2015 0 120 12.430632630410656 7.5 0.6033482142857143 -27.3660277734 -22.2788003332\n",
      "has_borrowed___policy 2013 2015 1 99 16.028708133971293 11.11111111111111 0.693200663349917 3.01906953167 -22.2788003332\n",
      "has_borrowed___policy 2013 2015 2 208 5.785123966942149 5.288461538461538 0.9141483516483516 -16.2521572793 -22.2788003332\n",
      "has_borrowed___policy 2013 2015 3 35 22.188449848024316 28.57142857142857 1.2876712328767121 -57.6725140427 -22.2788003332\n",
      "uses_credit___policy 2011 2013 0 66 6.666666666666667 15.151515151515152 2.272727272727273 63.2173423708 98.0094046816\n",
      "uses_credit___policy 2011 2013 1 94 18.441558441558442 25.53191489361702 1.3844770752172608 187.719676667 98.0094046816\n",
      "uses_credit___policy 2011 2013 2 89 16.693944353518823 14.606741573033707 0.8749724608944701 25.8017629654 98.0094046816\n",
      "uses_credit___policy 2011 2013 3 21 24.369747899159663 42.857142857142854 1.7586206896551724 -8.95413798691 98.0094046816\n",
      "uses_credit___policy 2013 2015 0 92 12.430632630410656 14.130434782608695 1.13674301242236 -27.3660277734 -22.2788003332\n",
      "uses_credit___policy 2013 2015 1 91 16.028708133971293 15.384615384615385 0.9598163030998852 3.01906953167 -22.2788003332\n",
      "uses_credit___policy 2013 2015 2 67 5.785123966942149 7.462686567164178 1.2899786780383793 -16.2521572793 -22.2788003332\n",
      "uses_credit___policy 2013 2015 3 51 22.188449848024316 23.52941176470588 1.0604351329572925 -57.6725140427 -22.2788003332\n",
      "quantity_of_chemical_fertilizers_used___policy 2011 2013 0 118 6.666666666666667 18.64406779661017 2.7966101694915255 63.2173423708 98.0094046816\n",
      "quantity_of_chemical_fertilizers_used___policy 2011 2013 1 146 18.441558441558442 23.972602739726025 1.2999228246189465 187.719676667 98.0094046816\n",
      "quantity_of_chemical_fertilizers_used___policy 2011 2013 2 244 16.693944353518823 17.21311475409836 1.031099324975892 25.8017629654 98.0094046816\n",
      "quantity_of_chemical_fertilizers_used___policy 2011 2013 3 74 24.369747899159663 29.72972972972973 1.2199440820130476 -8.95413798691 98.0094046816\n",
      "quantity_of_chemical_fertilizers_used___policy 2013 2015 0 249 12.430632630410656 17.670682730923694 1.4215433161216293 -27.3660277734 -22.2788003332\n",
      "quantity_of_chemical_fertilizers_used___policy 2013 2015 1 368 16.028708133971293 18.75 1.169776119402985 3.01906953167 -22.2788003332\n",
      "quantity_of_chemical_fertilizers_used___policy 2013 2015 2 180 5.785123966942149 12.777777777777777 2.2087301587301584 -16.2521572793 -22.2788003332\n",
      "quantity_of_chemical_fertilizers_used___policy 2013 2015 3 129 22.188449848024316 26.356589147286826 1.187851757459913 -57.6725140427 -22.2788003332\n",
      "quantity_of_improved_seeds_used___policy 2011 2013 0 72 6.666666666666667 15.277777777777779 2.291666666666667 63.2173423708 98.0094046816\n",
      "quantity_of_improved_seeds_used___policy 2011 2013 1 84 18.441558441558442 28.57142857142857 1.5492957746478873 187.719676667 98.0094046816\n",
      "quantity_of_improved_seeds_used___policy 2011 2013 2 144 16.693944353518823 11.805555555555555 0.7071759259259259 25.8017629654 98.0094046816\n",
      "quantity_of_improved_seeds_used___policy 2011 2013 3 47 24.369747899159663 23.404255319148938 0.9603815113719736 -8.95413798691 98.0094046816\n",
      "quantity_of_improved_seeds_used___policy 2013 2015 0 89 12.430632630410656 11.235955056179774 0.9038924558587479 -27.3660277734 -22.2788003332\n",
      "quantity_of_improved_seeds_used___policy 2013 2015 1 169 16.028708133971293 15.976331360946746 0.9967323147575732 3.01906953167 -22.2788003332\n",
      "quantity_of_improved_seeds_used___policy 2013 2015 2 66 5.785123966942149 13.636363636363635 2.3571428571428568 -16.2521572793 -22.2788003332\n",
      "quantity_of_improved_seeds_used___policy 2013 2015 3 82 22.188449848024316 26.82926829268293 1.2091546942866689 -57.6725140427 -22.2788003332\n",
      "uses_irrigation___policy 2011 2013 0 29 6.666666666666667 20.689655172413794 3.103448275862069 63.2173423708 98.0094046816\n",
      "uses_irrigation___policy 2011 2013 1 60 18.441558441558442 28.333333333333332 1.5363849765258215 187.719676667 98.0094046816\n",
      "uses_irrigation___policy 2011 2013 2 56 16.693944353518823 14.285714285714285 0.8557422969187675 25.8017629654 98.0094046816\n",
      "uses_irrigation___policy 2011 2013 3 12 24.369747899159663 8.333333333333332 0.34195402298850575 -8.95413798691 98.0094046816\n",
      "uses_irrigation___policy 2013 2015 0 52 12.430632630410656 32.69230769230769 2.6299793956043955 -27.3660277734 -22.2788003332\n",
      "uses_irrigation___policy 2013 2015 1 100 16.028708133971293 23.0 1.4349253731343283 3.01906953167 -22.2788003332\n",
      "uses_irrigation___policy 2013 2015 2 41 5.785123966942149 17.073170731707318 2.951219512195122 -16.2521572793 -22.2788003332\n",
      "uses_irrigation___policy 2013 2015 3 18 22.188449848024316 27.77777777777778 1.251902587519026 -57.6725140427 -22.2788003332\n"
     ]
    }
   ],
   "source": [
    "# Compute evidence of movement across years, the lift due to movement in relevant inputs.\n",
    "df = df_all\n",
    "series = pd.DataFrame()\n",
    "for imp_feat in imp_feats:\n",
    "    for output in ['segment_crop_sales___output']:\n",
    "        years = ['2011', '2013', '2015']\n",
    "        raw_output = 'crop_sales___output'\n",
    "        coef= {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}\n",
    "        for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "            #try:\n",
    "            z1 = for_year(output, y1)\n",
    "            z2 = for_year(output, y2)\n",
    "            r1 = for_year(raw_output, y1)\n",
    "            r2 = for_year(raw_output, y2)\n",
    "            f1 = for_year(imp_feat,y1)\n",
    "            f2 = for_year(imp_feat,y2)\n",
    "            df[output + '_change' + y1] = (df[z1]!=df[z2])\n",
    "            df[output + '_increase' + y1] = (df[z1]<df[z2])\n",
    "            df[output + '_decrease' + y1] = (df[z1]>df[z2])\n",
    "            expected = df[z1].apply(lambda x: coef[x])\n",
    "            df[imp_feat+'_increase'+y1] = (imp_df[f1]<imp_df[f2])\n",
    "            df[output + '_inversion' + y1] = (df[z2]-df[z1])*(df[f2]-df[f1])*expected\n",
    "            df[output + '_change_value' + y1] = (imp_df[r2]-imp_df[r1])\n",
    "\n",
    "        for per in [50,55,60,70,75,80,85,90,95]:\n",
    "            for seg in range(4):\n",
    "                exp_y_i = []\n",
    "                exp_y = []\n",
    "                exp = []\n",
    "                exp_i = []\n",
    "                for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "                    year = y1\n",
    "                    weight = for_year('weight', year)\n",
    "                    seg_y = for_year(output, year)\n",
    "                    df_seg = df[df[seg_y]==seg]\n",
    "                    df_seg = df_seg.loc[df_seg[output + '_change_value' + year].dropna().index]\n",
    "                    thres = np.percentile(df_seg[output + '_change_value' + year].dropna(), per)\n",
    "                    high_df = df_seg[df_seg[output + '_change_value' + year].apply(lambda x : x >= thres)]\n",
    "                    exp_y_i.append(len(high_df[(high_df[imp_feat + '_increase'+year]==True)]))\n",
    "                    exp_y.append(len(high_df))\n",
    "                    exp.append(len(df_seg))\n",
    "                    exp_i.append(len(df_seg[df_seg[imp_feat + '_increase'+year]==True]))\n",
    "                    avg_y_i = np.mean(high_df[(high_df[imp_feat + '_increase'+year]==True)][output + '_change_value' + year])\n",
    "                    avg_y = np.mean(high_df[output + '_change_value' + year].dropna())\n",
    "                    avg = np.median(df[output + '_change_value' + year].dropna())\n",
    "                row = {}\n",
    "                row['threshold'] = per\n",
    "                row['input'] = imp_feat\n",
    "                row['cluster'] = seg\n",
    "                row['movement overall'] = (sum(exp_y)/sum(exp))*100.0\n",
    "                row['movement conditioned'] = (sum(exp_y_i)/sum(exp_i))*100.0\n",
    "                row['movement lift'] = (sum(exp_y_i)/sum(exp_i))/(sum(exp_y)/sum(exp))\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "\n",
    "series.to_csv('./threshold-lift.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect best cross correlated variables\n",
    "# Might vary for Tanzania/Uganda.\n",
    "ccs = pd.read_csv('cross_correls_ethiopia_2015_v22.csv')\n",
    "policy = ccs['Unnamed: 0'].apply(lambda x: x.find(\"___policy\") != -1)\n",
    "no_lives = ccs['Unnamed: 0'].apply(lambda x: x.find(\"lives_in\") == -1)\n",
    "no_lat = ccs['Unnamed: 0'].apply(lambda x: x.find(\"latitude\") == -1)\n",
    "no_dist = ccs['Unnamed: 0'].apply(lambda x: x.find(\"distance\") == -1)\n",
    "no_lon = ccs['Unnamed: 0'].apply(lambda x: x.find(\"longitude\") == -1)\n",
    "no_equ = ccs['Unnamed: 0'].apply(lambda x: x.find(\"equal\") == -1)\n",
    "ccs = ccs[no_lives & no_lat & no_lon & no_equ & no_dist]\n",
    "cols = ccs.columns.values\n",
    "best_var_series = {}\n",
    "for output in cols:\n",
    "    if output == 'Unnamed: 0':\n",
    "        continue\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    ccs = ccs.sort_values(output, ascending=False)\n",
    "    best_vars = ccs[['Unnamed: 0', output]][:15].as_matrix()\n",
    "    best_var_series[output] = best_vars\n",
    "best_var_series['crop_sales___output___ethiopia_2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_axe_owned___policy___ethiopia_2015\n",
      "[['number_of_pick_axe_owned___policy___ethiopia_2015' 0.3153059718665343]\n",
      " ['crop_diversification___policy___ethiopia_2015' 0.14174075630319244]\n",
      " ['number_of_oxen_owned___policy___ethiopia_2015' 0.10308466772245027]\n",
      " ['increase_in_price_of_inputs___policy___ethiopia_2015'\n",
      "  0.09004134147752284]]\n",
      "number_of_pick_axe_owned___policy___ethiopia_2015\n",
      "[['number_of_axe_owned___policy___ethiopia_2015' 0.3153059718665343]\n",
      " ['crop_diversification___policy___ethiopia_2015' 0.1839787891236533]\n",
      " ['number_of_plough_owned___policy___ethiopia_2015' 0.12247590046462667]\n",
      " ['number_of_oxen_owned___policy___ethiopia_2015' 0.10963989145180954]]\n",
      "number_of_hired_workers___policy___ethiopia_2015\n",
      "[['uses_extension_program___policy___ethiopia_2015' 0.11621771224868127]\n",
      " ['quantity_of_improved_seeds_used___policy___ethiopia_2015'\n",
      "  0.114399538356677]\n",
      " ['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015'\n",
      "  0.11266978387196736]\n",
      " ['number_of_water_storage_pit_owned___policy___ethiopia_2015'\n",
      "  0.07793389329977922]]\n",
      "number_of_water_storage_pit_owned___policy___ethiopia_2015\n",
      "[['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015'\n",
      "  0.08885118604709569]\n",
      " ['uses_extension_program___policy___ethiopia_2015' 0.08064853564246081]\n",
      " ['number_of_hired_workers___policy___ethiopia_2015' 0.07793389329977922]\n",
      " ['prevent_damage___policy___ethiopia_2015' 0.053664190085626916]]\n",
      "uses_extension_program___policy___ethiopia_2015\n",
      "[['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015'\n",
      "  0.7049263290651897]\n",
      " ['number_of_plough_owned___policy___ethiopia_2015' 0.3540572752092405]\n",
      " ['number_of_oxen_owned___policy___ethiopia_2015' 0.3422291333866238]\n",
      " ['quantity_of_improved_seeds_used___policy___ethiopia_2015'\n",
      "  0.3295411492751853]]\n",
      "number_of_oxen_owned___policy___ethiopia_2015\n",
      "[['number_of_plough_owned___policy___ethiopia_2015' 0.5298331091317873]\n",
      " ['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015'\n",
      "  0.4260470268422771]\n",
      " ['number_of_sickle_owned___policy___ethiopia_2015' 0.3674807047540996]\n",
      " ['uses_extension_program___policy___ethiopia_2015' 0.3422291333866238]]\n",
      "quantity_of_chemical_fertilizers_used___policy___ethiopia_2015\n",
      "[['uses_extension_program___policy___ethiopia_2015' 0.7049263290651897]\n",
      " ['number_of_oxen_owned___policy___ethiopia_2015' 0.4260470268422771]\n",
      " ['quantity_of_improved_seeds_used___policy___ethiopia_2015'\n",
      "  0.41338043227674703]\n",
      " ['number_of_plough_owned___policy___ethiopia_2015' 0.39857969868746135]]\n"
     ]
    }
   ],
   "source": [
    "# Print highly correlated policy recommendations.\n",
    "collinear = read_csv('./policy_collinear.csv')\n",
    "cols = collinear.columns.values\n",
    "for c in cols:\n",
    "    if c == 'Unnamed: 0' or not(c.find('hired')!=-1 or c.find('oxen')!=-1 or c.find('axe')!=-1 or c.find('water')!=-1 or c.find('extension')!=-1 or c.find('fertilizer')!=-1):\n",
    "        continue\n",
    "    print(c)\n",
    "    print(collinear.nlargest(5, c)[['Unnamed: 0', c]].as_matrix()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['owns_land_certificate___policy___ethiopia_2015', 'price_rise_of_food_item___policy___ethiopia_2015', 'percentage_of_damaged_crop___policy___ethiopia_2015', 'number_of_axe_owned___policy___ethiopia_2015', 'number_of_pick_axe_owned___policy___ethiopia_2015', 'number_of_hired_workers___policy___ethiopia_2015', 'number_of_water_storage_pit_owned___policy___ethiopia_2015', 'uses_extension_program___policy___ethiopia_2015', 'number_of_oxen_owned___policy___ethiopia_2015', 'illness_of_household_member___policy___ethiopia_2015', 'has_health_issues___policy___ethiopia_2015', 'amount_of_assistance_received___policy___ethiopia_2015', 'number_of_plough_owned___policy___ethiopia_2015', 'increase_in_price_of_inputs___policy___ethiopia_2015', 'crop_diversification___policy___ethiopia_2015', 'prevent_damage___policy___ethiopia_2015', 'number_of_sickle_owned___policy___ethiopia_2015', 'has_borrowed___policy___ethiopia_2015', 'uses_credit___policy___ethiopia_2015', 'quantity_of_chemical_fertilizers_used___policy___ethiopia_2015', 'quantity_of_improved_seeds_used___policy___ethiopia_2015', 'uses_irrigation___policy___ethiopia_2015']\n"
     ]
    }
   ],
   "source": [
    "# Write input cross-correlation rank scores to csv file.\n",
    "feats = list(imp_feats)\n",
    "feats = [for_year(f, 2015) for f in feats]\n",
    "print(feats)\n",
    "imp_vars = re.compile('^.*(?=improved_seeds|water_storage|saved|extension|plough|oxen|hired_workers|fertilizer|health_issues|sickle|axe|credit).*$')\n",
    "input_vars = list(filter(imp_vars.search, feats))\n",
    "collinear = imp_df[feats].corr('spearman')\n",
    "collinear.to_csv('./policy_collinear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afar 0.0398055198279 2129.3261381\n",
      "amhara 25.8150368658 1155.75359963\n",
      "benishangul_gumuz 1.59543145271 2725.0879217\n",
      "dire_dawa 0.174135908025 1150.03649518\n",
      "gambella 0.31624091125 2110.87379671\n",
      "harari 0.180543438023 8034.89341087\n",
      "oromiya 41.046503324 1767.39513676\n",
      "snnp 26.6685963573 1036.59091902\n",
      "somalie 0.409996338586 1545.2165131\n",
      "tigray 3.75370988447 1898.11291005\n"
     ]
    }
   ],
   "source": [
    "#Inspect per area % population and average output.\n",
    "locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "imp_df = imp_df.loc[imp_df[output].dropna().index]\n",
    "for l in sorted(locations):\n",
    "    loc_feature = 'lives_in_' + l + '___ethiopia_' + str(2015)\n",
    "    df_loc = imp_df.loc[imp_df[loc_feature]==1]\n",
    "    output = 'crop_sales___output___ethiopia_2015'  \n",
    "    print (l, sum(df_loc['weight___ethiopia_2015'])/sum(imp_df['weight___ethiopia_2015'])*100.0, np.average(df_loc[output], weights=df_loc['weight___ethiopia_2015']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.4168190128\n",
      "67.0932358318\n",
      "82.449725777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "489.09268211400331"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To determine thresholds for change between clusters over years, some stats on what quantile is the difference\n",
    "# between mean of outputs of clusters.\n",
    "np.percentile(imp_df['crop_sales___output___ethiopia_2015'].dropna(), [25,50,75])\n",
    "np.std(imp_df['crop_sales___output___ethiopia_2015'].dropna())\n",
    "from scipy.stats import percentileofscore\n",
    "x = [568.191517737871, 245.304134630863, 902.739225809435]\n",
    "for i in x:\n",
    "    print(percentileofscore(df[output + '_change_value' + year].dropna(), i))\n",
    "# 75%ile overall\n",
    "np.percentile(df[output + '_change_value' + year].dropna(), 75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
