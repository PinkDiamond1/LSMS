{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import Imputer\n",
    "from fancyimpute import SoftImpute\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from scipy import stats\n",
    "import glmnet_python\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot \n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate cross correlation file\n",
    "filename = '../data/uganda_2013_cleaned.csv'\n",
    "imp_df = read_csv(filename)\n",
    "imp_cols = imp_df.columns.values\n",
    "corr = imp_df.corr('spearman')\n",
    "corr = imp_df.corr('spearman')['crop_sales___output']\n",
    "corr.to_csv('../data/uganda_corr.csv',  header=['crop_sales___output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children_education___output              16.429354\n",
      "expenditure___output                     59.802848\n",
      "has_hired_workers___policy                3.559693\n",
      "number_of_animals_owned___policy          3.614458\n",
      "number_of_days_hired_workers___policy     3.559693\n",
      "number_of_hired_workers___policy          3.559693\n",
      "owns_land_certificate___policy           10.240964\n",
      "uses_irrigation___policy                 23.767798\n",
      "land_surface                              1.204819\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "imp_df = read_csv(filename)\n",
    "cdf = imp_df.loc[imp_df['crop_sales___output'].dropna().index] #___tanzania_2014\n",
    "# Print missingness values for relevant inputs.\n",
    "missing = 100-(cdf.apply(lambda x: x.count(), axis=0)/len(cdf)*100.0)\n",
    "print (missing[missing > 1])\n",
    "\n",
    "#Plot variations of input features w.r.t output to pick variables to cluster on.\n",
    "varies = imp_df.groupby(pd.qcut(imp_df['crop_sales___output'],5,duplicates='drop')).mean()\n",
    "plt.clf()\n",
    "varies.plot(x='crop_sales___output', subplots=True,legend=True, figsize=(50,200),kind='bar',fontsize=20)\n",
    "plt.savefig('../figures/uganda_variations.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset matplotlib defaults if plots are skewed after generating variations.pdf.\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: attended_school : 18.69898270071554\n",
      "Removed: number_of_animals_owned___policy : 17.664015857164394\n",
      "Removed: number_of_tools_owned___policy : 6.4455528947018275\n",
      "Removed: household_size : 5.22403056304593\n",
      "Removed: crop_diversification___policy : 4.71536467411384\n",
      "Removed: literacy : 4.064549666228747\n",
      "Removed: number_of_hired_workers___policy : 3.91340156406197\n",
      "Removed: has_hired_workers___policy : 2.2621806823312527\n",
      "Removed: household_head_is_male : 2.055081254069203\n",
      "Removed: land_surface : 1.5906669251463286\n",
      "\n",
      "Final chosen features \n",
      "\n",
      "quantity_of_improved_seeds___policy 1.02027042423\n",
      "owns_land_certificate___policy 1.11905326438\n",
      "number_of_days_hired_workers___policy 1.15338734157\n",
      "number_of_ploughs_owned___policy 1.12084109816\n",
      "household_head_is_widowed 1.07705207062\n",
      "number_of_cows_owned___policy 1.05247198906\n",
      "has_borrowed___policy 1.12834866273\n",
      "quantity_of_fertilizers_used___policy 1.04659267965\n",
      "quantity_of_pesticides_used___policy 1.00798227542\n",
      "Removed: number_of_animals_owned___policy : 17.533581182331776\n",
      "Removed: number_of_tools_owned___policy : 4.126019840391844\n",
      "Removed: number_of_hired_workers___policy : 3.875806131048448\n",
      "Removed: crop_diversification___policy : 2.5324928311931068\n",
      "Removed: has_hired_workers___policy : 1.7954165639485955\n",
      "\n",
      "Final chosen features \n",
      "\n",
      "quantity_of_improved_seeds___policy 1.02026414089\n",
      "owns_land_certificate___policy 1.10758376579\n",
      "number_of_days_hired_workers___policy 1.14105501845\n",
      "number_of_ploughs_owned___policy 1.11989482896\n",
      "number_of_cows_owned___policy 1.05245065309\n",
      "has_borrowed___policy 1.09189338581\n",
      "quantity_of_fertilizers_used___policy 1.04655070902\n",
      "quantity_of_pesticides_used___policy 1.00753994268\n"
     ]
    }
   ],
   "source": [
    "imp_df = read_csv('../data/uganda_2013_cleaned.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "\n",
    "def select_all_year_feats():\n",
    "    imp_feats = set([])\n",
    "    for y in [2013]:\n",
    "        y_reg = re.compile('.*___policy.*' + str(y) + '$')\n",
    "        y_cols = set(filter(y_reg.search, imp_cols))\n",
    "        y_cols = set([t.replace('___uganda_' + str(y), '') for t in y_cols])\n",
    "        if len(imp_feats) == 0:\n",
    "            imp_feats = y_cols\n",
    "        else:\n",
    "            imp_feats = imp_feats.intersection(y_cols)\n",
    "    return imp_feats\n",
    "\n",
    "def select_corr_feats(thres = 0.1, only_policy=False):\n",
    "    ccs = pd.read_csv('../data/uganda_corr.csv')\n",
    "    output = 'crop_sales___output'\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    ccs = ccs[ccs[output] > thres]\n",
    "    imp_feats = ccs['Unnamed: 0']\n",
    "    non_raw_reg = re.compile('^((?!lives_in|longitude|latitude|output).)*$')\n",
    "    imp_feats = list(filter(non_raw_reg.search, imp_feats))\n",
    "    if only_policy:\n",
    "        y_reg = re.compile('.*___policy.*$')\n",
    "        imp_feats = list(filter(y_reg.search, imp_feats))\n",
    "    return imp_feats\n",
    "\n",
    "def normalize():\n",
    "    raw_df = read_csv('../data/uganda_2013_cleaned.csv')\n",
    "    output = 'crop_sales___output'\n",
    "    raw_df = raw_df.loc[raw_df[output].dropna().index]\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    return raw_df\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif_score\n",
    "\n",
    "def spearman():\n",
    "    collinear = raw_df[feats].corr('spearman')\n",
    "    cols = collinear.columns\n",
    "    for c in collinear.columns:\n",
    "        for v in zip(cols, collinear[c]):\n",
    "            if v[1] > 0.4 and v[1] < 0.5 and v[1] != 1:\n",
    "                print (c, v[0], v[1])\n",
    "    \n",
    "def vif(feats, raw_df, thres=5, debug=False):    \n",
    "    while True:\n",
    "        policy = raw_df[feats].as_matrix()\n",
    "        max_vif = 0\n",
    "        max_vif_feat = None\n",
    "        for i, f in enumerate(feats):\n",
    "            if max_vif < vif_score(policy, i):\n",
    "                max_vif = vif_score(policy, i)\n",
    "                max_vif_feat = f\n",
    "        if max_vif < thres:\n",
    "            break\n",
    "        feat_set = set(feats)\n",
    "        feat_set.remove(max_vif_feat)\n",
    "        if debug:\n",
    "            print ('Removed: {0} : {1}'.format(max_vif_feat, max_vif))\n",
    "        feats = list(feat_set)\n",
    "    if debug:\n",
    "        print ('\\nFinal chosen features \\n')\n",
    "        for i, f in enumerate(feats):\n",
    "            print (f, vif_score(policy, i))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "raw_df = normalize()\n",
    "vifs = []\n",
    "\n",
    "def grid_search():\n",
    "    c_thresholds = [0.05, 0.07, 0.1, 0.12, 0.14,  0.15, 0.16, 0.17, 0.2,0.25,0.3]\n",
    "    v_thresholds = [2,3,4,5]\n",
    "    for thres in thresholds:\n",
    "        imp_feats = select_corr_feats(0.1)\n",
    "        v = vif(imp_feats, raw_df, thres)\n",
    "        vifs += [len(v)]\n",
    "\n",
    "c = 0.05\n",
    "v = 1.5\n",
    "imp_feats = select_corr_feats(c)\n",
    "all_relevant = vif(imp_feats, raw_df, v, True)\n",
    "\n",
    "imp_feats = select_corr_feats(c, True)\n",
    "policy_relevant = vif(imp_feats, raw_df, v, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country = 'uganda'\n",
    "\n",
    "def get_classes(output_var, pred):\n",
    "    max_bins = 3\n",
    "    _, boundaries = np.histogram(output_var, bins=max_bins)\n",
    "    classes = np.digitize(pred, bins=boundaries)\n",
    "    return classes, max_bins\n",
    "\n",
    "def for_year(var, year):\n",
    "    return var + '___' + country + '_' + str(year)\n",
    "\n",
    "def run_regressions(fixed_k, in_name, out_dir, non_policy_inputs, segment_variables, inputs, output, year):\n",
    "    global table\n",
    "    global coef_table\n",
    "    global avg_table\n",
    "    global coef_map\n",
    "    # table for regressions and classification\n",
    "    table = pd.DataFrame()\n",
    "    avg_table = pd.DataFrame()\n",
    "    coef_map = {}\n",
    "    # create table of coefficients\n",
    "    coef_table = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except:\n",
    "        print(\"Dir exists\")\n",
    "    \n",
    "    df = read_csv(in_name)\n",
    "    df = df.loc[df[output].dropna().index] # drop rows with unobserved income\n",
    "    df = df.loc[df['weight'].dropna().index]\n",
    "    df = df.loc[df[output] != 0]\n",
    "    # Transform input\n",
    "    logged_inputs = ['crop_sales___output', 'expenditure___output', 'crop_diversification___policy', 'number_of_animals_owned___policy', 'number_of_hired_workers___policy', 'quantity_of_fertilizers_used___policy', 'quantity_of_pesticides_used___policy', 'household_size', 'land_surface']\n",
    "    for inp in logged_inputs:\n",
    "        df[inp] = df[inp].apply(lambda x: np.log(1+x))\n",
    "\n",
    "    df['nid']= df.index.tolist()\n",
    "\n",
    "    # select % of data in test set\n",
    "    test_split = 0.2\n",
    "    \n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "    \n",
    "    # reconstruct dataframe with completed matrix\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    \n",
    "    # Redo the same, but without the transformations, only matrix completion for raw matrix.\n",
    "    raw_df = read_csv(in_name)\n",
    "    raw_df = raw_df.loc[raw_df[output].dropna().index]\n",
    "    raw_df = raw_df.loc[raw_df['weight'].dropna().index]\n",
    "    raw_df = raw_df.loc[raw_df[output] != 0]\n",
    "    raw_df['nid']= raw_df.index.tolist()\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    raw_df['productivity'] = raw_df['crop_sales___output']/raw_df['land_surface']\n",
    "    raw_df['productivity'] = raw_df['productivity'].apply(lambda x: 0 if x == np.inf else x)\n",
    "    \n",
    "    # z-score the matrix mat used for clustering/regression.\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    \n",
    "    y = mat[output]\n",
    "    x = mat[inputs]\n",
    "    \n",
    "    def update_best_lambda(x_scaled):\n",
    "        copy_y = np.array(y, dtype=np.float64)\n",
    "        print (copy_y)\n",
    "        fit = cvglmnet(x = x_scaled.copy(), y = copy_y)\n",
    "        print(fit['lambda_min'])\n",
    "        return fit['lambda_min']\n",
    "    \n",
    "    # Split test/train\n",
    "    indices = range(len(mat))\n",
    "    x_train, x_test, y_train, y_test, ind_train, ind_test = \\\n",
    "        train_test_split(x, y, indices, test_size=test_split, random_state=42)\n",
    "    \n",
    "    def get_train_test(input_vars):\n",
    "        x = mat[input_vars].copy()\n",
    "        x_scaled = StandardScaler()\n",
    "        x_scaled.fit(x)\n",
    "        x_sc = x_scaled.transform(x)\n",
    "        # reconstruct DataFrame\n",
    "        x = pd.DataFrame(x_sc, columns=x.columns)\n",
    "        training_x = x.iloc[ind_train, :]\n",
    "        testing_x = x.iloc[ind_test, :]\n",
    "        return x_sc, training_x, testing_x\n",
    "    \n",
    "    def digitize(output_var, pred):\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        classes, max_bins = get_classes(output_var, pred)\n",
    "        b_classes = label_binarize(classes, range(max_bins))\n",
    "        return b_classes\n",
    "    \n",
    "    def calc_unsegmented(baseline, x_train, x_test): \n",
    "        global table\n",
    "        global coef_map\n",
    "        # reg keeps predictions from regressions along with keys\n",
    "        reg = dict()\n",
    "        name = 'OLS'\n",
    "        reg[name] = {}\n",
    "\n",
    "        # keys and values from test data\n",
    "        keys_list = []\n",
    "        y_list = []\n",
    "        for k, v in y_test.iteritems():\n",
    "            keys_list.append(k)\n",
    "            y_list.append(v)\n",
    "\n",
    "        # run regressions on full dataset\n",
    "        name = 'OLS'\n",
    "        model = sm.OLS(y_train, x_train)\n",
    "        fit = model.fit_regularized(alpha=1e-5, refit=True)\n",
    "        y_pred = fit.predict(x_test)\n",
    "\n",
    "        # add predictions to dict\n",
    "        for i, p in enumerate(y_pred):\n",
    "            t = reg[name]\n",
    "            t[keys_list[i]] = p\n",
    "\n",
    "        try:\n",
    "            test_c = digitize(y, y_test)\n",
    "            pred_c = digitize(y, y_pred)\n",
    "            auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "        except ValueError:\n",
    "            auc_c = 0.5\n",
    "        mse = mean_squared_error(y_test,y_pred)\n",
    "        scaled_mse = (mse/np.std(y))\n",
    "\n",
    "        # add row to table\n",
    "        new_row = pd.DataFrame({'model': name, 'segment': '', 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': False}, index=[0])\n",
    "        table = table.append(new_row, ignore_index=True)\n",
    "\n",
    "        # add coefficients to map\n",
    "        coef_map[name + '_' + baseline] = fit.params\n",
    "        lower_bounds = []\n",
    "        upper_bounds = []\n",
    "        for ci in fit.conf_int():\n",
    "            lower_bounds += [ci[0]]\n",
    "            upper_bounds += [ci[1]]\n",
    "        coef_map[name + '_' + baseline + '_lower_bound'] = lower_bounds\n",
    "        coef_map[name + '_' + baseline + '_upper_bound'] = upper_bounds\n",
    "    \n",
    "    def calc_segmented(segment_variables, baseline, x_train, x_test):\n",
    "        global table\n",
    "        global coef_map\n",
    "        global avg_table\n",
    "        segment_vars = list(segment_variables.keys())\n",
    "        \n",
    "        def add_clusters():\n",
    "            # elbow method\n",
    "            sse = []\n",
    "            seg_data = mat[segment_vars]\n",
    "            for seg_var in segment_vars:\n",
    "                seg_data[seg_var] = seg_data[seg_var].apply(lambda x: x*segment_variables[seg_var])\n",
    "            for k in range(1,9):\n",
    "                kmeans = KMeans(n_clusters=k).fit(seg_data)\n",
    "                labels = kmeans.labels_\n",
    "                sse.append(sum(np.min(cdist(seg_data, kmeans.cluster_centers_, 'euclidean'), axis=1)) / seg_data.shape[0])\n",
    "\n",
    "            # K-means elbow calculation\n",
    "            plt.clf()\n",
    "            plt.plot(range(1,9), sse)\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel('Sum of squared error')\n",
    "            plt.savefig(os.path.join(out_dir, 'elbow.png'))\n",
    "            print(sse)\n",
    "            min_k = sse.index(min(sse))\n",
    "            print (min_k)\n",
    "            \n",
    "            # K-means fixed K calculation.\n",
    "            min_k = fixed_k\n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "\n",
    "            labels = kmeans.labels_\n",
    "            mat['cluster'] = labels\n",
    "            # Sort cluster labels in order of mean of output within cluster.\n",
    "            means = []\n",
    "            for i in np.unique(labels):\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "                values = list(raw_clus[output].as_matrix())\n",
    "                # Weighted average can be done if the weight column exists.\n",
    "                weights = list(raw_clus['weight'].as_matrix())\n",
    "                average = np.average(values, weights=weights)\n",
    "                means.append(average)\n",
    "            sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "            print(sorted_ids)\n",
    "            mat['cluster'] = mat['cluster'].apply(lambda x: sorted_ids.index(x))\n",
    "\n",
    "            # Output ids of households, their cluster number, variables on which cluster is done along with lat/long if exists\n",
    "            # select_variables += ['latitude___ethiopia_2015', 'longitude___ethiopia_2015']\n",
    "            # Note (Sam): This is where the file I give you with Ids, cluster numbers is written.\n",
    "            # The baseline = relevant variables\n",
    "            select_variables = [output, 'y4_hhid'] + segment_vars\n",
    "            select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "            all_output = pd.concat([mat['cluster'], raw_df[select_variables]], 1)\n",
    "            all_output.to_csv(os.path.join(out_dir,'clus_' + baseline + '_' + output + '.csv'))\n",
    "            return min_k\n",
    "            \n",
    "        \n",
    "        # Add segments based on median.\n",
    "        def add_segments():\n",
    "            median_segments = {}\n",
    "            for seg_var in segment_vars:\n",
    "                #binary\n",
    "                if len(np.unique(mat[seg_var])) <= 3:\n",
    "                    median_segments[seg_var] = 0\n",
    "                else:\n",
    "                    median_segments[seg_var] = np.median(mat[seg_var])\n",
    "            mat['cluster'] = 0\n",
    "            for seg_var in segment_vars:\n",
    "                mat['cluster'] = 2*mat['cluster'] + [int(x) for x in mat[seg_var] > median_segments[seg_var]]\n",
    "            #mat[['cluster'] + segment_vars].to_csv(os.path.join(out_dir,'segment_' + ','.join(segment_vars) + '_' + baseline + '_' + output + '.csv'))\n",
    "            return int(math.pow(2, len(segment_vars)))\n",
    "\n",
    "        # Segments based solely on location.\n",
    "        def add_location_segments():\n",
    "            locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "            i = 0\n",
    "            mat['cluster'] = 0\n",
    "            for l in sorted(locations):\n",
    "                loc_feature = 'lives_in_' + l + '___ethiopia_' + str(year)\n",
    "                loc_val = mat[loc_feature].apply(lambda x: 0 if x < 0 else 1)\n",
    "                mat['cluster'] = mat['cluster'] + (i*loc_val)\n",
    "                i += 1\n",
    "            return len(locations) + 1\n",
    "            \n",
    "        def _run(max_clusters, method_name):\n",
    "            global table\n",
    "            global avg_table\n",
    "            global coef_map\n",
    "            # reg_clus keeps predictions from clustered regressions along with keys\n",
    "            reg_clus = dict()\n",
    "\n",
    "            name = 'OLS'\n",
    "            reg_clus[name] = {}\n",
    "\n",
    "            # need new dataframes with only training and test rows.\n",
    "            # we use this when looping through clusters\n",
    "            train_mat = mat.loc[ind_train]\n",
    "            test_mat = mat.loc[ind_test]\n",
    "            train_size = len(train_mat)\n",
    "            \n",
    "            series = {}\n",
    "            series[output] = []\n",
    "            for seg in segment_vars:\n",
    "                series[seg] = []\n",
    "            series = pd.DataFrame()\n",
    "            row = {}\n",
    "            raw_cols = raw_df.columns.values\n",
    "            raw_reg = re.compile('^((?!norm).)*$') #+ str(year) +\n",
    "            # avg_variables is set of all variables whose mean, 25%ile, 75%ile, stddev, stderr stats are written to *_avg file.\n",
    "            avg_variables = list(filter(raw_reg.search, raw_cols))\n",
    "            for i in range(max_clusters):\n",
    "                train_clus = x_train.loc[train_mat['cluster'] == i]\n",
    "                train_y = y_train.loc[train_mat['cluster'] == i]\n",
    "                test_clus = x_test.loc[test_mat['cluster'] == i]\n",
    "                test_y = y_test.loc[test_mat['cluster'] == i]\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "\n",
    "                for seg in avg_variables:\n",
    "                    values = raw_clus[seg].as_matrix()\n",
    "                    # Weighted average if weight column exists.\n",
    "                    weights = raw_clus['weight'].as_matrix()\n",
    "                    average = np.average(values, weights=weights)\n",
    "                    row['mean_' + seg] = average\n",
    "                    variance = np.average((values-average)**2, weights=weights)\n",
    "                    row['stddev_' + seg] = math.sqrt(variance)\n",
    "                    row['stderr_' + seg] = math.sqrt(variance)/math.sqrt(len(values))\n",
    "                    row['25ile_' + seg] = np.percentile(values, 25)\n",
    "                    row['75ile_' + seg] = np.percentile(values, 75)\n",
    "                \n",
    "                row['index'] = i\n",
    "                row['size'] = len(raw_clus)\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "                avg_table = avg_table.append(new_row, ignore_index=True)\n",
    "                cluster_percent = (len(train_clus)*100.0)/train_size\n",
    "                if train_clus.empty or test_clus.empty:\n",
    "                    continue\n",
    "\n",
    "                keys_list = []\n",
    "                y_list = []\n",
    "                for k, v in test_y.iteritems():\n",
    "                    keys_list.append(k)\n",
    "                    y_list.append(v)\n",
    "\n",
    "                # Regress per cluster\n",
    "                name = 'OLS'\n",
    "                model = sm.OLS(train_y, train_clus)\n",
    "                fit = model.fit_regularized(alpha=1e-5, refit=True)\n",
    "                y_pred = fit.predict(test_clus)\n",
    "\n",
    "                for a, b in enumerate(y_pred):\n",
    "                    t = reg_clus[name]\n",
    "                    t[keys_list[a]] = b\n",
    "\n",
    "                coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = fit.params\n",
    "                lower_bounds = []\n",
    "                upper_bounds = []\n",
    "                for ci in fit.conf_int():\n",
    "                    lower_bounds += [ci[0]]\n",
    "                    upper_bounds += [ci[1]]\n",
    "                coef_map[name + '_' + method_name + '_' + str(i) + '_lower_bound'] = lower_bounds\n",
    "                coef_map[name + '_' + method_name + '_' + str(i) + '_upper_bound'] = upper_bounds\n",
    "            \n",
    "            # plot sorted correlation\n",
    "            sorted_series = series.sort_values(['mean_' + output])\n",
    "            for seg in avg_variables:\n",
    "                plt.clf()\n",
    "                plt.plot(sorted_series['mean_' + output].as_matrix(), sorted_series['mean_'+seg].as_matrix(), marker='o')\n",
    "                plt.xlabel('Average ' + output.replace('___output___' + country + '_' + str(year), '') + ' output')\n",
    "                plt.ylabel('Average ' + seg.replace('___policy___'  + country + '_' +str(year), '').replace('_',' '))\n",
    "                plt.savefig(os.path.join(out_dir, 'plot_' + seg + '_' + output + '.pdf'))\n",
    "                \n",
    "            # add mse's to table\n",
    "            keys = sorted(y_test.keys())\n",
    "            name = 'OLS'\n",
    "            sort_t = []\n",
    "            sort_p = []\n",
    "\n",
    "            for key in keys:\n",
    "                if key not in y_test or key not in reg_clus[name]:\n",
    "                    continue\n",
    "                sort_t.append(reg_clus[name][key])\n",
    "                sort_p.append(y_test[key])\n",
    "\n",
    "            try:\n",
    "                test_c = digitize(y, sort_t)\n",
    "                pred_c = digitize(y, sort_p)\n",
    "                auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "            except ValueError:\n",
    "                auc_c = 0.5\n",
    "            mse = mean_squared_error(sort_t,sort_p)\n",
    "            scaled_mse = (mse/np.std(y))\n",
    "            new_row = pd.DataFrame({'model': name, 'segment': ','.join(segment_vars), 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': True, 'method': method_name}, index=[0])\n",
    "            table = table.append(new_row, ignore_index=True)\n",
    "                \n",
    "    \n",
    "        ##### Run grouped regressions\n",
    "        if (len(segment_vars) > 1):\n",
    "            _run(add_clusters(), 'clustered')\n",
    "        #_run(add_location_segments(), 'segmented')\n",
    "        #_run(add_segments(), 'segmented')\n",
    "    \n",
    "    def run_with_inputs(input_vars, name):\n",
    "        # map to be used in tracking coefficients\n",
    "        global coef_map\n",
    "        global coef_table\n",
    "        global x_train\n",
    "        global x_test\n",
    "        coef_map = {}\n",
    "        x_scaled, x_train, x_test = get_train_test(input_vars)\n",
    "        # Update Lasso Lambda using GLMNET.\n",
    "        update_best_lambda(x_scaled)\n",
    "        calc_unsegmented(name, x_train, x_test)\n",
    "        calc_segmented(segment_variables, name, x_train, x_test)\n",
    "        \n",
    "        for k,v in sorted(coef_map.items()):\n",
    "            kvp = dict()\n",
    "            kvp['model'] = k\n",
    "            kvp['inputs'] = name\n",
    "\n",
    "            for val,invar in zip(v,input_vars):\n",
    "                kvp[invar] = val\n",
    "\n",
    "            new_row = pd.DataFrame(kvp, index=[0])\n",
    "            coef_table = coef_table.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # Baseline 1\n",
    "    # input_vars = inputs + non_policy_inputs\n",
    "    # run_with_inputs(input_vars, 'All variables')\n",
    "\n",
    "    global all_relevant, policy_relevant\n",
    "    \n",
    "    print ('Relevant variables')\n",
    "    print(all_relevant, policy_relevant)\n",
    "    run_with_inputs(all_relevant, 'Highly correlated all')\n",
    "    run_with_inputs(policy_relevant, 'Highly correlated policy')\n",
    "    \n",
    "    # save coefficient and output tablesdrop\n",
    "    coef_table.to_csv(os.path.join(out_dir,'coef_' + output + '.csv'))\n",
    "    table.to_csv(os.path.join(out_dir,output + '.csv'))\n",
    "    avg_table.to_csv(os.path.join(out_dir,output + '_avg' + '.csv'))\n",
    "    \n",
    "\n",
    "# Functions to just populate clusters across years.\n",
    "\n",
    "def get_segments(df, output):\n",
    "    df_t = df.loc[df[output].dropna().index]\n",
    "    df['segment_' + output], _ = get_classes(df_t[output], df[output])\n",
    "    return df\n",
    "\n",
    "def complete(df):\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    return mat\n",
    "\n",
    "# Although this repeats calculation done above with clustering/regression, it allows us to add columns per year\n",
    "# in the same dataframe.\n",
    "def get_clusters(mat, output, segment_vars):\n",
    "    segment_vars = list(segment_vars.keys())\n",
    "    seg_data = mat[segment_vars]\n",
    "    min_k = 4\n",
    "    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "    labels = kmeans.labels_\n",
    "    means = []\n",
    "    for i in np.unique(labels):\n",
    "        df_clus = mat.loc[labels == i]\n",
    "        means.append(np.mean(df_clus[output].as_matrix()))\n",
    "    sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "    mat['segment_'+ output] = labels\n",
    "    mat['segment_'+ output] = mat['segment_'+ output].apply(lambda x: sorted_ids[x])\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'land_surface': 0.4354288395677967, 'number_of_tools_owned___policy': 0.3444603945768074, 'crop_diversification___policy': 0.33465334152305737, 'number_of_days_hired_workers___policy': 0.14621648709178992, 'household_head_is_monogamous': 0.043576416427216144}\n",
      "Dir exists\n",
      "Relevant variables\n",
      "['quantity_of_improved_seeds___policy', 'owns_land_certificate___policy', 'number_of_days_hired_workers___policy', 'number_of_ploughs_owned___policy', 'household_head_is_widowed', 'number_of_cows_owned___policy', 'has_borrowed___policy', 'quantity_of_fertilizers_used___policy', 'quantity_of_pesticides_used___policy'] ['quantity_of_improved_seeds___policy', 'owns_land_certificate___policy', 'number_of_days_hired_workers___policy', 'number_of_ploughs_owned___policy', 'number_of_cows_owned___policy', 'has_borrowed___policy', 'quantity_of_fertilizers_used___policy', 'quantity_of_pesticides_used___policy']\n",
      "[ 0.19456129  0.74992855  1.41741665 ...,  0.61468031  0.13070197\n",
      "  1.42912404]\n",
      "[ 0.00892927]\n",
      "[0.5901639037804377, 0.47186541199993015, 0.42635913189073044, 0.39906719994718454, 0.37758180038775951, 0.35214623802331724, 0.33768831379817726, 0.32814408311971754]\n",
      "7\n",
      "[1, 2, 0, 3]\n",
      "[ 0.19456129  0.74992855  1.41741665 ...,  0.61468031  0.13070197\n",
      "  1.42912404]\n",
      "[ 0.00675466]\n",
      "[0.5901639037804377, 0.47186541199993015, 0.42635913189073044, 0.39895585560193636, 0.37750722156094924, 0.35214623802331724, 0.33639065658552703, 0.32776436215418403]\n",
      "7\n",
      "[3, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "country = 'uganda'\n",
    "\n",
    "# change to true if you want to use all input fields\n",
    "def get_vars(year):\n",
    "    year_vars = df.columns.values\n",
    "    out_reg = re.compile('.*' + base_output + '___output.*$')\n",
    "    outputs = list(filter(out_reg.search, year_vars))\n",
    "    policy_reg = re.compile('(.*___policy.*)$')\n",
    "    policy_inputs = list(filter(policy_reg.search, year_vars))\n",
    "    non_policy_reg = re.compile('^((?!policy|output|weight).)*$')\n",
    "    non_policy_inputs = list(filter(non_policy_reg.search, year_vars))\n",
    "    \n",
    "    return outputs, policy_inputs, non_policy_inputs\n",
    "\n",
    "# When true, df will contain columns for clusters across years, which then can be used to calculate\n",
    "# evidence of change across years and agreement numbers.\n",
    "generate_multiple_years_clusters = False\n",
    "if generate_multiple_years_clusters:\n",
    "    years = [2009, 2011, 2013]\n",
    "    raise \"Not supported\"\n",
    "else:\n",
    "    years = [2013]\n",
    "\n",
    "base_output = 'crop_sales'\n",
    "# Choose variables to segment on based on correlation file.\n",
    "ccs = pd.read_csv(country + '_corr.csv')\n",
    "output = base_output + '___output'\n",
    "ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "\n",
    "# We chose clustering features based on the iterative approach.\n",
    "# Note(Sam): Modifying this regex will change variables to cluster on.\n",
    "select = ccs['Unnamed: 0'].str.contains('^.*(land_surface|number_of_days_hired|crop_diver|tools|mono)')\n",
    "ccs = ccs[select]\n",
    "ccs = ccs.sort_values(output, ascending=False)\n",
    "num_vars=8\n",
    "best_vars = ccs[['Unnamed: 0', output]][:num_vars].as_matrix()\n",
    "segment_variables = {}\n",
    "seg_vars = []\n",
    "for i in best_vars:\n",
    "    name = i[0]\n",
    "    segment_variables[name] = i[1]\n",
    "\n",
    "print(segment_variables)\n",
    "\n",
    "raw_df = df.copy()\n",
    "df = complete(df)\n",
    "for year in years:\n",
    "    outputs, policy_inputs, non_policy_inputs = get_vars(year)\n",
    "    for output in outputs:\n",
    "        if generate_multiple_years_clusters:\n",
    "            df = get_clusters(df, output, segment_variables)\n",
    "        run_regressions(fixed_k=4, in_name=filename, out_dir='../results/uganda_v2_' + base_output, non_policy_inputs=non_policy_inputs, segment_variables=segment_variables, inputs=policy_inputs, output=output, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Below cells are still not ready. Will tweak as we get more data for Tanzania.\n",
    "assert generate_multiple_years_clusters==True, \"Below cells are not supported\"\n",
    "\n",
    "# Run this once to avoid overwriting df in the code below.\n",
    "df_all = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owns_land_certificate___policy 2011 2013 0 100 6.666666666666667 11.0 1.6500000000000001 63.2173423708 98.0094046816\n",
      "owns_land_certificate___policy 2011 2013 1 129 18.441558441558442 20.930232558139537 1.1349492302653128 187.719676667 98.0094046816\n",
      "owns_land_certificate___policy 2011 2013 2 94 16.693944353518823 18.085106382978726 1.0833333333333335 25.8017629654 98.0094046816\n",
      "owns_land_certificate___policy 2011 2013 3 38 24.369747899159663 21.052631578947366 0.8638838475499092 -8.95413798691 98.0094046816\n",
      "owns_land_certificate___policy 2013 2015 0 160 12.430632630410656 10.0 0.8044642857142857 -27.3660277734 -22.2788003332\n",
      "owns_land_certificate___policy 2013 2015 1 149 16.028708133971293 22.14765100671141 1.3817489732545327 3.01906953167 -22.2788003332\n",
      "owns_land_certificate___policy 2013 2015 2 185 5.785123966942149 10.81081081081081 1.8687258687258688 -16.2521572793 -22.2788003332\n",
      "owns_land_certificate___policy 2013 2015 3 57 22.188449848024316 21.052631578947366 0.9488103821196827 -57.6725140427 -22.2788003332\n",
      "price_rise_of_food_item___policy 2011 2013 0 194 6.666666666666667 6.701030927835052 1.0051546391752577 63.2173423708 98.0094046816\n",
      "price_rise_of_food_item___policy 2011 2013 1 142 18.441558441558442 16.901408450704224 0.9164848244395952 187.719676667 98.0094046816\n",
      "price_rise_of_food_item___policy 2011 2013 2 24 16.693944353518823 16.666666666666664 0.9983660130718953 25.8017629654 98.0094046816\n",
      "price_rise_of_food_item___policy 2011 2013 3 6 24.369747899159663 16.666666666666664 0.6839080459770115 -8.95413798691 98.0094046816\n",
      "price_rise_of_food_item___policy 2013 2015 0 176 12.430632630410656 7.954545454545454 0.6399147727272727 -27.3660277734 -22.2788003332\n",
      "price_rise_of_food_item___policy 2013 2015 1 92 16.028708133971293 16.304347826086957 1.017196625567813 3.01906953167 -22.2788003332\n",
      "price_rise_of_food_item___policy 2013 2015 2 281 5.785123966942149 5.6939501779359425 0.9842399593289273 -16.2521572793 -22.2788003332\n",
      "price_rise_of_food_item___policy 2013 2015 3 44 22.188449848024316 20.454545454545457 0.9218555417185554 -57.6725140427 -22.2788003332\n",
      "percentage_of_damaged_crop___policy 2011 2013 0 316 6.666666666666667 13.924050632911392 2.088607594936709 63.2173423708 98.0094046816\n",
      "percentage_of_damaged_crop___policy 2011 2013 1 359 18.441558441558442 24.79108635097493 1.3443053866373729 187.719676667 98.0094046816\n",
      "percentage_of_damaged_crop___policy 2011 2013 2 264 16.693944353518823 15.530303030303031 0.9302956030897207 25.8017629654 98.0094046816\n",
      "percentage_of_damaged_crop___policy 2011 2013 3 86 24.369747899159663 25.581395348837212 1.04971932638332 -8.95413798691 98.0094046816\n",
      "percentage_of_damaged_crop___policy 2013 2015 0 582 12.430632630410656 12.199312714776632 0.9813911389297987 -27.3660277734 -22.2788003332\n",
      "percentage_of_damaged_crop___policy 2013 2015 1 516 16.028708133971293 17.05426356589147 1.0639824135138263 3.01906953167 -22.2788003332\n",
      "percentage_of_damaged_crop___policy 2013 2015 2 484 5.785123966942149 10.330578512396695 1.7857142857142858 -16.2521572793 -22.2788003332\n",
      "percentage_of_damaged_crop___policy 2013 2015 3 196 22.188449848024316 18.367346938775512 0.8277886497064579 -57.6725140427 -22.2788003332\n",
      "number_of_axe_owned___policy 2011 2013 0 290 6.666666666666667 9.655172413793103 1.4482758620689655 63.2173423708 98.0094046816\n",
      "number_of_axe_owned___policy 2011 2013 1 205 18.441558441558442 19.024390243902438 1.031604259704569 187.719676667 98.0094046816\n",
      "number_of_axe_owned___policy 2011 2013 2 89 16.693944353518823 17.97752808988764 1.0768891826393479 25.8017629654 98.0094046816\n",
      "number_of_axe_owned___policy 2011 2013 3 49 24.369747899159663 18.367346938775512 0.7536945812807883 -8.95413798691 98.0094046816\n",
      "number_of_axe_owned___policy 2013 2015 0 210 12.430632630410656 11.428571428571429 0.9193877551020407 -27.3660277734 -22.2788003332\n",
      "number_of_axe_owned___policy 2013 2015 1 192 16.028708133971293 19.270833333333336 1.2022699004975124 3.01906953167 -22.2788003332\n",
      "number_of_axe_owned___policy 2013 2015 2 256 5.785123966942149 9.375 1.6205357142857142 -16.2521572793 -22.2788003332\n",
      "number_of_axe_owned___policy 2013 2015 3 83 22.188449848024316 13.253012048192772 0.5972932827199208 -57.6725140427 -22.2788003332\n",
      "number_of_pick_axe_owned___policy 2011 2013 0 307 6.666666666666667 12.37785016286645 1.8566775244299674 63.2173423708 98.0094046816\n",
      "number_of_pick_axe_owned___policy 2011 2013 1 260 18.441558441558442 21.153846153846153 1.1470747562296857 187.719676667 98.0094046816\n",
      "number_of_pick_axe_owned___policy 2011 2013 2 130 16.693944353518823 17.692307692307693 1.0598039215686275 25.8017629654 98.0094046816\n",
      "number_of_pick_axe_owned___policy 2011 2013 3 67 24.369747899159663 14.925373134328357 0.6124549665465774 -8.95413798691 98.0094046816\n",
      "number_of_pick_axe_owned___policy 2013 2015 0 214 12.430632630410656 18.69158878504673 1.503671562082777 -27.3660277734 -22.2788003332\n",
      "number_of_pick_axe_owned___policy 2013 2015 1 153 16.028708133971293 15.032679738562091 0.9378597210028289 3.01906953167 -22.2788003332\n",
      "number_of_pick_axe_owned___policy 2013 2015 2 245 5.785123966942149 8.571428571428571 1.4816326530612245 -16.2521572793 -22.2788003332\n",
      "number_of_pick_axe_owned___policy 2013 2015 3 83 22.188449848024316 25.301204819277107 1.1402871761016669 -57.6725140427 -22.2788003332\n",
      "number_of_hired_workers___policy 2011 2013 0 62 6.666666666666667 24.193548387096776 3.6290322580645165 63.2173423708 98.0094046816\n",
      "number_of_hired_workers___policy 2011 2013 1 67 18.441558441558442 17.91044776119403 0.9712003363464368 187.719676667 98.0094046816\n",
      "number_of_hired_workers___policy 2011 2013 2 47 16.693944353518823 34.04255319148936 2.0392156862745097 25.8017629654 98.0094046816\n",
      "number_of_hired_workers___policy 2011 2013 3 45 24.369747899159663 35.55555555555556 1.4590038314176246 -8.95413798691 98.0094046816\n",
      "number_of_hired_workers___policy 2013 2015 0 113 12.430632630410656 18.58407079646018 1.4950221238938053 -27.3660277734 -22.2788003332\n",
      "number_of_hired_workers___policy 2013 2015 1 135 16.028708133971293 21.48148148148148 1.340187949143173 3.01906953167 -22.2788003332\n",
      "number_of_hired_workers___policy 2013 2015 2 102 5.785123966942149 17.647058823529413 3.050420168067227 -16.2521572793 -22.2788003332\n",
      "number_of_hired_workers___policy 2013 2015 3 62 22.188449848024316 35.483870967741936 1.5992045956694654 -57.6725140427 -22.2788003332\n",
      "number_of_water_storage_pit_owned___policy 2011 2013 0 30 6.666666666666667 10.0 1.5 63.2173423708 98.0094046816\n",
      "number_of_water_storage_pit_owned___policy 2011 2013 1 27 18.441558441558442 25.925925925925924 1.4058424621804901 187.719676667 98.0094046816\n",
      "number_of_water_storage_pit_owned___policy 2011 2013 2 31 16.693944353518823 25.806451612903224 1.5458570524984185 25.8017629654 98.0094046816\n",
      "number_of_water_storage_pit_owned___policy 2011 2013 3 9 24.369747899159663 33.33333333333333 1.367816091954023 -8.95413798691 98.0094046816\n",
      "number_of_water_storage_pit_owned___policy 2013 2015 0 39 12.430632630410656 17.94871794871795 1.4439102564102564 -27.3660277734 -22.2788003332\n",
      "number_of_water_storage_pit_owned___policy 2013 2015 1 56 16.028708133971293 16.071428571428573 1.0026652452025586 3.01906953167 -22.2788003332\n",
      "number_of_water_storage_pit_owned___policy 2013 2015 2 69 5.785123966942149 11.594202898550725 2.0041407867494825 -16.2521572793 -22.2788003332\n",
      "number_of_water_storage_pit_owned___policy 2013 2015 3 32 22.188449848024316 34.375 1.5492294520547945 -57.6725140427 -22.2788003332\n",
      "uses_extension_program___policy 2011 2013 0 139 6.666666666666667 19.424460431654676 2.9136690647482015 63.2173423708 98.0094046816\n",
      "uses_extension_program___policy 2011 2013 1 263 18.441558441558442 26.61596958174905 1.4432603223906175 187.719676667 98.0094046816\n",
      "uses_extension_program___policy 2011 2013 2 5 16.693944353518823 20.0 1.1980392156862745 25.8017629654 98.0094046816\n",
      "uses_extension_program___policy 2011 2013 3 67 24.369747899159663 32.83582089552239 1.3474009264024704 -8.95413798691 98.0094046816\n",
      "uses_extension_program___policy 2013 2015 0 269 12.430632630410656 17.100371747211895 1.375663834306957 -27.3660277734 -22.2788003332\n",
      "uses_extension_program___policy 2013 2015 1 11 16.028708133971293 18.181818181818183 1.1343283582089552 3.01906953167 -22.2788003332\n",
      "uses_extension_program___policy 2013 2015 2 138 5.785123966942149 12.318840579710146 2.129399585921325 -16.2521572793 -22.2788003332\n",
      "uses_extension_program___policy 2013 2015 3 54 22.188449848024316 18.51851851851852 0.8346017250126838 -57.6725140427 -22.2788003332\n",
      "number_of_oxen_owned___policy 2011 2013 0 83 6.666666666666667 10.843373493975903 1.6265060240963856 63.2173423708 98.0094046816\n",
      "number_of_oxen_owned___policy 2011 2013 1 190 18.441558441558442 24.736842105263158 1.3413639733135656 187.719676667 98.0094046816\n",
      "number_of_oxen_owned___policy 2011 2013 2 103 16.693944353518823 17.475728155339805 1.0468303826384924 25.8017629654 98.0094046816\n",
      "number_of_oxen_owned___policy 2011 2013 3 40 24.369747899159663 22.5 0.9232758620689656 -8.95413798691 98.0094046816\n",
      "number_of_oxen_owned___policy 2013 2015 0 205 12.430632630410656 11.707317073170733 0.9418118466898955 -27.3660277734 -22.2788003332\n",
      "number_of_oxen_owned___policy 2013 2015 1 179 16.028708133971293 20.670391061452513 1.2895855915950971 3.01906953167 -22.2788003332\n",
      "number_of_oxen_owned___policy 2013 2015 2 110 5.785123966942149 9.090909090909092 1.5714285714285714 -16.2521572793 -22.2788003332\n",
      "number_of_oxen_owned___policy 2013 2015 3 71 22.188449848024316 28.169014084507044 1.269535018329153 -57.6725140427 -22.2788003332\n",
      "illness_of_household_member___policy 2011 2013 0 149 6.666666666666667 4.697986577181208 0.7046979865771812 63.2173423708 98.0094046816\n",
      "illness_of_household_member___policy 2011 2013 1 90 18.441558441558442 12.222222222222221 0.6627543035993739 187.719676667 98.0094046816\n",
      "illness_of_household_member___policy 2011 2013 2 45 16.693944353518823 22.22222222222222 1.3311546840958604 25.8017629654 98.0094046816\n",
      "illness_of_household_member___policy 2011 2013 3 16 24.369747899159663 25.0 1.0258620689655173 -8.95413798691 98.0094046816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "illness_of_household_member___policy 2013 2015 0 175 12.430632630410656 13.714285714285715 1.1032653061224489 -27.3660277734 -22.2788003332\n",
      "illness_of_household_member___policy 2013 2015 1 134 16.028708133971293 16.417910447761194 1.0242815771886835 3.01906953167 -22.2788003332\n",
      "illness_of_household_member___policy 2013 2015 2 338 5.785123966942149 5.029585798816568 0.8693998309382924 -16.2521572793 -22.2788003332\n",
      "illness_of_household_member___policy 2013 2015 3 63 22.188449848024316 22.22222222222222 1.0015220700152205 -57.6725140427 -22.2788003332\n",
      "has_health_issues___policy 2011 2013 0 509 6.666666666666667 5.893909626719057 0.8840864440078586 63.2173423708 98.0094046816\n",
      "has_health_issues___policy 2011 2013 1 426 18.441558441558442 17.136150234741784 0.9292137803345897 187.719676667 98.0094046816\n",
      "has_health_issues___policy 2011 2013 2 218 16.693944353518823 18.34862385321101 1.0991185465011692 25.8017629654 98.0094046816\n",
      "has_health_issues___policy 2011 2013 3 85 24.369747899159663 24.705882352941178 1.013793103448276 -8.95413798691 98.0094046816\n",
      "has_health_issues___policy 2013 2015 0 201 12.430632630410656 12.935323383084576 1.0406005685856432 -27.3660277734 -22.2788003332\n",
      "has_health_issues___policy 2013 2015 1 183 16.028708133971293 16.939890710382514 1.0568469129761031 3.01906953167 -22.2788003332\n",
      "has_health_issues___policy 2013 2015 2 339 5.785123966942149 7.669616519174041 1.3257479983143698 -16.2521572793 -22.2788003332\n",
      "has_health_issues___policy 2013 2015 3 61 22.188449848024316 27.86885245901639 1.256007186166629 -57.6725140427 -22.2788003332\n",
      "amount_of_assistance_received___policy 2011 2013 0 258 6.666666666666667 1.937984496124031 0.29069767441860467 63.2173423708 98.0094046816\n",
      "amount_of_assistance_received___policy 2011 2013 1 165 18.441558441558442 12.727272727272727 0.6901408450704224 187.719676667 98.0094046816\n",
      "amount_of_assistance_received___policy 2011 2013 2 45 16.693944353518823 17.77777777777778 1.0649237472766884 25.8017629654 98.0094046816\n",
      "amount_of_assistance_received___policy 2011 2013 3 5 24.369747899159663 40.0 1.6413793103448278 -8.95413798691 98.0094046816\n",
      "amount_of_assistance_received___policy 2013 2015 0 182 12.430632630410656 10.989010989010989 0.8840266875981161 -27.3660277734 -22.2788003332\n",
      "amount_of_assistance_received___policy 2013 2015 1 109 16.028708133971293 13.761467889908257 0.8585512802957689 3.01906953167 -22.2788003332\n",
      "amount_of_assistance_received___policy 2013 2015 2 333 5.785123966942149 4.2042042042042045 0.7267267267267268 -16.2521572793 -22.2788003332\n",
      "amount_of_assistance_received___policy 2013 2015 3 27 22.188449848024316 14.814814814814813 0.6676813800101471 -57.6725140427 -22.2788003332\n",
      "number_of_plough_owned___policy 2011 2013 0 250 6.666666666666667 10.0 1.5 63.2173423708 98.0094046816\n",
      "number_of_plough_owned___policy 2011 2013 1 161 18.441558441558442 23.60248447204969 1.2798530312308634 187.719676667 98.0094046816\n",
      "number_of_plough_owned___policy 2011 2013 2 113 16.693944353518823 23.008849557522122 1.3782752038868644 25.8017629654 98.0094046816\n",
      "number_of_plough_owned___policy 2011 2013 3 38 24.369747899159663 23.684210526315788 0.9718693284936479 -8.95413798691 98.0094046816\n",
      "number_of_plough_owned___policy 2013 2015 0 126 12.430632630410656 15.079365079365079 1.2130810657596371 -27.3660277734 -22.2788003332\n",
      "number_of_plough_owned___policy 2013 2015 1 155 16.028708133971293 18.064516129032256 1.1270101107366393 3.01906953167 -22.2788003332\n",
      "number_of_plough_owned___policy 2013 2015 2 245 5.785123966942149 8.571428571428571 1.4816326530612245 -16.2521572793 -22.2788003332\n",
      "number_of_plough_owned___policy 2013 2015 3 69 22.188449848024316 26.08695652173913 1.1756998213222156 -57.6725140427 -22.2788003332\n",
      "increase_in_price_of_inputs___policy 2011 2013 0 63 6.666666666666667 11.11111111111111 1.6666666666666665 63.2173423708 98.0094046816\n",
      "increase_in_price_of_inputs___policy 2011 2013 1 83 18.441558441558442 22.89156626506024 1.241303241133548 187.719676667 98.0094046816\n",
      "increase_in_price_of_inputs___policy 2011 2013 2 43 16.693944353518823 13.953488372093023 0.8358413132694938 25.8017629654 98.0094046816\n",
      "increase_in_price_of_inputs___policy 2011 2013 3 9 24.369747899159663 0.0 0.0 -8.95413798691 98.0094046816\n",
      "increase_in_price_of_inputs___policy 2013 2015 0 103 12.430632630410656 12.62135922330097 1.0153432732316228 -27.3660277734 -22.2788003332\n",
      "increase_in_price_of_inputs___policy 2013 2015 1 151 16.028708133971293 14.56953642384106 0.908965108233666 3.01906953167 -22.2788003332\n",
      "increase_in_price_of_inputs___policy 2013 2015 2 96 5.785123966942149 14.583333333333334 2.5208333333333335 -16.2521572793 -22.2788003332\n",
      "increase_in_price_of_inputs___policy 2013 2015 3 83 22.188449848024316 25.301204819277107 1.1402871761016669 -57.6725140427 -22.2788003332\n",
      "crop_diversification___policy 2011 2013 0 279 6.666666666666667 16.48745519713262 2.4731182795698925 63.2173423708 98.0094046816\n",
      "crop_diversification___policy 2011 2013 1 291 18.441558441558442 19.243986254295535 1.0435119306906733 187.719676667 98.0094046816\n",
      "crop_diversification___policy 2011 2013 2 180 16.693944353518823 15.0 0.8985294117647058 25.8017629654 98.0094046816\n",
      "crop_diversification___policy 2011 2013 3 75 24.369747899159663 18.666666666666668 0.7659770114942529 -8.95413798691 98.0094046816\n",
      "crop_diversification___policy 2013 2015 0 230 12.430632630410656 18.26086956521739 1.4690217391304348 -27.3660277734 -22.2788003332\n",
      "crop_diversification___policy 2013 2015 1 262 16.028708133971293 17.17557251908397 1.0715506437279252 3.01906953167 -22.2788003332\n",
      "crop_diversification___policy 2013 2015 2 274 5.785123966942149 9.48905109489051 1.6402502606882168 -16.2521572793 -22.2788003332\n",
      "crop_diversification___policy 2013 2015 3 86 22.188449848024316 24.418604651162788 1.1005097164702133 -57.6725140427 -22.2788003332\n",
      "prevent_damage___policy 2011 2013 0 94 6.666666666666667 21.27659574468085 3.1914893617021276 63.2173423708 98.0094046816\n",
      "prevent_damage___policy 2011 2013 1 194 18.441558441558442 26.288659793814436 1.4255118338899375 187.719676667 98.0094046816\n",
      "prevent_damage___policy 2011 2013 2 174 16.693944353518823 13.218390804597702 0.7918075276087446 25.8017629654 98.0094046816\n",
      "prevent_damage___policy 2011 2013 3 48 24.369747899159663 33.33333333333333 1.367816091954023 -8.95413798691 98.0094046816\n",
      "prevent_damage___policy 2013 2015 0 222 12.430632630410656 13.513513513513514 1.0871138996138996 -27.3660277734 -22.2788003332\n",
      "prevent_damage___policy 2013 2015 1 246 16.028708133971293 17.88617886178862 1.115883994660842 3.01906953167 -22.2788003332\n",
      "prevent_damage___policy 2013 2015 2 121 5.785123966942149 14.87603305785124 2.5714285714285716 -16.2521572793 -22.2788003332\n",
      "prevent_damage___policy 2013 2015 3 108 22.188449848024316 21.296296296296298 0.9597919837645865 -57.6725140427 -22.2788003332\n",
      "number_of_sickle_owned___policy 2011 2013 0 404 6.666666666666667 6.188118811881188 0.9282178217821783 63.2173423708 98.0094046816\n",
      "number_of_sickle_owned___policy 2011 2013 1 343 18.441558441558442 18.367346938775512 0.9959758551307847 187.719676667 98.0094046816\n",
      "number_of_sickle_owned___policy 2011 2013 2 180 16.693944353518823 16.666666666666664 0.9983660130718953 25.8017629654 98.0094046816\n",
      "number_of_sickle_owned___policy 2011 2013 3 71 24.369747899159663 25.352112676056336 1.0403108305002429 -8.95413798691 98.0094046816\n",
      "number_of_sickle_owned___policy 2013 2015 0 227 12.430632630410656 15.418502202643172 1.240363436123348 -27.3660277734 -22.2788003332\n",
      "number_of_sickle_owned___policy 2013 2015 1 228 16.028708133971293 16.666666666666664 1.0398009950248754 3.01906953167 -22.2788003332\n",
      "number_of_sickle_owned___policy 2013 2015 2 344 5.785123966942149 6.686046511627906 1.1557308970099667 -16.2521572793 -22.2788003332\n",
      "number_of_sickle_owned___policy 2013 2015 3 91 22.188449848024316 24.175824175824175 1.089567966280295 -57.6725140427 -22.2788003332\n",
      "has_borrowed___policy 2011 2013 0 212 6.666666666666667 9.433962264150944 1.4150943396226416 63.2173423708 98.0094046816\n",
      "has_borrowed___policy 2011 2013 1 182 18.441558441558442 21.978021978021978 1.191765980498375 187.719676667 98.0094046816\n",
      "has_borrowed___policy 2011 2013 2 124 16.693944353518823 17.741935483870968 1.0627767235926628 25.8017629654 98.0094046816\n",
      "has_borrowed___policy 2011 2013 3 39 24.369747899159663 35.8974358974359 1.4730327144120248 -8.95413798691 98.0094046816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_borrowed___policy 2013 2015 0 120 12.430632630410656 7.5 0.6033482142857143 -27.3660277734 -22.2788003332\n",
      "has_borrowed___policy 2013 2015 1 99 16.028708133971293 11.11111111111111 0.693200663349917 3.01906953167 -22.2788003332\n",
      "has_borrowed___policy 2013 2015 2 208 5.785123966942149 5.288461538461538 0.9141483516483516 -16.2521572793 -22.2788003332\n",
      "has_borrowed___policy 2013 2015 3 35 22.188449848024316 28.57142857142857 1.2876712328767121 -57.6725140427 -22.2788003332\n",
      "uses_credit___policy 2011 2013 0 66 6.666666666666667 15.151515151515152 2.272727272727273 63.2173423708 98.0094046816\n",
      "uses_credit___policy 2011 2013 1 94 18.441558441558442 25.53191489361702 1.3844770752172608 187.719676667 98.0094046816\n",
      "uses_credit___policy 2011 2013 2 89 16.693944353518823 14.606741573033707 0.8749724608944701 25.8017629654 98.0094046816\n",
      "uses_credit___policy 2011 2013 3 21 24.369747899159663 42.857142857142854 1.7586206896551724 -8.95413798691 98.0094046816\n",
      "uses_credit___policy 2013 2015 0 92 12.430632630410656 14.130434782608695 1.13674301242236 -27.3660277734 -22.2788003332\n",
      "uses_credit___policy 2013 2015 1 91 16.028708133971293 15.384615384615385 0.9598163030998852 3.01906953167 -22.2788003332\n",
      "uses_credit___policy 2013 2015 2 67 5.785123966942149 7.462686567164178 1.2899786780383793 -16.2521572793 -22.2788003332\n",
      "uses_credit___policy 2013 2015 3 51 22.188449848024316 23.52941176470588 1.0604351329572925 -57.6725140427 -22.2788003332\n",
      "quantity_of_chemical_fertilizers_used___policy 2011 2013 0 118 6.666666666666667 18.64406779661017 2.7966101694915255 63.2173423708 98.0094046816\n",
      "quantity_of_chemical_fertilizers_used___policy 2011 2013 1 146 18.441558441558442 23.972602739726025 1.2999228246189465 187.719676667 98.0094046816\n",
      "quantity_of_chemical_fertilizers_used___policy 2011 2013 2 244 16.693944353518823 17.21311475409836 1.031099324975892 25.8017629654 98.0094046816\n",
      "quantity_of_chemical_fertilizers_used___policy 2011 2013 3 74 24.369747899159663 29.72972972972973 1.2199440820130476 -8.95413798691 98.0094046816\n",
      "quantity_of_chemical_fertilizers_used___policy 2013 2015 0 249 12.430632630410656 17.670682730923694 1.4215433161216293 -27.3660277734 -22.2788003332\n",
      "quantity_of_chemical_fertilizers_used___policy 2013 2015 1 368 16.028708133971293 18.75 1.169776119402985 3.01906953167 -22.2788003332\n",
      "quantity_of_chemical_fertilizers_used___policy 2013 2015 2 180 5.785123966942149 12.777777777777777 2.2087301587301584 -16.2521572793 -22.2788003332\n",
      "quantity_of_chemical_fertilizers_used___policy 2013 2015 3 129 22.188449848024316 26.356589147286826 1.187851757459913 -57.6725140427 -22.2788003332\n",
      "quantity_of_improved_seeds_used___policy 2011 2013 0 72 6.666666666666667 15.277777777777779 2.291666666666667 63.2173423708 98.0094046816\n",
      "quantity_of_improved_seeds_used___policy 2011 2013 1 84 18.441558441558442 28.57142857142857 1.5492957746478873 187.719676667 98.0094046816\n",
      "quantity_of_improved_seeds_used___policy 2011 2013 2 144 16.693944353518823 11.805555555555555 0.7071759259259259 25.8017629654 98.0094046816\n",
      "quantity_of_improved_seeds_used___policy 2011 2013 3 47 24.369747899159663 23.404255319148938 0.9603815113719736 -8.95413798691 98.0094046816\n",
      "quantity_of_improved_seeds_used___policy 2013 2015 0 89 12.430632630410656 11.235955056179774 0.9038924558587479 -27.3660277734 -22.2788003332\n",
      "quantity_of_improved_seeds_used___policy 2013 2015 1 169 16.028708133971293 15.976331360946746 0.9967323147575732 3.01906953167 -22.2788003332\n",
      "quantity_of_improved_seeds_used___policy 2013 2015 2 66 5.785123966942149 13.636363636363635 2.3571428571428568 -16.2521572793 -22.2788003332\n",
      "quantity_of_improved_seeds_used___policy 2013 2015 3 82 22.188449848024316 26.82926829268293 1.2091546942866689 -57.6725140427 -22.2788003332\n",
      "uses_irrigation___policy 2011 2013 0 29 6.666666666666667 20.689655172413794 3.103448275862069 63.2173423708 98.0094046816\n",
      "uses_irrigation___policy 2011 2013 1 60 18.441558441558442 28.333333333333332 1.5363849765258215 187.719676667 98.0094046816\n",
      "uses_irrigation___policy 2011 2013 2 56 16.693944353518823 14.285714285714285 0.8557422969187675 25.8017629654 98.0094046816\n",
      "uses_irrigation___policy 2011 2013 3 12 24.369747899159663 8.333333333333332 0.34195402298850575 -8.95413798691 98.0094046816\n",
      "uses_irrigation___policy 2013 2015 0 52 12.430632630410656 32.69230769230769 2.6299793956043955 -27.3660277734 -22.2788003332\n",
      "uses_irrigation___policy 2013 2015 1 100 16.028708133971293 23.0 1.4349253731343283 3.01906953167 -22.2788003332\n",
      "uses_irrigation___policy 2013 2015 2 41 5.785123966942149 17.073170731707318 2.951219512195122 -16.2521572793 -22.2788003332\n",
      "uses_irrigation___policy 2013 2015 3 18 22.188449848024316 27.77777777777778 1.251902587519026 -57.6725140427 -22.2788003332\n"
     ]
    }
   ],
   "source": [
    "# Compute evidence of movement across years, the lift due to movement in relevant inputs.\n",
    "df = df_all\n",
    "series = pd.DataFrame()\n",
    "for imp_feat in imp_feats:\n",
    "    for output in ['segment_crop_sales___output']:\n",
    "        years = ['2011', '2013', '2015']\n",
    "        raw_output = 'crop_sales___output'\n",
    "        coef= {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}\n",
    "        for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "            #try:\n",
    "            z1 = for_year(output, y1)\n",
    "            z2 = for_year(output, y2)\n",
    "            r1 = for_year(raw_output, y1)\n",
    "            r2 = for_year(raw_output, y2)\n",
    "            f1 = for_year(imp_feat,y1)\n",
    "            f2 = for_year(imp_feat,y2)\n",
    "            df[output + '_change' + y1] = (df[z1]!=df[z2])\n",
    "            df[output + '_increase' + y1] = (df[z1]<df[z2])\n",
    "            df[output + '_decrease' + y1] = (df[z1]>df[z2])\n",
    "            expected = df[z1].apply(lambda x: coef[x])\n",
    "            df[imp_feat+'_increase'+y1] = (imp_df[f1]<imp_df[f2])\n",
    "            df[output + '_inversion' + y1] = (df[z2]-df[z1])*(df[f2]-df[f1])*expected\n",
    "            df[output + '_change_value' + y1] = (imp_df[r2]-imp_df[r1])\n",
    "\n",
    "        for per in [50,55,60,70,75,80,85,90,95]:\n",
    "            for seg in range(4):\n",
    "                exp_y_i = []\n",
    "                exp_y = []\n",
    "                exp = []\n",
    "                exp_i = []\n",
    "                for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "                    year = y1\n",
    "                    weight = for_year('weight', year)\n",
    "                    seg_y = for_year(output, year)\n",
    "                    df_seg = df[df[seg_y]==seg]\n",
    "                    df_seg = df_seg.loc[df_seg[output + '_change_value' + year].dropna().index]\n",
    "                    thres = np.percentile(df_seg[output + '_change_value' + year].dropna(), per)\n",
    "                    high_df = df_seg[df_seg[output + '_change_value' + year].apply(lambda x : x >= thres)]\n",
    "                    exp_y_i.append(len(high_df[(high_df[imp_feat + '_increase'+year]==True)]))\n",
    "                    exp_y.append(len(high_df))\n",
    "                    exp.append(len(df_seg))\n",
    "                    exp_i.append(len(df_seg[df_seg[imp_feat + '_increase'+year]==True]))\n",
    "                    avg_y_i = np.mean(high_df[(high_df[imp_feat + '_increase'+year]==True)][output + '_change_value' + year])\n",
    "                    avg_y = np.mean(high_df[output + '_change_value' + year].dropna())\n",
    "                    avg = np.median(df[output + '_change_value' + year].dropna())\n",
    "                row = {}\n",
    "                row['threshold'] = per\n",
    "                row['input'] = imp_feat\n",
    "                row['cluster'] = seg\n",
    "                row['movement overall'] = (sum(exp_y)/sum(exp))*100.0\n",
    "                row['movement conditioned'] = (sum(exp_y_i)/sum(exp_i))*100.0\n",
    "                row['movement lift'] = (sum(exp_y_i)/sum(exp_i))/(sum(exp_y)/sum(exp))\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "\n",
    "series.to_csv('../results/threshold-lift.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
