{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import Imputer\n",
    "from fancyimpute import SoftImpute\n",
    "from sklearn.metrics import roc_curve, auc, silhouette_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from scipy import stats\n",
    "import glmnet_python\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot \n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "import math\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome-aware Policy Recommendations\n",
    "\n",
    "This file does outcome aware clustering followed by cluster-wise regression to get the policy recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Inputs and Outputs\n",
    "\n",
    "We only use one output at a time. Here are the list of outputs we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_output = 'crop_sales'\n",
    "# Other options include:\n",
    "# 'crop_sales_growth',\n",
    "# 'expenditure',\n",
    "# 'food_expenditure_diversification',\n",
    "# 'has_medical_assistance',\n",
    "# 'no_food_deficiency',\n",
    "# 'productivity',\n",
    "# 'productivity_growth',\n",
    "# 'children_education'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "children_education___output___ethiopia_2015                  16.456970\n",
       "food_expenditure_diversification___output___ethiopia_2015     1.962758\n",
       "owns_land_certificate___policy___ethiopia_2015                1.157524\n",
       "quantity_of_improved_seeds_used___policy___ethiopia_2015      4.579768\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_df = read_csv('../data/ethiopia_v23_raw.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "y = 2015\n",
    "\n",
    "# Utility function for getting variables for each of the 3 years: 2011,13,15.\n",
    "def for_year(var, year):\n",
    "    return var + '___ethiopia_' + str(year)\n",
    "\n",
    "y_reg = re.compile('.*___.*' + str(y) + '$')\n",
    "y_cols = list(filter(y_reg.search, imp_cols))\n",
    "imp_df = imp_df[y_cols]\n",
    "output = for_year('crop_sales___output', y)\n",
    "imp_df = imp_df.loc[imp_df[output].dropna().index]\n",
    "missing = (len(imp_df.index) - imp_df.count())/len(imp_df.index)*100.0\n",
    "missing[missing > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature choice for regression based on correl and vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: average_temperature___ethiopia_2015 : 18.472989798562818\n",
      "Removed: elevation___ethiopia_2015 : 13.182135429557304\n",
      "Removed: household_size___ethiopia_2015 : 7.279737966100593\n",
      "Removed: average_precipitation___ethiopia_2015 : 5.572895977396103\n",
      "Removed: household_head_is_male___ethiopia_2015 : 3.6755065060487286\n",
      "Removed: distance_to_population_center___ethiopia_2015 : 3.0143175152781425\n",
      "Removed: number_of_plough_owned___policy___ethiopia_2015 : 2.2687246376740005\n",
      "Removed: number_of_oxen_owned___policy___ethiopia_2015 : 2.0068706529980638\n",
      "Removed: uses_extension_program___policy___ethiopia_2015 : 1.7790754637073296\n",
      "Removed: land_surface___ethiopia_2015 : 1.5292427289340589\n",
      "\n",
      "Final chosen features \n",
      "\n",
      "prevent_damage___policy___ethiopia_2015 1.19743605819\n",
      "has_saved___policy___ethiopia_2015 1.13110452009\n",
      "number_of_droughts___ethiopia_2015 1.07379545835\n",
      "uses_irrigation___policy___ethiopia_2015 1.06132163347\n",
      "quantity_of_chemical_fertilizers_used___policy___ethiopia_2015 1.03744894401\n",
      "distance_to_market___ethiopia_2015 1.32750829077\n",
      "number_of_hired_workers___policy___ethiopia_2015 1.07309856147\n",
      "Removed: number_of_plough_owned___policy___ethiopia_2015 : 2.0964472039050497\n",
      "Removed: uses_extension_program___policy___ethiopia_2015 : 1.8121315670631386\n",
      "\n",
      "Final chosen features \n",
      "\n",
      "number_of_oxen_owned___policy___ethiopia_2015 1.34689380808\n",
      "prevent_damage___policy___ethiopia_2015 1.27033550951\n",
      "has_saved___policy___ethiopia_2015 1.07660741997\n",
      "uses_irrigation___policy___ethiopia_2015 1.03508670546\n",
      "quantity_of_chemical_fertilizers_used___policy___ethiopia_2015 1.04582659867\n",
      "number_of_hired_workers___policy___ethiopia_2015 1.09626959341\n"
     ]
    }
   ],
   "source": [
    "imp_df = read_csv('../data/ethiopia_v23_raw.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "\n",
    "\n",
    "def select_all_year_feats():\n",
    "    imp_feats = set([])\n",
    "#     for y in [2013, 2015]:\n",
    "    for y in [2011, 2013, 2015]:\n",
    "        y_reg = re.compile('.*___policy.*' + str(y) + '$')\n",
    "        y_cols = set(filter(y_reg.search, imp_cols))\n",
    "        y_cols = set([t.replace('___ethiopia_' + str(y), '') for t in y_cols])\n",
    "        if len(imp_feats) == 0:\n",
    "            imp_feats = y_cols\n",
    "        else:\n",
    "            imp_feats = imp_feats.intersection(y_cols)\n",
    "    return imp_feats\n",
    "\n",
    "def select_corr_feats(thres = 0.1, only_policy=False):\n",
    "    ccs = pd.read_csv('../data/cross_correls_ethiopia_2015_v22.csv')\n",
    "    output = for_year('crop_sales___output', 2015)\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    ccs = ccs[ccs[output] > thres]\n",
    "    imp_feats = ccs['Unnamed: 0']\n",
    "    non_raw_reg = re.compile('^((?!lives_in|longitude|latitude).)*$')\n",
    "    imp_feats = list(filter(non_raw_reg.search, imp_feats))\n",
    "    if only_policy:\n",
    "        y_reg = re.compile('.*___policy.*$')\n",
    "        imp_feats = list(filter(y_reg.search, imp_feats))\n",
    "    return imp_feats\n",
    "\n",
    "def normalize():\n",
    "    raw_df = read_csv('../data/ethiopia_v23_raw.csv')\n",
    "    output = for_year('crop_sales___output', 2015)\n",
    "    raw_df = raw_df.loc[raw_df[output].dropna().index]\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    return raw_df\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif_score\n",
    "\n",
    "def spearman():\n",
    "    collinear = raw_df[feats].corr('spearman')\n",
    "\n",
    "    cols = collinear.columns\n",
    "    for c in collinear.columns:\n",
    "        for v in zip(cols, collinear[c]):\n",
    "            if v[1] > 0.4 and v[1] < 0.5 and v[1] != 1:\n",
    "                print (c, v[0], v[1])\n",
    "    collinear.to_csv('../results/output_collinear.csv')\n",
    "    \n",
    "def vif(feats, raw_df, thres=5, debug=False):    \n",
    "    while True:\n",
    "        policy = raw_df[feats].as_matrix()\n",
    "        max_vif = 0\n",
    "        max_vif_feat = None\n",
    "        for i, f in enumerate(feats):\n",
    "            if max_vif < vif_score(policy, i):\n",
    "                max_vif = vif_score(policy, i)\n",
    "                max_vif_feat = f\n",
    "        if max_vif < thres:\n",
    "            break\n",
    "        feat_set = set(feats)\n",
    "        feat_set.remove(max_vif_feat)\n",
    "        if debug:\n",
    "            print ('Removed: {0} : {1}'.format(max_vif_feat, max_vif))\n",
    "        feats = list(feat_set)\n",
    "    if debug:\n",
    "        print ('\\nFinal chosen features \\n')\n",
    "        for i, f in enumerate(feats):\n",
    "            print (f, vif_score(policy, i))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "raw_df = normalize()\n",
    "vifs = []\n",
    "\n",
    "# Grid search to find correlation and VIF score thresholds.\n",
    "def grid_search():\n",
    "    c_thresholds = [0.05, 0.07, 0.1, 0.12, 0.14,  0.15, 0.16, 0.17, 0.2,0.25,0.3]\n",
    "    v_thresholds = [2,3,4,5]\n",
    "    for thres in v_thresholds:\n",
    "        imp_feats = select_corr_feats(0.1)\n",
    "        v = vif(imp_feats, raw_df, thres)\n",
    "        vifs += [len(v)]\n",
    "\n",
    "# Chosen values of thresholds after grid search.    \n",
    "c = 0.1\n",
    "v = 1.5\n",
    "imp_feats = select_corr_feats(c)\n",
    "all_relevant = vif(imp_feats, raw_df, v, True)\n",
    "\n",
    "imp_feats = select_corr_feats(c, True)\n",
    "policy_relevant = vif(imp_feats, raw_df, v, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used for outcome aware clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for segmentation based on output directly. This was used for baseline for outcome-aware clustering.\n",
    "# Currently not used.\n",
    "def get_classes(output_var, pred):\n",
    "    max_bins = 3\n",
    "    _, boundaries = np.histogram(output_var, bins=max_bins)\n",
    "    classes = np.digitize(pred, bins=boundaries)\n",
    "    return classes, max_bins\n",
    "\n",
    "# Writes policy recommendations in results files after doing cluster specific outcome-aware clustering. \n",
    "def run_regressions(in_name, out_dir, non_policy_inputs, segment_variables, inputs, output, year):\n",
    "    # args:\n",
    "    # in_name: input file name\n",
    "    # out_dir: output directory where the results (coefficients, averages per cluster, etc) are written.\n",
    "    # non_policy_inputs: list of all non-actionable inputs\n",
    "    # segment_variables: list of variables on which the outcome-aware clustering is done\n",
    "    # inputs: list of all actionable inputs\n",
    "    # output: the output variable\n",
    "    # year: the year for which the policy recommendations are generated\n",
    "    \n",
    "    # table for storing the average regression error for population vs cluster specific clusters.\n",
    "    global table\n",
    "    # table for storing coefficients of the regressions\n",
    "    global coef_table\n",
    "    # table for storing average, std deviation, std error values of the inputs for each of the clusters\n",
    "    global avg_table\n",
    "    # Map of coefficients, this is an intermediate map which gets then translated to coef_table\n",
    "    global coef_map\n",
    "    table = pd.DataFrame()\n",
    "    avg_table = pd.DataFrame()\n",
    "    coef_map = {}\n",
    "    coef_table = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except:\n",
    "        print(\"Dir exists\")\n",
    "    \n",
    "    # pick which regression algos to use\n",
    "    regression_algorithms = ()\n",
    "   \n",
    "    # Read raw and normalized values\n",
    "    df = read_csv(in_name)\n",
    "    raw_df = read_csv(in_name.replace('normed', 'raw'))\n",
    "    \n",
    "    # drop rows with unobserved income\n",
    "    df = df.loc[df[output].dropna().index]\n",
    "    raw_df = raw_df.loc[df[output].dropna().index]\n",
    "    df['nid']= df.index.tolist()\n",
    "\n",
    "    # select % of data in test set\n",
    "    test_split = 0.2\n",
    "    \n",
    "    # perform matrix completion. completed is returned as a np array\n",
    "    # we've discussed not using it, but I left it in because I wasn't able to\n",
    "    # fit the StandardScaler with a DataFrame that contained NaN's\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "\n",
    "    # reconstruct dataframes for normalized and raw matrix with completed matrix\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    raw_df['nid']= raw_df.index.tolist()\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    \n",
    "    # Add productivity as a synthetic output based on crop sales per land surface.\n",
    "    raw_df['productivity'] = raw_df[for_year('crop_sales___output', 2015)]/raw_df[for_year('land_surface', 2015)]\n",
    "    raw_df['productivity'] = raw_df['productivity'].apply(lambda x: 0 if x == np.inf else x)\n",
    "\n",
    "    # Re-scale completed data by doing z-scoring per feature.\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    \n",
    "    # Optionally add aggregated input variables which could be used. This is however not used in the regression variables.\n",
    "    mat[for_year('animals___policy', year)] = mat[for_year('number_of_oxen_owned___policy', year)] + mat[for_year('number_of_plough_owned___policy', year)] + mat[for_year('number_of_axe_owned___policy', year)] + mat[for_year('number_of_pick_axe_owned___policy', year)] + mat[for_year('number_of_sickle_owned___policy', year)]\n",
    "    mat[for_year('tools___policy', year)] = mat[for_year('number_of_axe_owned___policy', year)] + mat[for_year('number_of_pick_axe_owned___policy', year)] + mat[for_year('number_of_sickle_owned___policy', year)]\n",
    "    inputs += [for_year('animals___policy', year), for_year('tools___policy', year)]\n",
    "\n",
    "    y = mat[output]\n",
    "    x = mat[inputs]\n",
    "    \n",
    "    # When using Lasso, this function returns the best lambda using glmnet.\n",
    "    def update_best_lambda(x_scaled):\n",
    "        copy_y = np.array(y, dtype=np.float64)\n",
    "        print (copy_y)\n",
    "        fit = cvglmnet(x = x_scaled.copy(), y = copy_y)\n",
    "        print ('Best alpha={0}'.format(fit['lambda_min']))\n",
    "        return fit['lambda_min']\n",
    "    \n",
    "    # Split test/train\n",
    "    indices = range(len(mat))\n",
    "    x_train, x_test, y_train, y_test, ind_train, ind_test = \\\n",
    "        train_test_split(x, y, indices, test_size=test_split, random_state=42)\n",
    "    \n",
    "    # Returns the input rows as per the split per testing/training.\n",
    "    def get_train_test(input_vars):\n",
    "        x = mat[input_vars].copy()\n",
    "        x_scaled = StandardScaler()\n",
    "        x_scaled.fit(x)\n",
    "        x_sc = x_scaled.transform(x)\n",
    "        # reconstruct DataFrame\n",
    "        x = pd.DataFrame(x_sc, columns=x.columns)\n",
    "        training_x = x.iloc[ind_train, :]\n",
    "        testing_x = x.iloc[ind_test, :]\n",
    "        return x_sc, training_x, testing_x\n",
    "    \n",
    "    # This is to convert baseline segments into one-hot encoding. \n",
    "    # Currently not used.\n",
    "    def digitize(output_var, pred):\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        classes, max_bins = get_classes(output_var, pred)\n",
    "        b_classes = label_binarize(classes, range(max_bins))\n",
    "        return b_classes\n",
    "    \n",
    "    # Do regressions without clustering, this gives the population level recommendations.\n",
    "    def calc_unsegmented(baseline, x_train, x_test): \n",
    "        global table\n",
    "        global coef_map\n",
    "\n",
    "        # Convert test dataframe to list to pass to regression equation.\n",
    "        keys_list = []\n",
    "        y_list = []\n",
    "        for k, v in y_test.iteritems():\n",
    "            keys_list.append(k)\n",
    "            y_list.append(v)\n",
    "\n",
    "        # run regressions on population level using OLS.\n",
    "        name = 'OLS'\n",
    "        # If you want to run Lasso, change this to True.\n",
    "        run_regularized = False\n",
    "        model = sm.OLS(y_train, x_train)\n",
    "        if run_regularized:\n",
    "            fit = model.fit_regularized(alpha=0.006, refit=True)\n",
    "        else:\n",
    "            fit = model.fit()\n",
    "        print(\"Confidence intervals are {0}\".format(fit.conf_int()))\n",
    "        y_pred = fit.predict(x_test)\n",
    "\n",
    "        # add MSE and AUC to table\n",
    "        try:\n",
    "            test_c = digitize(y, y_test)\n",
    "            pred_c = digitize(y, y_pred)\n",
    "            auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "        except ValueError:\n",
    "            auc_c = 0.5\n",
    "        mse = mean_squared_error(y_test,y_pred)\n",
    "        scaled_mse = (mse/np.std(y))\n",
    "        new_row = pd.DataFrame({'model': name, 'segment': '', 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': False}, index=[0])\n",
    "        table = table.append(new_row, ignore_index=True)\n",
    "\n",
    "        # add coefficients to map\n",
    "        coef_map[name + '_' + baseline] = fit.params\n",
    "        lower_bounds = []\n",
    "        upper_bounds = []\n",
    "        ci = fit.conf_int()\n",
    "        if run_regularized:\n",
    "            for ci in fit.conf_int():\n",
    "                lower_bounds += [ci[0]]\n",
    "                upper_bounds += [ci[1]]\n",
    "        else:\n",
    "            for ci_row in ci.iterrows():\n",
    "                lower_bounds += [ci_row[1][0]]\n",
    "                upper_bounds += [ci_row[1][1]]\n",
    "        coef_map[name + '_' + baseline + '_lower_bound'] = lower_bounds\n",
    "        coef_map[name + '_' + baseline + '_upper_bound'] = upper_bounds\n",
    "    \n",
    "    def calc_segmented(segment_variables, baseline, x_train, x_test):\n",
    "        global table\n",
    "        global coef_map\n",
    "        global avg_table\n",
    "\n",
    "        # list of variables to segment on.\n",
    "        segment_vars = list(segment_variables.keys())\n",
    "        \n",
    "        # Choosing iteratively the features to do outcome aware clustering on.\n",
    "        # Returns the silhouette coefficient and the kmeans data object with labels in them.\n",
    "        def iterative_clustering(sc_thres, corr_thres, alpha):\n",
    "            max_sc = 0\n",
    "            best_kmeans = None\n",
    "\n",
    "            sc_incr = 1e-8\n",
    "            sc_thres = -1e-1\n",
    "            C = set([])\n",
    "            avail = set(segment_variables.keys())\n",
    "            prev_sc = 0\n",
    "            min_k = 4\n",
    "            corr_thres = 0.05\n",
    "            while sc_incr >= sc_thres:\n",
    "                u_f = 0\n",
    "                best_f = None\n",
    "                sc_best = 0\n",
    "                for f in avail:\n",
    "                    if segment_variables[f] < corr_thres:\n",
    "                        continue\n",
    "                    seg_data = mat[list(avail) + [f]]\n",
    "                    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "                    labels = kmeans.labels_\n",
    "                    sc = silhouette_score(seg_data, labels)\n",
    "                    sc_delta = sc - prev_sc\n",
    "                    if u_f < (segment_variables[f] + (alpha)*sc_delta):\n",
    "                        u_f = (segment_variables[f] + (alpha)*sc_delta)\n",
    "                        best_f = f\n",
    "                        sc_best = sc\n",
    "                if best_f == None:\n",
    "                    break\n",
    "                avail.remove(best_f)\n",
    "                C.add(best_f)\n",
    "                sc_incr = sc - prev_sc\n",
    "                prev_sc = sc\n",
    "            \n",
    "            seg_data = mat[list(C)]\n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "            labels = kmeans.labels_\n",
    "            sc = silhouette_score(seg_data, labels)\n",
    "            return sc, kmeans\n",
    "\n",
    "        \n",
    "        def add_clusters():\n",
    "            # elbow method\n",
    "            sse = []\n",
    "           \n",
    "            # Multiply by correlation coefficient to make weigh feature distances as per outcome importance.\n",
    "            seg_data = mat[segment_vars]\n",
    "            for seg_var in segment_vars:\n",
    "                seg_data[seg_var] = seg_data[seg_var].apply(lambda x: x*segment_variables[seg_var])\n",
    "    \n",
    "            ### Plot elbow. Currently not run. Do this once before fixing k.\n",
    "            def plot_elbow():\n",
    "                for k in range(1,9):\n",
    "                    kmeans = KMeans(n_clusters=k).fit(seg_data)\n",
    "                    labels = kmeans.labels_\n",
    "                    sse.append(sum(np.min(cdist(seg_data, kmeans.cluster_centers_, 'euclidean'), axis=1)) / seg_data.shape[0])\n",
    "\n",
    "                plt.clf()\n",
    "                plt.plot(range(1,9), sse)\n",
    "                plt.xlabel('k')\n",
    "                plt.ylabel('Sum of squared error')\n",
    "                plt.savefig(os.path.join(out_dir, 'elbow.png'))\n",
    "                print(sse)\n",
    "                min_k = sse.index(min(sse))\n",
    "                print (min_k)\n",
    "\n",
    "            ### Parameter search for iterative clustering. Use this while doing search over parameters\n",
    "            def best_kmeans_params():\n",
    "                max_sc = -1\n",
    "                best_kmeans = None\n",
    "                for sc_thres in [-0.1, -0.05, -0.01, -0.001, -1e-3, 0, 1e-3, 1e-2]:\n",
    "                    for corr_thres in [0.05, 0.06, 0.08, 0.1, 0.12, 0.15]:\n",
    "                        for alpha in [1e-4,1e-3,1e-2,0.1,1,10,100,1000,10000]:\n",
    "                            sc, kmeans = iterative_clustering(sc_thres, corr_thres, alpha)\n",
    "                            if sc > max_sc:\n",
    "                                max_sc = sc\n",
    "                                best_kmeans = kmeans\n",
    "                                print ('Best found: {0}, {1}, {2}'.format(sc_thres, corr_thres, alpha))\n",
    "            \n",
    "                return best_kmeans\n",
    "    \n",
    "            min_k = 4\n",
    "            # After features are chosen, simply do kmeans. Uncomment below line to redo the feature choice.\n",
    "            # kmeans = best_kmeans_params()\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "            labels = kmeans.labels_\n",
    "            mat['cluster'] = labels\n",
    "            \n",
    "            # Record average segment variable values per cluster and write to file.\n",
    "            means = []\n",
    "            for i in np.unique(labels):\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "                values = raw_clus[output].as_matrix()\n",
    "                weights = raw_clus[for_year('weight', year)].as_matrix()\n",
    "                average = np.average(values, weights=weights)\n",
    "                means.append(average)\n",
    "            sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "            mat['cluster'] = mat['cluster'].apply(lambda x: sorted_ids.index(x))\n",
    "\n",
    "            # Add location features to plot the clusters on the map.\n",
    "            select_variables = [output, 'latitude___ethiopia_2015', 'longitude___ethiopia_2015', 'nid'] + segment_vars\n",
    "            select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "            all_output = pd.concat([mat['cluster'], raw_df[list(select_variables)]], 1)\n",
    "            all_output.to_csv(os.path.join(out_dir,'clus_' + baseline + '_' + output + '.csv'))\n",
    "            return min_k\n",
    "            \n",
    "        \n",
    "        # Add segments. Used for heuristic output based segmentation for baselines. Not used anymore.\n",
    "        def add_segments():\n",
    "            median_segments = {}\n",
    "            for seg_var in segment_vars:\n",
    "                #binary\n",
    "                if len(np.unique(mat[seg_var])) <= 3:\n",
    "                    median_segments[seg_var] = 0\n",
    "                else:\n",
    "                    median_segments[seg_var] = np.median(mat[seg_var])\n",
    "            mat['cluster'] = 0\n",
    "            for seg_var in segment_vars:\n",
    "                mat['cluster'] = 2*mat['cluster'] + [int(x) for x in mat[seg_var] > median_segments[seg_var]]\n",
    "            return int(math.pow(2, len(segment_vars)))\n",
    "\n",
    "        # Add location specific segments. Used as baseline. Not used anymore.\n",
    "        def add_location_segments():\n",
    "            locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "            i = 0\n",
    "            mat['cluster'] = 0\n",
    "            for l in sorted(locations):\n",
    "                loc_feature = 'lives_in_' + l + '___ethiopia_' + str(year)\n",
    "                loc_val = mat[loc_feature].apply(lambda x: 0 if x < 0 else 1)\n",
    "                mat['cluster'] = mat['cluster'] + (i*loc_val)\n",
    "                i += 1\n",
    "            return len(locations) + 1\n",
    "            \n",
    "        # Base function which aggregates cluster based regression procedures, given the clusters.\n",
    "        def _run(max_clusters, method_name):\n",
    "            global table\n",
    "            global avg_table\n",
    "            global coef_map\n",
    "            # reg_clus keeps predictions from clustered regressions along with keys\n",
    "            reg_clus = dict()\n",
    "            name = 'OLS'\n",
    "            reg_clus[name] = {}\n",
    "\n",
    "            # need new dataframes with only training and test rows.\n",
    "            # we use this when looping through clusters\n",
    "            train_mat = mat.loc[ind_train]\n",
    "            test_mat = mat.loc[ind_test]\n",
    "            train_size = len(train_mat)\n",
    "            series = {}\n",
    "            series[output] = []\n",
    "            for seg in segment_vars:\n",
    "                series[seg] = []\n",
    "            series = pd.DataFrame()\n",
    "            row = {}\n",
    "            raw_cols = raw_df.columns.values\n",
    "            \n",
    "            # Keep only normed values.\n",
    "            raw_reg = re.compile('^((?!norm).)*2015$')\n",
    "            avg_variables = list(filter(raw_reg.search, raw_cols))\n",
    "\n",
    "            # Iterate through each of the clusters and\n",
    "            # 1. Store the average input variables per cluster\n",
    "            # 2. Do Regressions per cluster\n",
    "            for i in range(max_clusters):\n",
    "                train_clus = x_train.loc[train_mat['cluster'] == i]\n",
    "                train_y = y_train.loc[train_mat['cluster'] == i]\n",
    "                test_clus = x_test.loc[test_mat['cluster'] == i]\n",
    "                test_y = y_test.loc[test_mat['cluster'] == i]\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "\n",
    "                # Store average input variables.\n",
    "                for seg in avg_variables:\n",
    "                    values = raw_clus[seg].as_matrix()\n",
    "                    weights = raw_clus[for_year('weight', year)].as_matrix()\n",
    "                    average = np.average(values, weights=weights)\n",
    "                    row['mean_' + seg] = average\n",
    "                    row['avg_' + seg] = np.mean(values)\n",
    "                    variance = np.average((values-average)**2, weights=weights)\n",
    "                    row['stddev_' + seg] = math.sqrt(variance)\n",
    "                    row['stderr_' + seg] = math.sqrt(variance)/math.sqrt(len(values))\n",
    "                    row['25ile_' + seg] = np.percentile(values, 25)\n",
    "                    row['75ile_' + seg] = np.percentile(values, 75)\n",
    "\n",
    "                row['index'] = i\n",
    "                row['size'] = len(raw_clus)\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "                avg_table = avg_table.append(new_row, ignore_index=True)\n",
    "                cluster_percent = (len(train_clus)*100.0)/train_size\n",
    "                if train_clus.empty or test_clus.empty or cluster_percent < 5:\n",
    "                    print(\"Cluster size too small, retry!\")\n",
    "                    continue\n",
    "\n",
    "                keys_list = []\n",
    "                y_list = []\n",
    "                for k, v in test_y.iteritems():\n",
    "                    keys_list.append(k)\n",
    "                    y_list.append(v)\n",
    "\n",
    "                model = sm.OLS(train_y, train_clus)\n",
    "                run_regularized = False\n",
    "                if run_regularized:\n",
    "                    fit = model.fit_regularized(alpha=0.00617565, refit=True)\n",
    "                else:\n",
    "                    fit = model.fit()\n",
    "                print(\"Confidence intervals are {0}\".format(fit.conf_int()))\n",
    "                y_pred = fit.predict(test_clus)\n",
    "\n",
    "                for a, b in enumerate(y_pred):\n",
    "                    t = reg_clus[name]\n",
    "                    t[keys_list[a]] = b\n",
    "\n",
    "                coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = fit.params\n",
    "                lower_bounds = []\n",
    "                upper_bounds = []\n",
    "                ci = fit.conf_int()\n",
    "                if run_regularized:\n",
    "                    for ci in fit.conf_int():\n",
    "                        lower_bounds += [ci[0]]\n",
    "                        upper_bounds += [ci[1]]\n",
    "                else:\n",
    "                    for ci_row in ci.iterrows():\n",
    "                        lower_bounds += [ci_row[1][0]]\n",
    "                        upper_bounds += [ci_row[1][1]]\n",
    "                coef_map[name + '_' + method_name + '_' + str(i) + '_lower_bound'] = lower_bounds\n",
    "                coef_map[name + '_' + method_name + '_' + str(i) + '_upper_bound'] = upper_bounds\n",
    "\n",
    "            # plot sorted correlation between average inputs across clusters.\n",
    "            sorted_series = series.sort_values(['mean_' + output])\n",
    "            raw_segment_vars = avg_variables\n",
    "            for seg in raw_segment_vars:\n",
    "                plt.clf()\n",
    "                plt.plot(sorted_series['mean_' + output].as_matrix(), sorted_series['mean_'+seg].as_matrix(), marker='o')\n",
    "                plt.xlabel('Average ' + output.replace('___output___ethiopia_2015', '') + ' output')\n",
    "                plt.ylabel('Average ' + seg.replace('___policy___ethiopia_' +str(year), '').replace('_',' '))\n",
    "                plt.savefig(os.path.join(out_dir, 'plot_' + seg + '_' + output + '.pdf'))\n",
    "                \n",
    "            # add mse's to table\n",
    "            keys = sorted(y_test.keys())\n",
    "            sort_t = []\n",
    "            sort_p = []\n",
    "            for key in keys:\n",
    "                if key not in y_test or key not in reg_clus[name]:\n",
    "                    continue\n",
    "                sort_t.append(reg_clus[name][key])\n",
    "                sort_p.append(y_test[key])\n",
    "\n",
    "            try:\n",
    "                test_c = digitize(y, sort_t)\n",
    "                pred_c = digitize(y, sort_p)\n",
    "                auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "            except ValueError:\n",
    "                auc_c = 0.5\n",
    "            mse = mean_squared_error(sort_t,sort_p)\n",
    "            scaled_mse = (mse/np.std(y))\n",
    "            new_row = pd.DataFrame({'model': name, 'segment': ','.join(segment_vars), 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': True, 'method': method_name}, index=[0])\n",
    "            table = table.append(new_row, ignore_index=True)\n",
    "                \n",
    "    \n",
    "        ##### Run grouped regressions\n",
    "        if (len(segment_vars) > 1):\n",
    "            _run(add_clusters(), 'clustered')\n",
    "        \n",
    "        # Uncomment if you want to get baseline if segmentation is done based on raw output value/location.\n",
    "        #_run(add_location_segments(), 'segmented')\n",
    "        #_run(add_segments(), 'segmented')\n",
    "        \n",
    "\n",
    "    # Runs outcome aware clustering policy recommendations along with population level baseline given the set\n",
    "    # of input variables to be considered for regression.\n",
    "    def run_with_inputs(input_vars, name):\n",
    "        # map to be used in tracking coefficients\n",
    "        global coef_map\n",
    "        global coef_table\n",
    "        global x_train\n",
    "        global x_test\n",
    "        coef_map = {}\n",
    "        x_scaled, x_train, x_test = get_train_test(input_vars)\n",
    "        \n",
    "        # Use this lambda when running Lasso. Currently unused as we use OLS.\n",
    "        update_best_lambda(x_scaled)\n",
    "        calc_unsegmented(name, x_train, x_test)\n",
    "        calc_segmented(segment_variables, name, x_train, x_test)\n",
    "        \n",
    "        # Convert coef_map to coef_table with column headers.\n",
    "        for k,v in sorted(coef_map.items()):\n",
    "            kvp = dict()\n",
    "            kvp['model'] = k\n",
    "            kvp['inputs'] = name\n",
    "\n",
    "            for val,invar in zip(v,input_vars):\n",
    "                kvp[invar] = val\n",
    "\n",
    "            new_row = pd.DataFrame(kvp, index=[0])\n",
    "            coef_table = coef_table.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # Use the relevant variables generated above for regression here.\n",
    "    # TODO(ananth): Plumb it through within the function.\n",
    "    global all_relevant, policy_relevant\n",
    "    print ('Relevant variables')\n",
    "    print(all_relevant, policy_relevant)\n",
    "    run_with_inputs(all_relevant, 'Highly correlated all')\n",
    "    run_with_inputs(policy_relevant, 'Highly correlated policy')\n",
    "    \n",
    "    # Baseline using all possible inputs without removing collinear variables. Uncomment for comparison.\n",
    "    # input_vars = inputs + non_policy_inputs\n",
    "    # run_with_inputs(input_vars, 'All variables')\n",
    "    \n",
    "    # save coefficient and output tables\n",
    "    coef_table.to_csv(os.path.join(out_dir,'coef_' + output + '.csv'))\n",
    "    table.to_csv(os.path.join(out_dir,output + '.csv'))\n",
    "    avg_table.to_csv(os.path.join(out_dir,output + '_avg' + '.csv'))\n",
    "    \n",
    "\n",
    "# Utility function to complete the missing matrix features.\n",
    "def complete(df):\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    return mat\n",
    "\n",
    "\n",
    "# Utility function to do clustering. This is currently used to do multiple year clusters comparison.\n",
    "def get_clusters(mat, output, segment_vars, out_dir):\n",
    "    segment_vars = list(segment_vars.keys())\n",
    "    seg_data = mat[segment_vars]\n",
    "    min_k = 4\n",
    "    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "    labels = kmeans.labels_\n",
    "    means = []\n",
    "    for i in np.unique(labels):\n",
    "        df_clus = mat.loc[labels == i]\n",
    "        means.append(np.mean(df_clus[output].as_matrix()))\n",
    "    sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "    mat['segment_'+ output] = labels\n",
    "    mat['segment_'+ output] = mat['segment_'+ output].apply(lambda x: sorted_ids.index(x))\n",
    "    select_variables = [output, 'latitude___ethiopia_2015', 'longitude___ethiopia_2015'] + segment_vars\n",
    "    select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "    all_output = pd.concat([mat['segment_' + output], raw_df[select_variables]], 1)\n",
    "    all_output.to_csv(os.path.join(out_dir,'clus_' + '_' + output + '.csv'))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the above functions for Ethiopia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number_of_hired_workers___policy___ethiopia_2015': 0.273808267736554, 'land_surface___ethiopia_2015': 0.272516052599146, 'quantity_of_chemical_fertilizers_used___policy___ethiopia_2015': 0.160090553373268, 'number_of_oxen_owned___policy___ethiopia_2015': 0.157053494140115, 'household_size___ethiopia_2015': 0.14258227392332698, 'number_of_plough_owned___policy___ethiopia_2015': 0.14223988258113301, 'uses_extension_program___policy___ethiopia_2015': 0.12981246190952803}\n",
      "['crop_sales___output___ethiopia_2011']\n",
      "['crop_sales___output___ethiopia_2013']\n",
      "['crop_sales___output___ethiopia_2015']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "filename = '../data/ethiopia_v23_normed.csv'\n",
    "df = pd.read_csv(filename)\n",
    "# If true, we will generate only clusters across multiple years. Use this mode, when policy recommendations\n",
    "# for a given year is already computed and the lifts need to be computed.\n",
    "generate_multiple_years_clusters = True\n",
    "\n",
    "# Extract actionable, non-actionable variables and the output from the full list of columns.\n",
    "def get_vars(year, base_output):\n",
    "    base_suffix = '___ethiopia_{0}'.format(year)\n",
    "    year_vars = df.filter(regex='.*{0}'.format(base_suffix)).columns.values\n",
    "    out_reg = re.compile('.*' + base_output + '___output.*$')\n",
    "    outputs = list(filter(out_reg.search, year_vars))\n",
    "    policy_reg = re.compile('(.*___policy.*)$')\n",
    "    policy_inputs = list(filter(policy_reg.search, year_vars))\n",
    "    non_policy_reg = re.compile('^((?!policy|output|weight).)*$')\n",
    "    non_policy_inputs = list(filter(non_policy_reg.search, year_vars))\n",
    "    \n",
    "    return outputs, policy_inputs, non_policy_inputs\n",
    "\n",
    "if generate_multiple_years_clusters:\n",
    "    years = [2011, 2013, 2015]\n",
    "else:\n",
    "    years = [2015]\n",
    "    \n",
    "for base_output in [chosen_output]:\n",
    "    ccs = pd.read_csv('../data/cross_correls_ethiopia_2015_v22.csv')\n",
    "    output = base_output + '___output___ethiopia_2015'\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    \n",
    "    # Set of 8 segmenting variables chosen based on iterative feature choosing mentioned above.\n",
    "    # Choose correlation values from selected variables\n",
    "    select = ccs['Unnamed: 0'].str.contains('^.*(oxen|extension|hired|chemical|plough|land_surface|household_size)') # gender, has_saved\n",
    "    ccs = ccs[select]\n",
    "    ccs = ccs.sort_values(output, ascending=False)\n",
    "    num_vars=8\n",
    "    \n",
    "    best_vars = ccs[['Unnamed: 0', output]][:num_vars].as_matrix()\n",
    "    segment_variables = {}\n",
    "    seg_vars = []\n",
    "    for i in best_vars:\n",
    "        name = i[0]\n",
    "        # Location and distance variables are not well defined, ignore.\n",
    "        if 'lives' in name or 'distance' in name:\n",
    "            continue\n",
    "        segment_variables[name] = abs(i[1])\n",
    "\n",
    "    print(segment_variables)\n",
    "\n",
    "    raw_df = df.copy()\n",
    "    df = complete(df)\n",
    "    for year in years:\n",
    "        base_suffix = '___ethiopia_{0}'.format(year)\n",
    "        year_segment_variables = {}\n",
    "        for k, v in segment_variables.items():\n",
    "            year_segment_variables[k.replace('2015', str(year))] = v\n",
    "        outputs, policy_inputs, non_policy_inputs = get_vars(year, base_output)\n",
    "        print(outputs)\n",
    "        raw_df = normalize()\n",
    "        for output in outputs:\n",
    "            if generate_multiple_years_clusters:\n",
    "                df = get_clusters(df, output, year_segment_variables, out_dir='../results/ethiopia_crop_sales')\n",
    "            else:\n",
    "                for c in [0.05]:\n",
    "                    for v in [1.5]:\n",
    "                        out_dir = '../results/grid_' + base_output + '/{0}/{1}'.format(c, v)\n",
    "                        try:\n",
    "                            os.makedirs(out_dir)\n",
    "                        except:\n",
    "                            print('File exists: ' + out_dir)\n",
    "                        global all_relevant, policy_relevant\n",
    "                        imp_feats = select_corr_feats(c)\n",
    "                        all_relevant = vif(imp_feats, raw_df, v)\n",
    "                        run_regressions(in_name=filename, out_dir=out_dir, non_policy_inputs=non_policy_inputs, segment_variables=segment_variables, inputs=policy_inputs, output=output, year=year)\n",
    "\n",
    "                        imp_feats = select_corr_feats(c, True)\n",
    "                        policy_relevant = vif(imp_feats, raw_df, v)\n",
    "                        run_regressions(in_name=filename, out_dir=out_dir, non_policy_inputs=non_policy_inputs, segment_variables=segment_variables, inputs=policy_inputs, output=output, year=year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence of movement across years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframe to a new variable once which will not be altered.\n",
    "df_all = df\n",
    "assert generate_multiple_years_clusters==True, \"Ensure that df has clusters of all years populated by setting this to True, first\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_pick_axe_owned___policy 0.0 2011\n",
      "number_of_pick_axe_owned___policy 0.0 2013\n",
      "number_of_pick_axe_owned___policy 0.0 2011\n",
      "number_of_pick_axe_owned___policy 0.0 2013\n",
      "number_of_pick_axe_owned___policy 0.0 2011\n",
      "number_of_pick_axe_owned___policy 0.0 2013\n",
      "number_of_pick_axe_owned___policy 0.0 2011\n",
      "number_of_pick_axe_owned___policy 0.0 2013\n",
      "number_of_sickle_owned___policy 0.0 2011\n",
      "number_of_sickle_owned___policy 0.0 2013\n",
      "number_of_sickle_owned___policy 0.0 2011\n",
      "number_of_sickle_owned___policy 0.0 2013\n",
      "number_of_sickle_owned___policy 0.0 2011\n",
      "number_of_sickle_owned___policy 0.0 2013\n",
      "number_of_sickle_owned___policy 0.0 2011\n",
      "number_of_sickle_owned___policy 0.0 2013\n",
      "number_of_hired_workers___policy 0.0 2011\n",
      "number_of_hired_workers___policy 0.0 2013\n",
      "number_of_hired_workers___policy 0.0 2011\n",
      "number_of_hired_workers___policy 0.0 2013\n",
      "number_of_hired_workers___policy 0.0 2011\n",
      "number_of_hired_workers___policy 0.0 2013\n",
      "number_of_hired_workers___policy -9.0 2011\n",
      "number_of_hired_workers___policy -8.0 2013\n",
      "number_of_oxen_owned___policy 0.0 2011\n",
      "number_of_oxen_owned___policy 0.0 2013\n",
      "number_of_oxen_owned___policy 0.0 2011\n",
      "number_of_oxen_owned___policy 0.0 2013\n",
      "number_of_oxen_owned___policy 0.0 2011\n",
      "number_of_oxen_owned___policy 0.0 2013\n",
      "number_of_oxen_owned___policy 0.0 2011\n",
      "number_of_oxen_owned___policy 0.0 2013\n",
      "prevent_damage___policy 0.0 2011\n",
      "prevent_damage___policy 0.0 2013\n",
      "prevent_damage___policy 0.0 2011\n",
      "prevent_damage___policy 0.0 2013\n",
      "prevent_damage___policy 0.0 2011\n",
      "prevent_damage___policy 0.0 2013\n",
      "prevent_damage___policy 0.0 2011\n",
      "prevent_damage___policy 0.0 2013\n",
      "amount_of_assistance_received___policy 0.0 2011\n",
      "amount_of_assistance_received___policy 0.0 2013\n",
      "amount_of_assistance_received___policy 0.0 2011\n",
      "amount_of_assistance_received___policy 0.0 2013\n",
      "amount_of_assistance_received___policy 0.0 2011\n",
      "amount_of_assistance_received___policy 0.0 2013\n",
      "amount_of_assistance_received___policy 0.0 2011\n",
      "amount_of_assistance_received___policy 0.0 2013\n",
      "has_borrowed___policy 0.0 2011\n",
      "has_borrowed___policy 0.0 2013\n",
      "has_borrowed___policy 0.0 2011\n",
      "has_borrowed___policy 0.0 2013\n",
      "has_borrowed___policy 0.0 2011\n",
      "has_borrowed___policy 0.0 2013\n",
      "has_borrowed___policy 0.0 2011\n",
      "has_borrowed___policy 0.0 2013\n",
      "uses_irrigation___policy 0.0 2011\n",
      "uses_irrigation___policy 0.0 2013\n",
      "uses_irrigation___policy 0.0 2011\n",
      "uses_irrigation___policy 0.0 2013\n",
      "uses_irrigation___policy 0.0 2011\n",
      "uses_irrigation___policy 0.0 2013\n",
      "uses_irrigation___policy 0.0 2011\n",
      "uses_irrigation___policy 0.0 2013\n",
      "number_of_plough_owned___policy 0.0 2011\n",
      "number_of_plough_owned___policy 0.0 2013\n",
      "number_of_plough_owned___policy 0.0 2011\n",
      "number_of_plough_owned___policy 0.0 2013\n",
      "number_of_plough_owned___policy 0.0 2011\n",
      "number_of_plough_owned___policy 0.0 2013\n",
      "number_of_plough_owned___policy 0.0 2011\n",
      "number_of_plough_owned___policy 0.0 2013\n",
      "illness_of_household_member___policy 0.0 2011\n",
      "illness_of_household_member___policy 0.0 2013\n",
      "illness_of_household_member___policy 0.0 2011\n",
      "illness_of_household_member___policy 0.0 2013\n",
      "illness_of_household_member___policy 0.0 2011\n",
      "illness_of_household_member___policy 0.0 2013\n",
      "illness_of_household_member___policy 0.0 2011\n",
      "illness_of_household_member___policy 0.0 2013\n",
      "uses_credit___policy 0.0 2011\n",
      "uses_credit___policy 0.0 2013\n",
      "uses_credit___policy 0.0 2011\n",
      "uses_credit___policy 0.0 2013\n",
      "uses_credit___policy 0.0 2011\n",
      "uses_credit___policy 0.0 2013\n",
      "uses_credit___policy 0.0 2011\n",
      "uses_credit___policy 0.0 2013\n",
      "crop_diversification___policy 0.0 2011\n",
      "crop_diversification___policy 0.0 2013\n",
      "crop_diversification___policy 0.0 2011\n",
      "crop_diversification___policy 0.0 2013\n",
      "crop_diversification___policy 0.0 2011\n",
      "crop_diversification___policy 0.0 2013\n",
      "crop_diversification___policy 0.0 2011\n",
      "crop_diversification___policy 0.0 2013\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2011\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2013\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2011\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2013\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2011\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2013\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2011\n",
      "quantity_of_chemical_fertilizers_used___policy 0.0 2013\n",
      "increase_in_price_of_inputs___policy 0.0 2011\n",
      "increase_in_price_of_inputs___policy 0.0 2013\n",
      "increase_in_price_of_inputs___policy 0.0 2011\n",
      "increase_in_price_of_inputs___policy 0.0 2013\n",
      "increase_in_price_of_inputs___policy 0.0 2011\n",
      "increase_in_price_of_inputs___policy 0.0 2013\n",
      "increase_in_price_of_inputs___policy 0.0 2011\n",
      "increase_in_price_of_inputs___policy 0.0 2013\n",
      "quantity_of_improved_seeds_used___policy 0.0 2011\n",
      "quantity_of_improved_seeds_used___policy 0.0 2013\n",
      "quantity_of_improved_seeds_used___policy 0.0 2011\n",
      "quantity_of_improved_seeds_used___policy 0.0 2013\n",
      "quantity_of_improved_seeds_used___policy 0.0 2011\n",
      "quantity_of_improved_seeds_used___policy 0.0 2013\n",
      "quantity_of_improved_seeds_used___policy 0.0 2011\n",
      "quantity_of_improved_seeds_used___policy 0.0 2013\n",
      "uses_extension_program___policy 0.0 2011\n",
      "uses_extension_program___policy 0.0 2013\n",
      "uses_extension_program___policy 0.0 2011\n",
      "uses_extension_program___policy 0.0 2013\n",
      "uses_extension_program___policy 0.0 2011\n",
      "uses_extension_program___policy 0.0 2013\n",
      "uses_extension_program___policy 0.0 2011\n",
      "uses_extension_program___policy 0.0 2013\n",
      "percentage_of_damaged_crop___policy 0.0 2011\n",
      "percentage_of_damaged_crop___policy 2.05263157895 2013\n",
      "percentage_of_damaged_crop___policy -0.8125 2011\n",
      "percentage_of_damaged_crop___policy 5.75 2013\n",
      "percentage_of_damaged_crop___policy 0.0 2011\n",
      "percentage_of_damaged_crop___policy 6.0 2013\n",
      "percentage_of_damaged_crop___policy 0.0 2011\n",
      "percentage_of_damaged_crop___policy 4.01041666667 2013\n",
      "number_of_axe_owned___policy 0.0 2011\n",
      "number_of_axe_owned___policy 0.0 2013\n",
      "number_of_axe_owned___policy 0.0 2011\n",
      "number_of_axe_owned___policy 0.0 2013\n",
      "number_of_axe_owned___policy 0.0 2011\n",
      "number_of_axe_owned___policy 0.0 2013\n",
      "number_of_axe_owned___policy 0.0 2011\n",
      "number_of_axe_owned___policy 0.0 2013\n",
      "price_rise_of_food_item___policy 0.0 2011\n",
      "price_rise_of_food_item___policy 0.0 2013\n",
      "price_rise_of_food_item___policy 0.0 2011\n",
      "price_rise_of_food_item___policy 0.0 2013\n",
      "price_rise_of_food_item___policy 0.0 2011\n",
      "price_rise_of_food_item___policy 0.0 2013\n",
      "price_rise_of_food_item___policy 0.0 2011\n",
      "price_rise_of_food_item___policy 0.0 2013\n",
      "number_of_water_storage_pit_owned___policy 0.0 2011\n",
      "number_of_water_storage_pit_owned___policy 0.0 2013\n",
      "number_of_water_storage_pit_owned___policy 0.0 2011\n",
      "number_of_water_storage_pit_owned___policy 0.0 2013\n",
      "number_of_water_storage_pit_owned___policy 0.0 2011\n",
      "number_of_water_storage_pit_owned___policy 0.0 2013\n",
      "number_of_water_storage_pit_owned___policy 0.0 2011\n",
      "number_of_water_storage_pit_owned___policy 0.0 2013\n",
      "owns_land_certificate___policy 0.0 2011\n",
      "owns_land_certificate___policy 0.0 2013\n",
      "owns_land_certificate___policy 0.0 2011\n",
      "owns_land_certificate___policy 0.0 2013\n",
      "owns_land_certificate___policy 0.0 2011\n",
      "owns_land_certificate___policy 0.0 2013\n",
      "owns_land_certificate___policy 0.0 2011\n",
      "owns_land_certificate___policy 0.0 2013\n"
     ]
    }
   ],
   "source": [
    "df = df_all\n",
    "\n",
    "imp_feats = select_all_year_feats()\n",
    "\n",
    "series = pd.DataFrame()\n",
    "output = 'segment_crop_sales___output'\n",
    "raw_output = 'crop_sales___output'\n",
    "\n",
    "# Compute conditional lifts in output when there is increase in each of the features across years.\n",
    "for imp_feat in imp_feats:\n",
    "    for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "        z1 = for_year(output, y1)\n",
    "        z2 = for_year(output, y2)\n",
    "        r1 = for_year(raw_output, y1)\n",
    "        r2 = for_year(raw_output, y2)\n",
    "        f1 = for_year(imp_feat,y1)\n",
    "        f2 = for_year(imp_feat,y2)\n",
    "        df[output + '_change' + y1] = (df[z1]!=df[z2])\n",
    "        df[output + '_increase' + y1] = (df[z1]<df[z2])\n",
    "        df[output + '_decrease' + y1] = (df[z1]>df[z2])\n",
    "        df[imp_feat+'_increase'+y1] = (imp_df[f2]-imp_df[f1])\n",
    "        df[output + '_change_value' + y1] = (imp_df[r2]-imp_df[r1])\n",
    "\n",
    "    # Percentile thresholds if using one to take only significant changes into account.\n",
    "    # per_thresholds = [0, 5,10,15,20,25,30,35,40,45,50,55,60,70,75,80,85,90,95]\n",
    "    \n",
    "    # Iterate through all clusters to see if different variables have different lifts.\n",
    "    for per in [0]:\n",
    "        for seg in range(4):\n",
    "            exp_y_i = []\n",
    "            exp_y = []\n",
    "            exp = []\n",
    "            exp_i = []\n",
    "            avg_y_i = []\n",
    "            avg_y = []\n",
    "            std_y_i = []\n",
    "            std_y = []\n",
    "\n",
    "            for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "                dec_y = []\n",
    "                num_dec_y = []\n",
    "                dec_base_y = []\n",
    "                year = y1\n",
    "                weight = for_year('weight', year)\n",
    "                seg_y = for_year(output, year)\n",
    "                df_seg = df[df[seg_y]==seg]\n",
    "                df_seg = df_seg.loc[df_seg[output + '_change_value' + year].dropna().index]\n",
    "                high_df = df_seg\n",
    "                std_dev = np.std(high_df[imp_feat + '_increase'+year])\n",
    "                feat_mean = np.mean(high_df[imp_feat + '_increase'+year])\n",
    "                def get_thresholds(v):\n",
    "                    if v < -std_dev:\n",
    "                        return 0\n",
    "                    elif v < 0:\n",
    "                        return 1\n",
    "                    elif v > std_dev:\n",
    "                        return 3\n",
    "                    elif v >= 0:\n",
    "                        return 2\n",
    "\n",
    "                median = np.median(high_df[imp_feat + '_increase'+year].dropna())\n",
    "                print (imp_feat, median, year)\n",
    "                def get_simple_thresholds(v):\n",
    "                    if v < median:\n",
    "                        return 0\n",
    "                    elif v == median:\n",
    "                        return 1\n",
    "                    elif v > median:\n",
    "                        return 2\n",
    "                    else:\n",
    "                        return -1\n",
    "\n",
    "                # Compute sub-populations' lift based on increase, no-change or decrease in the input feature.\n",
    "                high_df[imp_feat + '_decile'+year] = high_df[imp_feat + '_increase'+year].apply(lambda x: get_simple_thresholds(x))\n",
    "                clus_avg = np.mean(df_seg[output + '_change_value' + year])\n",
    "                dec_base_y = clus_avg\n",
    "                for t in range(3):\n",
    "                    num_dec_y.append(len(high_df[high_df[imp_feat + '_decile'+year]==t]))\n",
    "                    dec_y.append(np.mean(high_df[high_df[imp_feat + '_decile'+year]==t][output + '_change_value' + year])/clus_avg)\n",
    "                \n",
    "                exp_y_i.append(len(high_df[(high_df[imp_feat + '_increase'+year]>0)]))\n",
    "                exp_y.append(len(high_df))\n",
    "                exp = len(df_seg)\n",
    "                exp_i.append(len(df_seg[df_seg[imp_feat + '_increase'+year]>0]))\n",
    "                avg_y_i.append(np.mean(high_df[(high_df[imp_feat + '_increase'+year]>0)][output + '_change_value' + year]))\n",
    "                avg_y.append(np.mean(high_df[output + '_change_value' + year].dropna()))\n",
    "                std_y_i.append(np.std(high_df[(high_df[imp_feat + '_increase'+year]>0)][output + '_change_value' + year]))\n",
    "                std_y.append(np.std(high_df[output + '_change_value' + year].dropna()))\n",
    "                avg = np.median(df[output + '_change_value' + year].dropna())\n",
    "                row = {}\n",
    "                row['threshold'] = per\n",
    "                row['input'] = imp_feat\n",
    "                row['from'] = year\n",
    "                row['to'] = y2\n",
    "                row['cluster #'] = seg\n",
    "                row['num_hh_in_cluster'] = exp\n",
    "                row['num_hh_in_bins'] = str(num_dec_y)\n",
    "                row['binned_conditional_lift_ratios'] = str(dec_y) \n",
    "                row['average_change_in_output'] = dec_base_y\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "\n",
    "series.to_csv('../results/threshold-lift-simple.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
