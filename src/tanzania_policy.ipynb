{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import Imputer\n",
    "from fancyimpute import SoftImpute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate cross correlation file\n",
    "imp_df = read_csv('../data/tanzania_2014_v8_cleaned.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "corr = imp_df.corr('spearman')['crop_sales___output']\n",
    "corr.to_csv('../data/tanzania_corr.csv', header=['crop_sales___output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children_education___output         15.535957\n",
      "crop_diversification___policy        4.002714\n",
      "has_hired_workers___policy          29.036635\n",
      "number_of_animals_owned___policy    33.175034\n",
      "number_of_hired_workers___policy    44.979647\n",
      "owns_land_certificate___policy      19.063772\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "imp_df = read_csv('../data/tanzania_2014_v8_cleaned.csv')\n",
    "cdf = imp_df.loc[imp_df['crop_sales___output'].dropna().index]\n",
    "# Print missingness values for relevant inputs.\n",
    "missing = 100-(cdf.apply(lambda x: x.count(), axis=0)/len(cdf)*100.0)\n",
    "print (missing[missing > 1])\n",
    "\n",
    "#Plot variations of input features w.r.t output to pick variables to cluster on.\n",
    "varies = imp_df.groupby(pd.qcut(imp_df['crop_sales___output'],5,duplicates='drop')).mean()\n",
    "plt.clf()\n",
    "varies.plot(x='crop_sales___output', subplots=True,legend=True, figsize=(50,200),kind='bar',fontsize=20)\n",
    "plt.savefig('../figures/tanzania_variations.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset matplotlib defaults if plots are skewed after generating variations.pdf.\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature choice for regression based on correl and vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: attended_school : 16.545442650059865\n",
      "Removed: rural_household : 7.307044832667261\n",
      "Removed: household_head_is_male : 6.514077854017189\n",
      "Removed: has_hired_workers___policy : 5.584559393474525\n",
      "Removed: household_size : 5.142702498632374\n",
      "Removed: y4_hhid : 4.662034223507678\n",
      "Removed: literacy : 3.5607803770879505\n",
      "Removed: number_of_animals_owned___policy : 2.2354639741372417\n",
      "Removed: household_head_is_monogamous : 1.7694991549633585\n",
      "\n",
      "Final chosen features \n",
      "\n",
      "household_head_is_divorced 1.02538168751\n",
      "land_surface 1.39960476611\n",
      "owns_land_certificate___policy 1.08311181667\n",
      "household_head_is_polygamous 1.16665401059\n",
      "household_head_is_separated 1.03710721911\n",
      "quantity_of_fertilizers_used___policy 1.08588391493\n",
      "uses_credit___policy 1.24789819785\n",
      "has_borrowed___policy 1.38275057847\n",
      "has_bank_account___policy 1.24877505349\n",
      "uses_irrigation___policy 1.04556629609\n",
      "number_of_ploughs_owned___policy 1.25009904305\n",
      "household_head_is_widowed 1.09187039755\n",
      "quantity_of_pesticides_used___policy 1.01909031835\n",
      "number_of_hired_workers___policy 1.39090684589\n",
      "Removed: has_hired_workers___policy : 3.1182738711803966\n",
      "Removed: number_of_animals_owned___policy : 1.7842434450447773\n",
      "\n",
      "Final chosen features \n",
      "\n",
      "owns_land_certificate___policy 1.07900883531\n",
      "quantity_of_fertilizers_used___policy 1.08325773591\n",
      "uses_credit___policy 1.23008974139\n",
      "has_borrowed___policy 1.29956510877\n",
      "has_bank_account___policy 1.24719179026\n",
      "number_of_ploughs_owned___policy 1.08038571057\n",
      "uses_irrigation___policy 1.04414157707\n",
      "quantity_of_pesticides_used___policy 1.01210213566\n",
      "number_of_hired_workers___policy 1.26764886321\n"
     ]
    }
   ],
   "source": [
    "imp_df = read_csv('../data/tanzania_2014_v8_cleaned.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "\n",
    "\n",
    "def select_all_year_feats():\n",
    "    imp_feats = set([])\n",
    "    for y in [2010, 2012, 2014]:\n",
    "        y_reg = re.compile('.*___policy.*' + str(y) + '$')\n",
    "        y_cols = set(filter(y_reg.search, imp_cols))\n",
    "        y_cols = set([t.replace('___' + country + '_' + str(y), '') for t in y_cols])\n",
    "        if len(imp_feats) == 0:\n",
    "            imp_feats = y_cols\n",
    "        else:\n",
    "            imp_feats = imp_feats.intersection(y_cols)\n",
    "    return imp_feats\n",
    "\n",
    "def select_corr_feats(thres = 0.1, only_policy=False):\n",
    "    ccs = pd.read_csv('../data/tanzania_corr.csv')\n",
    "    output = 'crop_sales___output'\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    ccs = ccs[ccs[output] > thres]\n",
    "    imp_feats = ccs['Unnamed: 0']\n",
    "    non_raw_reg = re.compile('^((?!lives_in|longitude|latitude|output|month).)*$')\n",
    "    imp_feats = list(filter(non_raw_reg.search, imp_feats))\n",
    "    if only_policy:\n",
    "        y_reg = re.compile('.*___policy.*$')\n",
    "        imp_feats = list(filter(y_reg.search, imp_feats))\n",
    "    return imp_feats\n",
    "\n",
    "def normalize():\n",
    "    raw_df = read_csv('../data/tanzania_2014_v8_cleaned.csv')\n",
    "    output = 'crop_sales___output'\n",
    "    raw_df = raw_df.loc[raw_df[output].dropna().index]\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    return raw_df\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif_score\n",
    "\n",
    "def spearman():\n",
    "    collinear = raw_df[feats].corr('spearman')\n",
    "\n",
    "    cols = collinear.columns\n",
    "    for c in collinear.columns:\n",
    "        for v in zip(cols, collinear[c]):\n",
    "            if v[1] > 0.4 and v[1] < 0.5 and v[1] != 1:\n",
    "                print (c, v[0], v[1])\n",
    "    \n",
    "def vif(feats, raw_df, thres=5, debug=False):    \n",
    "    while True:\n",
    "        policy = raw_df[feats].as_matrix()\n",
    "        max_vif = 0\n",
    "        max_vif_feat = None\n",
    "        for i, f in enumerate(feats):\n",
    "            if max_vif < vif_score(policy, i):\n",
    "                max_vif = vif_score(policy, i)\n",
    "                max_vif_feat = f\n",
    "        if max_vif < thres:\n",
    "            break\n",
    "        feat_set = set(feats)\n",
    "        feat_set.remove(max_vif_feat)\n",
    "        if debug:\n",
    "            print ('Removed: {0} : {1}'.format(max_vif_feat, max_vif))\n",
    "        feats = list(feat_set)\n",
    "    if debug:\n",
    "        print ('\\nFinal chosen features \\n')\n",
    "        for i, f in enumerate(feats):\n",
    "            print (f, vif_score(policy, i))\n",
    "    \n",
    "    return feats\n",
    "\n",
    "raw_df = normalize()\n",
    "vifs = []\n",
    "\n",
    "def grid_search():\n",
    "    c_thresholds = [0.05, 0.07, 0.1, 0.12, 0.14,  0.15, 0.16, 0.17, 0.2,0.25,0.3]\n",
    "    v_thresholds = [2,3,4,5]\n",
    "    for thres in thresholds:\n",
    "        imp_feats = select_corr_feats(0.1)\n",
    "        v = vif(imp_feats, raw_df, thres)\n",
    "        vifs += [len(v)]\n",
    "c = 0.05\n",
    "v = 1.5\n",
    "imp_feats = select_corr_feats(c)\n",
    "all_relevant = vif(imp_feats, raw_df, v, True)\n",
    "\n",
    "imp_feats = select_corr_feats(c, True)\n",
    "policy_relevant = vif(imp_feats, raw_df, v, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used for outcome aware clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from scipy import stats\n",
    "import glmnet_python\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot \n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "\n",
    "country = 'tanzania'\n",
    "\n",
    "def get_classes(output_var, pred):\n",
    "    max_bins = 3\n",
    "    _, boundaries = np.histogram(output_var, bins=max_bins)\n",
    "    classes = np.digitize(pred, bins=boundaries)\n",
    "    return classes, max_bins\n",
    "\n",
    "def for_year(var, year):\n",
    "    return var + '___' + country + '_' + str(year)\n",
    "\n",
    "def run_regressions(fixed_k, in_name, out_dir, non_policy_inputs, segment_variables, inputs, output, year):\n",
    "    global table\n",
    "    global coef_table\n",
    "    global avg_table\n",
    "    global coef_map\n",
    "    # table for regressions and classification\n",
    "    table = pd.DataFrame()\n",
    "    avg_table = pd.DataFrame()\n",
    "    coef_map = {}\n",
    "    # create table of coefficients\n",
    "    coef_table = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except:\n",
    "        print(\"Dir exists\")\n",
    "       \n",
    "    df = read_csv(in_name)\n",
    "    df = df.loc[df[output].dropna().index] # drop rows with unobserved income\n",
    "    df = df.loc[df['weight'].dropna().index]\n",
    "    df = df.loc[df[output] != 0] # drop zero outputs\n",
    "    # Transform input\n",
    "    logged_inputs = ['crop_sales___output', 'expenditure___output', 'crop_diversification___policy', 'number_of_animals_owned___policy', 'number_of_hired_workers___policy', 'quantity_of_fertilizers_used___policy', 'quantity_of_pesticides_used___policy', 'distance_to_road', 'distance_to_market', 'household_size', 'land_surface']\n",
    "    for inp in logged_inputs:\n",
    "        df[inp] = df[inp].apply(lambda x: np.log(1+x))\n",
    "\n",
    "    df['nid']= df.index.tolist()\n",
    "    \n",
    "    # select % of data in test set\n",
    "    test_split = 0.2\n",
    "    \n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "    \n",
    "    # reconstruct dataframe with completed matrix\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    \n",
    "    # Redo the same, but without the transformations, only matrix completion for raw matrix.\n",
    "    raw_df = read_csv(in_name)\n",
    "    raw_df = raw_df.loc[raw_df[output].dropna().index]\n",
    "    raw_df = raw_df.loc[raw_df['weight'].dropna().index]\n",
    "    raw_df = raw_df.loc[raw_df[output] != 0] # drop zero outputs\n",
    "    raw_df['nid']= raw_df.index.tolist()\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    raw_df['productivity'] = raw_df['crop_sales___output']/raw_df['land_surface']\n",
    "    raw_df['productivity'] = raw_df['productivity'].apply(lambda x: 0 if x == np.inf else x)\n",
    "\n",
    "    # z-score the matrix mat used for clustering/regression.\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    \n",
    "    y = mat[output]\n",
    "    x = mat[inputs]\n",
    "    \n",
    "    def update_best_lambda(x_scaled):\n",
    "        copy_y = np.array(y, dtype=np.float64)\n",
    "        print (copy_y)\n",
    "        fit = cvglmnet(x = x_scaled.copy(), y = copy_y)\n",
    "        print(fit['lambda_min'])\n",
    "        return fit['lambda_min']\n",
    "    \n",
    "    # Split test/train\n",
    "    indices = range(len(mat))\n",
    "    x_train, x_test, y_train, y_test, ind_train, ind_test = \\\n",
    "        train_test_split(x, y, indices, test_size=test_split, random_state=42)\n",
    "    \n",
    "    def get_train_test(input_vars):\n",
    "        x = mat[input_vars].copy()\n",
    "        x_scaled = StandardScaler()\n",
    "        x_scaled.fit(x)\n",
    "        x_sc = x_scaled.transform(x)\n",
    "        # reconstruct DataFrame\n",
    "        x = pd.DataFrame(x_sc, columns=x.columns)\n",
    "        training_x = x.iloc[ind_train, :]\n",
    "        testing_x = x.iloc[ind_test, :]\n",
    "        return x_sc, training_x, testing_x\n",
    "    \n",
    "    def digitize(output_var, pred):\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        classes, max_bins = get_classes(output_var, pred)\n",
    "        b_classes = label_binarize(classes, range(max_bins))\n",
    "        return b_classes\n",
    "    \n",
    "    def calc_unsegmented(baseline, x_train, x_test): \n",
    "        global table\n",
    "        global coef_map\n",
    "\n",
    "        # keys and values from test data\n",
    "        keys_list = []\n",
    "        y_list = []\n",
    "        for k, v in y_test.iteritems():\n",
    "            keys_list.append(k)\n",
    "            y_list.append(v)\n",
    "\n",
    "        # run regressions on full dataset\n",
    "        name = 'OLS'\n",
    "        model = sm.OLS(y_train, x_train)\n",
    "        fit = model.fit_regularized(alpha=1e-8, refit=True)\n",
    "        y_pred = fit.predict(x_test)\n",
    "\n",
    "        try:\n",
    "            test_c = digitize(y, y_test)\n",
    "            pred_c = digitize(y, y_pred)\n",
    "            auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "        except ValueError:\n",
    "            auc_c = 0.5\n",
    "        mse = mean_squared_error(y_test,y_pred)\n",
    "        scaled_mse = (mse/np.std(y))\n",
    "\n",
    "        # add row to table\n",
    "        new_row = pd.DataFrame({'model': name, 'segment': '', 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': False}, index=[0])\n",
    "        table = table.append(new_row, ignore_index=True)\n",
    "\n",
    "        # add coefficients to map\n",
    "        coef_map[name + '_' + baseline] = fit.params\n",
    "        lower_bounds = []\n",
    "        upper_bounds = []\n",
    "        for ci in fit.conf_int():\n",
    "            lower_bounds += [ci[0]]\n",
    "            upper_bounds += [ci[1]]\n",
    "        coef_map[name + '_' + baseline + '_lower_bound'] = lower_bounds\n",
    "        coef_map[name + '_' + baseline + '_upper_bound'] = upper_bounds\n",
    "    \n",
    "    def calc_segmented(segment_variables, baseline, x_train, x_test):\n",
    "        global table\n",
    "        global coef_map\n",
    "        global avg_table\n",
    "        segment_vars = list(segment_variables.keys())\n",
    "        \n",
    "        def add_clusters():\n",
    "            # elbow method\n",
    "            sse = []\n",
    "            seg_data = mat[segment_vars]\n",
    "            for seg_var in segment_vars:\n",
    "                seg_data[seg_var] = seg_data[seg_var].apply(lambda x: x*segment_variables[seg_var])\n",
    "            for k in range(1,9):\n",
    "                kmeans = KMeans(n_clusters=k).fit(seg_data)\n",
    "                labels = kmeans.labels_\n",
    "                sse.append(sum(np.min(cdist(seg_data, kmeans.cluster_centers_, 'euclidean'), axis=1)) / seg_data.shape[0])\n",
    "\n",
    "            # K-means elbow calculation\n",
    "            plt.clf()\n",
    "            plt.plot(range(1,9), sse)\n",
    "            plt.xlabel('k')\n",
    "            plt.ylabel('Sum of squared error')\n",
    "            plt.savefig(os.path.join(out_dir, 'elbow.png'))\n",
    "            print(sse)\n",
    "            min_k = sse.index(min(sse))\n",
    "            print (min_k)\n",
    "            \n",
    "            # K-means fixed K calculation.\n",
    "            min_k = fixed_k\n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "\n",
    "            labels = kmeans.labels_\n",
    "            mat['cluster'] = labels\n",
    "            # Sort cluster labels in order of mean of output within cluster.\n",
    "            means = []\n",
    "            for i in np.unique(labels):\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "                values = list(raw_clus[output].as_matrix())\n",
    "                # Weighted average can be done if the weight column exists.\n",
    "                weights = list(raw_clus['weight'].as_matrix())\n",
    "                average = np.average(values, weights=weights)\n",
    "                means.append(average)\n",
    "            sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "            print(sorted_ids)\n",
    "            mat['cluster'] = mat['cluster'].apply(lambda x: sorted_ids.index(x))\n",
    "\n",
    "            # Output ids of households, their cluster number, variables on which cluster is done along with lat/long if exists\n",
    "            # select_variables += ['latitude___ethiopia_2015', 'longitude___ethiopia_2015']\n",
    "            # Note (Sam): This is where the file I give you with Ids, cluster numbers is written.\n",
    "            # The baseline = relevant variables\n",
    "            select_variables = [output, 'y4_hhid'] + segment_vars\n",
    "            select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "            all_output = pd.concat([mat['cluster'], raw_df[select_variables]], 1)\n",
    "            all_output.to_csv(os.path.join(out_dir,'clus_' + baseline + '_' + output + '.csv'))\n",
    "            return min_k\n",
    "            \n",
    "        \n",
    "        # Add segments based on median. Used as baseline, no longer used.\n",
    "        def add_segments():\n",
    "            median_segments = {}\n",
    "            for seg_var in segment_vars:\n",
    "                #binary\n",
    "                if len(np.unique(mat[seg_var])) <= 3:\n",
    "                    median_segments[seg_var] = 0\n",
    "                else:\n",
    "                    median_segments[seg_var] = np.median(mat[seg_var])\n",
    "            mat['cluster'] = 0\n",
    "            for seg_var in segment_vars:\n",
    "                mat['cluster'] = 2*mat['cluster'] + [int(x) for x in mat[seg_var] > median_segments[seg_var]]\n",
    "            return int(math.pow(2, len(segment_vars)))\n",
    "\n",
    "        # Segments based solely on location.Used as baseline, no longer used.\n",
    "        def add_location_segments():\n",
    "            locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "            i = 0\n",
    "            mat['cluster'] = 0\n",
    "            for l in sorted(locations):\n",
    "                loc_feature = 'lives_in_' + l + '___ethiopia_' + str(year)\n",
    "                loc_val = mat[loc_feature].apply(lambda x: 0 if x < 0 else 1)\n",
    "                mat['cluster'] = mat['cluster'] + (i*loc_val)\n",
    "                i += 1\n",
    "            return len(locations) + 1\n",
    "            \n",
    "        def _run(max_clusters, method_name):\n",
    "            global table\n",
    "            global avg_table\n",
    "            global coef_map\n",
    "            # reg_clus keeps predictions from clustered regressions along with keys\n",
    "            reg_clus = dict()\n",
    "            name = 'OLS'\n",
    "            reg_clus[name] = {}\n",
    "            \n",
    "            # need new dataframes with only training and test rows.\n",
    "            # we use this when looping through clusters\n",
    "            train_mat = mat.loc[ind_train]\n",
    "            test_mat = mat.loc[ind_test]\n",
    "            train_size = len(train_mat)\n",
    "            \n",
    "            series = {}\n",
    "            series[output] = []\n",
    "            for seg in segment_vars:\n",
    "                series[seg] = []\n",
    "            series = pd.DataFrame()\n",
    "            row = {}\n",
    "            raw_cols = raw_df.columns.values\n",
    "            raw_reg = re.compile('^((?!norm).)*$') #+ str(year) +\n",
    "            # avg_variables is set of all variables whose mean, 25%ile, 75%ile, stddev, stderr stats are written to *_avg file.\n",
    "            avg_variables = list(filter(raw_reg.search, raw_cols))\n",
    "            for i in range(max_clusters):\n",
    "                train_clus = x_train.loc[train_mat['cluster'] == i]\n",
    "                train_y = y_train.loc[train_mat['cluster'] == i]\n",
    "                test_clus = x_test.loc[test_mat['cluster'] == i]\n",
    "                test_y = y_test.loc[test_mat['cluster'] == i]\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "\n",
    "                for seg in avg_variables:\n",
    "                    values = raw_clus[seg].as_matrix()\n",
    "                    # Weighted average if weight column exists.\n",
    "                    weights = raw_clus['weight'].as_matrix()\n",
    "                    average = np.average(values, weights=weights)\n",
    "                    row['mean_' + seg] = average\n",
    "#                     print (average)\n",
    "                    variance = np.average((values-average)**2, weights=weights)\n",
    "                    row['stddev_' + seg] = math.sqrt(variance)\n",
    "                    row['stderr_' + seg] = math.sqrt(variance)/math.sqrt(len(values))\n",
    "                    row['25ile_' + seg] = np.percentile(values, 25)\n",
    "                    row['75ile_' + seg] = np.percentile(values, 75)\n",
    "                \n",
    "                row['index'] = i\n",
    "                row['size'] = len(raw_clus)\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "                avg_table = avg_table.append(new_row, ignore_index=True)\n",
    "                cluster_percent = (len(train_clus)*100.0)/train_size\n",
    "                if train_clus.empty or test_clus.empty:\n",
    "                    continue\n",
    "\n",
    "                keys_list = []\n",
    "                y_list = []\n",
    "                for k, v in test_y.iteritems():\n",
    "                    keys_list.append(k)\n",
    "                    y_list.append(v)\n",
    "\n",
    "                # Regress per cluster\n",
    "                name = 'OLS'\n",
    "                model = sm.OLS(train_y, train_clus)\n",
    "                fit = model.fit_regularized(alpha=1e-8, refit=True)\n",
    "                y_pred = fit.predict(test_clus)\n",
    "\n",
    "                for a, b in enumerate(y_pred):\n",
    "                    t = reg_clus[name]\n",
    "                    t[keys_list[a]] = b\n",
    "                    \n",
    "                coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = fit.params\n",
    "                lower_bounds = []\n",
    "                upper_bounds = []\n",
    "                for ci in fit.conf_int():\n",
    "                    lower_bounds += [ci[0]]\n",
    "                    upper_bounds += [ci[1]]\n",
    "                coef_map[name + '_' + method_name + '_' + str(i) + '_lower_bound'] = lower_bounds\n",
    "                coef_map[name + '_' + method_name + '_' + str(i) + '_upper_bound'] = upper_bounds\n",
    "\n",
    "            # plot sorted correlation\n",
    "            sorted_series = series.sort_values(['mean_' + output])\n",
    "            for seg in avg_variables:\n",
    "                plt.clf()\n",
    "                plt.plot(sorted_series['mean_' + output].as_matrix(), sorted_series['mean_'+seg].as_matrix(), marker='o')\n",
    "                plt.xlabel('Average ' + output.replace('___output___' + country + '_' + str(year), '') + ' output')\n",
    "                plt.ylabel('Average ' + seg.replace('___policy___'  + country + '_' +str(year), '').replace('_',' '))\n",
    "                plt.savefig(os.path.join(out_dir, 'plot_' + seg + '_' + output + '.pdf'))\n",
    "                \n",
    "            # add mse's to table\n",
    "            keys = sorted(y_test.keys())\n",
    "            name = 'OLS'\n",
    "            sort_t = []\n",
    "            sort_p = []\n",
    "\n",
    "            for key in keys:\n",
    "                if key not in y_test or key not in reg_clus[name]:\n",
    "                    continue\n",
    "                sort_t.append(reg_clus[name][key])\n",
    "                sort_p.append(y_test[key])\n",
    "\n",
    "            try:\n",
    "                test_c = digitize(y, sort_t)\n",
    "                pred_c = digitize(y, sort_p)\n",
    "                auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "            except ValueError:\n",
    "                auc_c = 0.5\n",
    "            mse = mean_squared_error(sort_t,sort_p)\n",
    "            scaled_mse = (mse/np.std(y))\n",
    "            new_row = pd.DataFrame({'model': name, 'segment': ','.join(segment_vars), 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': True, 'method': method_name}, index=[0])\n",
    "            table = table.append(new_row, ignore_index=True)\n",
    "                \n",
    "    \n",
    "        ##### Run grouped regressions\n",
    "        if (len(segment_vars) > 1):\n",
    "            _run(add_clusters(), 'clustered')\n",
    "        \n",
    "        # Baselines based on raw segmentation and location based segmentations.\n",
    "        #_run(add_segments(), 'segmented')\n",
    "        #_run(add_location_segments(), 'segmented')\n",
    "    \n",
    "    def run_with_inputs(input_vars, name):\n",
    "        # map to be used in tracking coefficients\n",
    "        global coef_map\n",
    "        global coef_table\n",
    "        global x_train\n",
    "        global x_test\n",
    "        coef_map = {}\n",
    "        x_scaled, x_train, x_test = get_train_test(input_vars)\n",
    "        # Update Lasso Lambda using GLMNET.\n",
    "        calc_unsegmented(name, x_train, x_test)\n",
    "        update_best_lambda(x_scaled)\n",
    "        calc_segmented(segment_variables, name, x_train, x_test)\n",
    "        \n",
    "        for k,v in sorted(coef_map.items()):\n",
    "            kvp = dict()\n",
    "            kvp['model'] = k\n",
    "            kvp['inputs'] = name\n",
    "\n",
    "            for val,invar in zip(v,input_vars):\n",
    "                kvp[invar] = val\n",
    "\n",
    "            new_row = pd.DataFrame(kvp, index=[0])\n",
    "            coef_table = coef_table.append(new_row, ignore_index=True)\n",
    "    \n",
    "    # Baseline 1\n",
    "    # input_vars = inputs + non_policy_inputs\n",
    "    # run_with_inputs(input_vars, 'All variables')\n",
    "\n",
    "    global all_relevant, policy_relevant\n",
    "    \n",
    "    print ('Relevant variables')\n",
    "    print(all_relevant, policy_relevant)\n",
    "    run_with_inputs(all_relevant, 'Highly correlated all')\n",
    "    run_with_inputs(policy_relevant, 'Highly correlated policy')\n",
    "\n",
    "    # save coefficient and output tablesdrop\n",
    "    coef_table.to_csv(os.path.join(out_dir,'coef_' + output + '.csv'))\n",
    "    table.to_csv(os.path.join(out_dir,output + '.csv'))\n",
    "    avg_table.to_csv(os.path.join(out_dir,output + '_avg' + '.csv'))\n",
    "    \n",
    "\n",
    "# Functions to just populate clusters across years.\n",
    "def get_segments(df, output):\n",
    "    df_t = df.loc[df[output].dropna().index]\n",
    "    df['segment_' + output], _ = get_classes(df_t[output], df[output])\n",
    "    return df\n",
    "\n",
    "def complete(df):\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    return mat\n",
    "\n",
    "# Although this repeats calculation done above with clustering/regression, it allows us to add columns per year\n",
    "# in the same dataframe.\n",
    "def get_clusters(mat, output, segment_vars):\n",
    "    segment_vars = list(segment_vars.keys())\n",
    "    seg_data = mat[segment_vars]\n",
    "    min_k = 4\n",
    "    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "    labels = kmeans.labels_\n",
    "    means = []\n",
    "    for i in np.unique(labels):\n",
    "        df_clus = mat.loc[labels == i]\n",
    "        means.append(np.mean(df_clus[output].as_matrix()))\n",
    "    sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "    mat['segment_'+ output] = labels\n",
    "    mat['segment_'+ output] = mat['segment_'+ output].apply(lambda x: sorted_ids[x])\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'land_surface': 0.35053444487641044, 'number_of_animals_owned___policy': 0.2817111411237926, 'has_hired_workers___policy': 0.19052291722913028, 'household_size': 0.17627112215002008, 'household_head_is_widowed': 0.10803100253121313}\n",
      "Dir exists\n",
      "Relevant variables\n",
      "['household_head_is_divorced', 'land_surface', 'owns_land_certificate___policy', 'household_head_is_polygamous', 'household_head_is_separated', 'quantity_of_fertilizers_used___policy', 'uses_credit___policy', 'has_borrowed___policy', 'has_bank_account___policy', 'uses_irrigation___policy', 'number_of_ploughs_owned___policy', 'household_head_is_widowed', 'quantity_of_pesticides_used___policy', 'number_of_hired_workers___policy'] ['owns_land_certificate___policy', 'quantity_of_fertilizers_used___policy', 'uses_credit___policy', 'has_borrowed___policy', 'has_bank_account___policy', 'number_of_ploughs_owned___policy', 'uses_irrigation___policy', 'quantity_of_pesticides_used___policy', 'number_of_hired_workers___policy']\n",
      "[ 0.07582827  0.48055669 -0.90791973 ..., -2.50892367 -1.4343748\n",
      " -1.65375061]\n",
      "[ 0.00093851]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:166: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.48209705328214791, 0.38923715116180557, 0.35584060303805415, 0.3279263424029546, 0.30445543907817296, 0.29161658535797691, 0.27716901067326311, 0.26771309512983626]\n",
      "7\n",
      "[0, 3, 1, 2]\n",
      "[ 0.07582827  0.48055669 -0.90791973 ..., -2.50892367 -1.4343748\n",
      " -1.65375061]\n",
      "[ 0.00348628]\n",
      "[0.48209705328214791, 0.38923715116180557, 0.35584060303805415, 0.32791233754315702, 0.30445543907817296, 0.29182054199481688, 0.27703687611100453, 0.26790568111118868]\n",
      "7\n",
      "[1, 3, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "filename = '../data/tanzania_2014_v8_cleaned.csv'\n",
    "df = pd.read_csv(filename)\n",
    "country = 'tanzania'\n",
    "\n",
    "def get_vars(year):\n",
    "    year_vars = df.columns.values\n",
    "    out_reg = re.compile('.*' + base_output + '___output.*$')\n",
    "    outputs = list(filter(out_reg.search, year_vars))\n",
    "    policy_reg = re.compile('(.*___policy.*)$')\n",
    "    policy_inputs = list(filter(policy_reg.search, year_vars))\n",
    "    non_policy_reg = re.compile('^((?!policy|output|weight).)*$')\n",
    "    non_policy_inputs = list(filter(non_policy_reg.search, year_vars))\n",
    "    \n",
    "    return outputs, policy_inputs, non_policy_inputs\n",
    "\n",
    "# When true, df will contain columns for clusters across years, which then can be used to calculate\n",
    "# evidence of change across years and agreement numbers.\n",
    "generate_multiple_years_clusters = False\n",
    "if generate_multiple_years_clusters:\n",
    "    years = [2010, 2012, 2014]\n",
    "    raise \"Not supported\"\n",
    "else:\n",
    "    years = [2014]\n",
    "    \n",
    "base_output = 'crop_sales'\n",
    "# Choose variables to segment on based on correlation file.\n",
    "ccs = pd.read_csv('../data/' + country + '_corr.csv')\n",
    "output = base_output + '___output'\n",
    "ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "\n",
    "# We chose clustering features based on the iterative approach.\n",
    "# Note(Sam): Modifying this regex will change variables to cluster on.\n",
    "select = ccs['Unnamed: 0'].str.contains('^.*(?=animals|household_size|has_hired|land_surface|widowed).*$')\n",
    "ccs = ccs[select]\n",
    "ccs = ccs.sort_values(output, ascending=False)\n",
    "num_vars=8\n",
    "best_vars = ccs[['Unnamed: 0', output]][:num_vars].as_matrix()\n",
    "segment_variables = {}\n",
    "seg_vars = []\n",
    "for i in best_vars:\n",
    "    name = i[0]\n",
    "    segment_variables[name] = i[1]\n",
    "\n",
    "print(segment_variables)\n",
    "\n",
    "raw_df = df.copy()\n",
    "df = complete(df)\n",
    "for year in years:\n",
    "    outputs, policy_inputs, non_policy_inputs = get_vars(year)\n",
    "    for output in outputs:\n",
    "        if generate_multiple_years_clusters:\n",
    "            df = get_clusters(df, output, segment_variables)\n",
    "        else:\n",
    "            run_regressions(fixed_k=4, in_name=filename, out_dir='../results/tanzania_v6_7_' + base_output, non_policy_inputs=non_policy_inputs, segment_variables=segment_variables, inputs=policy_inputs, output=output, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Below cells are still not ready. Will tweak as we get more data for Tanzania.\n",
    "assert generate_multiple_years_clusters==True, \"Below cells are not supported\"\n",
    "# Run this once to avoid overwriting df in the code below.\n",
    "df_all = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_df = read_csv('../data/tanzania_2014_v8_cleaned.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "imp_feats = set([])\n",
    "for y in [2014]:\n",
    "    y_reg = re.compile('.*___.*' + str(y) + '$')\n",
    "    y_cols = set(filter(y_reg.search, imp_cols))\n",
    "    y_cols = set([t.replace('___tanzania_' + str(y), '') for t in y_cols])\n",
    "    if len(imp_feats) == 0:\n",
    "        imp_feats = y_cols\n",
    "    else:\n",
    "        imp_feats = imp_feats.intersection(y_cols)\n",
    "imp_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-08f0b54ab98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute evidence of movement across years, the lift due to movement in relevant inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimp_feat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimp_feats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'segment_crop_sales___output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute evidence of movement across years, the lift due to movement in relevant inputs.\n",
    "df = df_all\n",
    "series = pd.DataFrame()\n",
    "for imp_feat in imp_feats:\n",
    "    for output in ['segment_crop_sales___output']:\n",
    "        years = ['2011', '2013', '2015']\n",
    "        raw_output = 'crop_sales___output'\n",
    "        coef= {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}\n",
    "        for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "            #try:\n",
    "            z1 = for_year(output, y1)\n",
    "            z2 = for_year(output, y2)\n",
    "            r1 = for_year(raw_output, y1)\n",
    "            r2 = for_year(raw_output, y2)\n",
    "            f1 = for_year(imp_feat,y1)\n",
    "            f2 = for_year(imp_feat,y2)\n",
    "            df[output + '_change' + y1] = (df[z1]!=df[z2])\n",
    "            df[output + '_increase' + y1] = (df[z1]<df[z2])\n",
    "            df[output + '_decrease' + y1] = (df[z1]>df[z2])\n",
    "            expected = df[z1].apply(lambda x: coef[x])\n",
    "            df[imp_feat+'_increase'+y1] = (imp_df[f1]<imp_df[f2])\n",
    "            df[output + '_inversion' + y1] = (df[z2]-df[z1])*(df[f2]-df[f1])*expected\n",
    "            df[output + '_change_value' + y1] = (imp_df[r2]-imp_df[r1])\n",
    "\n",
    "        for per in [50,55,60,70,75,80,85,90,95]:\n",
    "            for seg in range(4):\n",
    "                exp_y_i = []\n",
    "                exp_y = []\n",
    "                exp = []\n",
    "                exp_i = []\n",
    "                for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "                    year = y1\n",
    "                    weight = for_year('weight', year)\n",
    "                    seg_y = for_year(output, year)\n",
    "                    df_seg = df[df[seg_y]==seg]\n",
    "                    df_seg = df_seg.loc[df_seg[output + '_change_value' + year].dropna().index]\n",
    "                    thres = np.percentile(df_seg[output + '_change_value' + year].dropna(), per)\n",
    "                    high_df = df_seg[df_seg[output + '_change_value' + year].apply(lambda x : x >= thres)]\n",
    "                    exp_y_i.append(len(high_df[(high_df[imp_feat + '_increase'+year]==True)]))\n",
    "                    exp_y.append(len(high_df))\n",
    "                    exp.append(len(df_seg))\n",
    "                    exp_i.append(len(df_seg[df_seg[imp_feat + '_increase'+year]==True]))\n",
    "                    avg_y_i = np.mean(high_df[(high_df[imp_feat + '_increase'+year]==True)][output + '_change_value' + year])\n",
    "                    avg_y = np.mean(high_df[output + '_change_value' + year].dropna())\n",
    "                    avg = np.median(df[output + '_change_value' + year].dropna())\n",
    "                row = {}\n",
    "                row['threshold'] = per\n",
    "                row['input'] = imp_feat\n",
    "                row['cluster'] = seg\n",
    "                row['movement overall'] = (sum(exp_y)/sum(exp))*100.0\n",
    "                row['movement conditioned'] = (sum(exp_y_i)/sum(exp_i))*100.0\n",
    "                row['movement lift'] = (sum(exp_y_i)/sum(exp_i))/(sum(exp_y)/sum(exp))\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "\n",
    "series.to_csv('../results/threshold-lift-tanzania.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
